<!doctype html>
<html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Evals — AI Architect Academy</title><script src="https://cdn.tailwindcss.com"></script></head>
<body class="bg-slate-50 text-slate-900"><main class="max-w-4xl mx-auto px-4 py-10">
<h1 class="text-2xl font-bold">Evaluation & Guardrails</h1>
<p class="text-slate-600">Measure what matters and prevent regressions.</p>
<h2 class="mt-6 font-semibold">Quick Wins</h2>
<ul class="list-disc list-inside text-sm">
<li>Evaluate faithfulness and citation coverage for RAG</li>
<li>Trace latency and cost; alert on SLO breaches</li>
<li>Add CI evals for prompt/model changes</li>
</ul>
<h2 class="mt-6 font-semibold">Top Links</h2>
<ul class="list-disc list-inside">
<li><a class="text-cyan-600" href="https://github.com/langfuse/langfuse" target="_blank">langfuse/langfuse</a> • <a class="text-cyan-600" href="https://github.com/promptfoo/promptfoo" target="_blank">promptfoo/promptfoo</a></li>
<li><a class="text-cyan-600" href="https://github.com/AI-Architect-Academy/ai-architect-academy/blob/main/07-evaluation/metrics.md" target="_blank">Metrics</a> • <a class="text-cyan-600" href="https://github.com/AI-Architect-Academy/ai-architect-academy/blob/main/07-evaluation/eval-harness.md" target="_blank">Eval harness</a></li>
<li><a class="text-cyan-600" href="https://github.com/AI-Architect-Academy/ai-architect-academy/blob/main/03-awesome/awesome-evals.md" target="_blank">Awesome Evals</a></li>
</ul>
</main></body></html>
