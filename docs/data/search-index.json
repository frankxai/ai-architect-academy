[
  {
    "id": "00-roadmap\\ROADMAP.md",
    "title": "Experience Roadmap",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/00-roadmap/ROADMAP.md",
    "section": "Roadmap",
    "path": "00-roadmap\\ROADMAP.md",
    "headings": [
      {
        "level": 1,
        "text": "Experience Roadmap"
      },
      {
        "level": 2,
        "text": "Shipping Now"
      },
      {
        "level": 2,
        "text": "Next 30 Days"
      },
      {
        "level": 2,
        "text": "60–90 Day Horizon"
      },
      {
        "level": 2,
        "text": "Guiding Principles"
      }
    ],
    "excerpt": "# Experience Roadmap ## Shipping Now - **Unified navigation + experience hub.** Refreshed with an Experience page, consistent nav, and persona-first copy. - **Brand voice alignment.** Added and updated + to reflect the new narrative. - **Visual system.** New SVG assets ( , ) mirrored under for the live site. ## Next 30",
    "content": "# Experience Roadmap ## Shipping Now - **Unified navigation + experience hub.** Refreshed with an Experience page, consistent nav, and persona-first copy. - **Brand voice alignment.** Added and updated + to reflect the new narrative. - **Visual system.** New SVG assets ( , ) mirrored under for the live site. ## Next 30 Days - **AI Lab Research Sync.** Launch weekly intelligence loop covering OpenAI, DeepMind, Anthropic, Meta, Microsoft, NVIDIA; feed updates into design patterns and micro-learning modules. - **Persona dashboards.** Build dynamic cards on the Experience page that auto-pull highlights from learning paths, projects, and governance modules. - **Interactive project filters.** Extend with saved views (RAG, Agentic, Ops) and top-tier search presets. - **Governance deep dives.** Expand with checklists for procurement, incident response, and human-in-the-loop reviews. ## 60–90 Day Horizon - **Playground templates.** Bundle minimal repos for each flagship project with dockerised quick starts and CI-ready eval harnesses. - **Persona-specific onboarding emails.** Ship optional email sequences that reference repo content and live site milestones. - **Metrics dashboard.** Surface repo commit velocity, project freshness, and evaluation coverage on the homepage pulse cards. ## Guiding Principles - **Lead with clarity.** Every page answers “what value do I get in five minutes?” - **Ship in loops.** Pattern → Project → Eval → Story is the default development cadence. - **Stay truthful.** Document only what exists or is in flight. Ship fast, update often. - **Accessible to all.** Friends, family, enterprise buyers, and influencers should find a path immediately."
  },
  {
    "id": "01-design-patterns\\README.md",
    "title": "Design Pattern Library",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/README.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Design Pattern Library"
      },
      {
        "level": 3,
        "text": "How to Use"
      }
    ],
    "excerpt": "# Design Pattern Library - **Customer Experience**: Content Generation, Language Understanding, Personalization, Conversational Commerce, Creator Studio Automation (new). - **Decision Support & Workflow**: Decision Support Intelligence, Intelligent Orchestration Workflow, Predictive Operations, Autonomous Optimisation.",
    "content": "# Design Pattern Library - **Customer Experience**: Content Generation, Language Understanding, Personalization, Conversational Commerce, Creator Studio Automation (new). - **Decision Support & Workflow**: Decision Support Intelligence, Intelligent Orchestration Workflow, Predictive Operations, Autonomous Optimisation. - **Platform Enablement**: Rapid Innovation, Security & Compliance Automation, Synthetic Data Generation for experimentation. - **AI Infrastructure**: Multicloud Orchestration, Model Lifecycle Management, Governance & Compliance Automation, AI Performance Optimisation. - **Industry Blueprints**: Insurance Rate Intelligence, Energy Trading Optimisation, Cyber Vulnerability Management, Genomic Analytics, GIS Intelligence. Each pattern now includes: 1. **Mission & Use Cases** with measurable KPIs. 2. **Experience Blueprint** mapping human + agent actions. 3. **Technical Architecture Stack** aligned with CoE BOMs. 4. **Implementation Sprints** and **Agent Build Instructions** for rapid prototyping. 5. **Evaluation, Governance, and Deliverables** referencing reusable templates. ### How to Use - Start with the pattern that matches your customer or internal need. - Open to orchestrate build tasks. - Pull supporting assets from for architecture diagrams, BOMs, and guides. - Use governance checklists under to ensure compliance. - Feed outputs back into persona dashboards and Projects saved views for visibility."
  },
  {
    "id": "01-design-patterns\\content-generation.md",
    "title": "Pattern: Content Generation Platform",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/content-generation.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\content-generation.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Content Generation Platform"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints (Agentic Prototyping)"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Lab Insights (Sept 2025)"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Content Generation Platform **Mission:** Deliver personalised, compliant content at speed while keeping humans-in-the-loop and insights flowing back into the campaign brain. ## High-Value Use Cases | Use Case | Impact | Example KPI | | --- | --- | --- | | Multichannel campaign factory | Launch product campai",
    "content": "# Pattern: Content Generation Platform **Mission:** Deliver personalised, compliant content at speed while keeping humans-in-the-loop and insights flowing back into the campaign brain. ## High-Value Use Cases | Use Case | Impact | Example KPI | | --- | --- | --- | | Multichannel campaign factory | Launch product campaigns across web, email, social within hours. | Time-to-publish < 6h; variant lift >15%. | | Personalised nurture flows | Tailor messaging by persona, intent, or lifecycle stage. | Pipeline influence per segment. | | Thought-leadership ghostwriting | Arm executives/SMEs with research-backed narratives. | Content acceptance >90%; engagement >25%. | | Localisation & compliance rewrites | Translate while enforcing disclosures and policy rules. | Zero compliance violations; review SLA <24h. | ## Experience Blueprint | Stage | Human Actions | AI/Agent Responsibilities | Systems | | --- | --- | --- | --- | | Brief Intake | Marketing lead captures campaign brief, goals, audiences. | Brief parser extracts entities, tone, mandatory messaging. | Notion/Asana intake ? Orchestrator queue. | | Research & Retrieval | Strategist curates references, product updates. | Hybrid retrieval agent pulls knowledge base snippets with citations. | Vector store (pgvector) + policy DB. | | Draft & Variant Generation | Editor reviews outlines, selects formats (blog, email, spot). | Generation agents create structured drafts, CTA variants, CTA tests. | LLM runtime + prompt registry. | | Compliance & Review | Legal/compliance review high-risk assets; brand lead approves. | Guardrails run toxicity, claims, IP, accessibility checks; agent summarises diffs. | Policy engine, Langfuse dashboards. | | Publish & Amplify | Ops schedules posts, emails; loops feedback. | Distribution agent pushes to CMS, ESP, social via APIs; analytics agent logs metrics. | Contentful/Webflow, Braze, Hootsuite, analytics warehouse. | ## Technical Architecture Stack 1. **Workflow & State:** Temporal/Camunda orch"
  },
  {
    "id": "01-design-patterns\\creator-studio-automation.md",
    "title": "Pattern: Creator Studio Automation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/creator-studio-automation.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\creator-studio-automation.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Creator Studio Automation"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Creator Studio Automation **Mission:** Build an AI-assisted creator studio that transforms briefs into multi-channel content with brand-safe guardrails and analytics feedback loops. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Campaign factory | Launch coordinated campaigns acro",
    "content": "# Pattern: Creator Studio Automation **Mission:** Build an AI-assisted creator studio that transforms briefs into multi-channel content with brand-safe guardrails and analytics feedback loops. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Campaign factory | Launch coordinated campaigns across channels in hours. | Time-to-publish <6h, engagement uplift 20%. | | Persona-driven content | Tailor copy, visuals, CTAs for each segment. | CTR/conversion per persona. | | Compliance & review | Keep tone, legal language, accessibility consistent. | Compliance violations, review SLA. | | Analytics-to-prompt loop | Feed performance data into prompts weekly. | Improvement per iteration, content acceptance. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Brief Intake | Marketing lead submits campaign brief, goals. | Brief agent normalises metadata, tags audiences. | Intake form (Notion/Asana), orchestrator. | | Research & Inspiration | Creative strategist curates references. | Retrieval agent pulls brand assets, product facts, past wins. | Vector store, DAM. | | Creation & Variants | Writers/designers guide iterations. | Generation agents draft scripts, copy, thumbnails, captions. | LLMs, diffusion models, prompt registry. | | Review & Compliance | Brand/legal review, give feedback. | Guardrail agent runs policy checks, accessibility, style scoring, diff summary. | Policy engine, review console. | | Publish & Amplify | Ops schedules posts/emails, monitors. | Distribution agent pushes to CMS, ESP, social; analytics agent logs metrics. | CMS, email, social schedulers, analytics warehouse. | | Learn & Optimise | Team reviews performance, updates prompts. | Analytics agent surfaces insights, updates prompt packs, experimentation plan. | Data warehouse, dashboards, micro-modules. ## Technical Architecture Stack 1. **Workflow Engine:** Temporal/Camunda orchestrating tasks, approvals, SLA tracking. 2. **Content Br"
  },
  {
    "id": "01-design-patterns\\decision-support.md",
    "title": "Pattern: Decision Support Intelligence",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/decision-support.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\decision-support.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Decision Support Intelligence"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Lab Insights (Sept 2025)"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Decision Support Intelligence **Mission:** Equip decision makers with transparent recommendations, simulations, and audit-ready reasoning while keeping humans accountable. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Credit & underwriting adjudication | Increase approval speed w",
    "content": "# Pattern: Decision Support Intelligence **Mission:** Equip decision makers with transparent recommendations, simulations, and audit-ready reasoning while keeping humans accountable. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Credit & underwriting adjudication | Increase approval speed without raising loss rates. | Decision cycle time, override rate, default delta. | | Workforce & capacity planning | Align staffing and scheduling with demand surges. | Forecast accuracy �5%, overtime reduction 12%. | | Portfolio & pricing optimisation | Simulate scenarios, recommend mix changes. | Margin uplift per segment, scenario coverage. | | Policy compliance review | Summarise regulations, flag conflicts before sign-off. | SLA adherence, audit findings reduced. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Request & Context | Decision owner submits case payload, selects scenario templates. | Intake agent validates metadata, pulls prior decisions. | Workflow engine + decision notebook. | | Evidence Assembly | Analysts curate source-of-truth datasets, policies, playbooks. | Retrieval agent collects facts, comparable cases, regulations. | Data warehouse, policy KB (OPA). | | Scenario Exploration | SMEs run what-if, tweak parameters. | Simulation agent drives optimisation solvers, risk calculators, LLM reasoning narrative. | Solver stack (Gurobi/OR-Tools), LLM runtime. | | Recommendation & Review | Decision board reviews recommendations, set thresholds. | Explanation agent produces reason code, impact summary, uncertainty bands. | Review UI, Langfuse insights. | | Execution & Feedback | Ops applies decision, tracks outcomes. | Feedback agent logs results, creates eval tasks, triggers model retraining backlog. | Line-of-business system, evaluation pipeline. | ## Technical Architecture Stack 1. **Workflow Backbone:** Temporal/Camunda for multi-step approvals, SLA tracking, escalations. 2. **Context Fabric"
  },
  {
    "id": "01-design-patterns\\energy-market-trading-optimization.md",
    "title": "Pattern: Energy Market Trading & Optimisation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/energy-market-trading-optimization.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\energy-market-trading-optimization.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Energy Market Trading & Optimisation"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Energy Market Trading & Optimisation **Mission:** Optimise bidding, dispatch, and risk management across energy markets with transparent analytics and resilient operations. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Day-ahead bidding optimisation | Maximise margins while respe",
    "content": "# Pattern: Energy Market Trading & Optimisation **Mission:** Optimise bidding, dispatch, and risk management across energy markets with transparent analytics and resilient operations. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Day-ahead bidding optimisation | Maximise margins while respecting constraints. | Gross margin uplift, constraint violations = 0. | | Intraday balancing & risk hedging | Reduce imbalance charges, manage intermittency. | Imbalance cost %, reserve utilisation. | | Asset health & outage response | Quickly re-optimise when units fail. | Response time <15 min, availability. | | Market intelligence & reporting | Produce trader briefings, regulatory submissions. | Report turnaround, compliance breaches avoided. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Data & Forecast Prep | Ops ingests telemetry, traders review anomalies. | Data agent validates streams, backfills gaps, triggers alerts. | SCADA feeds, weather APIs, market data lake. | | Forecast Generation | Quants tune models, set scenario assumptions. | Forecast agent runs ensemble (statistical + ML), quantile outputs, confidence bands. | Forecast service (Prophet, XGBoost, deep models). | | Optimisation & Simulation | Traders configure constraints, risk appetite. | Optimisation agent runs solvers (Gurobi, OR-Tools), scenario agent performs Monte Carlo stress. | Solver cluster, risk engine. | | Recommendation & Execution | Trader reviews recommended bids/dispatch. | LLM assistant summarises strategy, provides citations, push to market API. | Bid submission service, Langfuse trace. | | Monitoring & Feedback | Control room monitors results, logs overrides. | Feedback agent tracks actual vs planned, triggers retraining/backtesting tasks. | Observability stack, evaluation pipeline. | ## Technical Architecture Stack 1. **Data Platform:** Time-series lakehouse (Delta/TimescaleDB), weather + commodity ingestion pipelines, "
  },
  {
    "id": "01-design-patterns\\genomic-sequence-processing.md",
    "title": "Pattern: Genomic Sequence Intelligence",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/genomic-sequence-processing.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\genomic-sequence-processing.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Genomic Sequence Intelligence"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Genomic Sequence Intelligence **Mission:** Accelerate genomic analysis for surveillance and clinical care with secure pipelines, transparent reporting, and collaborative workflows. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Variant detection for clinical decisions | Faster dia",
    "content": "# Pattern: Genomic Sequence Intelligence **Mission:** Accelerate genomic analysis for surveillance and clinical care with secure pipelines, transparent reporting, and collaborative workflows. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Variant detection for clinical decisions | Faster diagnosis, targeted therapies. | Turnaround <12h, variant confidence scores, clinician adoption. | | Public health surveillance | Detect emerging strains, track spread. | Time-to-alert, coverage % population, lineage classification accuracy. | | Research collaboration | Share de-identified datasets with partners. | Data requests served, reproducibility, compliance events. | | Pharmacogenomics reporting | Tailor drug regimens to patient profiles. | Prescription adjustments, adverse event reduction. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Intake & QC | Lab tech uploads sequences, metadata, consent. | QC agent checks read quality, contamination, schema. | LIMS, secure object storage. | | Pipeline Execution | Bioinformatician monitors progress, adjusts parameters. | Workflow agent orchestrates alignment, variant calling, annotation (Nextflow/Snakemake). | HPC/Cloud compute. | | Knowledge Integration | Clinician reviews annotations, guidelines. | Retrieval agent links variants to ClinVar, drug labels, institutional knowledge. | Knowledge graph, vector store. | | Reporting & Review | Clinical team signs off, public health shares insights. | Narrative agent drafts reports with risk levels, recommended actions, cites evidence. | Reporting UI, Langfuse trace. | | Feedback & Learning | Outcomes tracked, pipelines tuned. | Analytics agent aggregates metrics, flags drifts, schedules revalidation. | Data warehouse, evaluation suite. | ## Technical Architecture Stack 1. **Workflow Orchestration:** Nextflow/Cromwell orchestrating containerised tasks; integrated with Temporal for metadata + approvals. 2. **Secure Data"
  },
  {
    "id": "01-design-patterns\\gis-intelligence.md",
    "title": "Pattern: Geographic Intelligence Platform",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/gis-intelligence.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\gis-intelligence.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Geographic Intelligence Platform"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Geographic Intelligence Platform **Mission:** Deliver spatial insights for planning, routing, and asset management with rich analytics and natural language interfaces. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Network planning | Optimise site selection for retail, towers, inf",
    "content": "# Pattern: Geographic Intelligence Platform **Mission:** Deliver spatial insights for planning, routing, and asset management with rich analytics and natural language interfaces. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Network planning | Optimise site selection for retail, towers, infrastructure. | Time-to-site shortlist <2 weeks, demand coverage gains. | | Field operations & routing | Reduce travel, improve SLA compliance for crews. | Route efficiency +10%, SLA adherence. | | Risk & compliance mapping | Visualise environmental, regulatory, safety overlays. | Incident reduction, compliance audit success. | | Market expansion intelligence | Identify white spaces, competitor encroachment. | Pipeline value per geography, conversion rate. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Data Intake | GIS analyst curates layers, licensing metadata. | Data agent normalises projections, tags usage constraints, QC. | Spatial data lake, metadata catalog. | | Exploration | Planner queries scenarios, overlays filters. | NL-to-SQL agent translates questions to geospatial queries, produces map cards. | PostGIS/BigQuery GIS, vector tiles. | | Modelling & Prediction | Data scientist runs demand forecasting, risk scoring. | ML agent trains/serves models, provides feature importances. | ML platform + spatial feature store. | | Decision Packaging | Team assembles proposal, routing plan, risk brief. | Narrative agent summarises findings, generates decks/maps, ensures licensing compliance. | Storytelling templates, Langfuse logs. | | Execution & Monitoring | Ops executes plan, monitors KPIs. | Feedback agent tracks outcomes, updates dashboards, triggers retraining. | BI, telemetry ingestion. | ## Technical Architecture Stack 1. **Spatial Data Platform:** Data lake with raster/vector support, ETL pipelines, licensing metadata (DataHub). 2. **Query & Analytics Layer:** PostGIS/BigQuery GIS, tile server (Tegol"
  },
  {
    "id": "01-design-patterns\\governance-compliance.md",
    "title": "Pattern: Governance & Compliance Automation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/governance-compliance.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\governance-compliance.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Governance & Compliance Automation"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Lab Insights (Sept 2025)"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Governance & Compliance Automation **Mission:** Create an AI governance operating system that automates policy enforcement, evidence collection, and risk response without slowing delivery teams. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Policy-as-code enforcement | Ensure mod",
    "content": "# Pattern: Governance & Compliance Automation **Mission:** Create an AI governance operating system that automates policy enforcement, evidence collection, and risk response without slowing delivery teams. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Policy-as-code enforcement | Ensure models/prompts meet regulatory requirements pre-release. | Gate compliance %, audit findings. | | Evidence automation | Auto-generate audit packets for regulators/boards. | Evidence prep time, completeness. | | Governance dashboards | Provide leadership with real-time risk posture. | SLA adherence, open risk items, incident MTTR. | | Change management | Track model/prompt changes with approvals and rollback. | Change success rate, rollback frequency. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Register & Classify | Product owner registers system, risk tier. | Intake agent categorises by domain, regulatory obligations. | Governance registry, metadata store. | | Policy Mapping | Governance lead selects controls, SLAs. | Policy agent applies templates, identifies gaps, suggests requirements. | Policy-as-code repo (OPA). | | Control Execution | Teams integrate guardrails, tests, logging. | Automation agents insert CI/CD checks, evidence collectors. | CI pipelines, evaluation suites. | | Review & Approval | Risk committee reviews artefacts, decisions. | Explainability agent summarises compliance, issues status dashboards. | Review workflow, dashboards. | | Monitor & Respond | Ops monitors drift, incidents. | Monitoring agent correlates metrics, triggers incident response, updates risk log. | Langfuse, incident management. | ## Technical Architecture Stack 1. **Governance Registry:** Central catalog of models/prompts/services with metadata, lifecycle status. 2. **Policy Engine:** Declarative control templates (OPA/Rego), mapping to regulatory frameworks. 3. **Automation Hub:** Integrations with CI/CD, evaluation"
  },
  {
    "id": "01-design-patterns\\insurance-rate-modeling.md",
    "title": "Pattern: Insurance Rate Intelligence",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/insurance-rate-modeling.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\insurance-rate-modeling.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Insurance Rate Intelligence"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Insurance Rate Intelligence **Mission:** Deliver compliant, data-rich pricing decisions with transparent models, human oversight, and rapid iteration. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Personal lines pricing refresh | Adjust rates quickly with granular risk signals. |",
    "content": "# Pattern: Insurance Rate Intelligence **Mission:** Deliver compliant, data-rich pricing decisions with transparent models, human oversight, and rapid iteration. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Personal lines pricing refresh | Adjust rates quickly with granular risk signals. | Time-to-rate change, loss ratio, hit ratio. | | Commercial underwriting assistance | Equip underwriters with insights, narratives, data retrieval. | Underwriter productivity, approval speed. | | Regulatory filing automation | Generate SERFF-ready documentation with evidence. | Filing success rate, prep time. | | Portfolio monitoring & risk alerts | Track drift, adverse trends, recommend actions. | Drift alerts, profitability, retention. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Intake & Data Prep | Actuary/underwriter collects exposure, claims, external data. | Data agent validates, enriches, logs provenance. | Data warehouse, feature store. | | Model Development | Actuarial team trains GLM/GBM, sets constraints. | AutoML agent runs experiments, enforces monotonicity, fairness checks. | ML platform (Databricks/SageMaker). | | Rate Scenario Analysis | Pricing team explores impact by segment. | Simulation agent generates rate curves, sensitivity analysis, scenario narratives. | Solver + analytics workspace. | | Filing & Governance | Compliance/legal review justification, produce documentation. | Narrative agent creates SERFF filing, citations, reviewer checklist. | Document generator, governance workflow. | | Monitoring & Feedback | Ops monitors metrics, runs regular evals. | Evaluation agent triggers drift alerts, recalibration tasks, updates dashboards. | Evaluation pipeline, Langfuse. ## Technical Architecture Stack 1. **Data Foundation:** Governed lakehouse + feature store, external data connectors (credit, weather, telematics). 2. **Model Factory:** Versioned training pipelines (GLM, GBM, neural n"
  },
  {
    "id": "01-design-patterns\\model-lifecycle-management.md",
    "title": "Pattern: Model & Prompt Lifecycle Management",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/model-lifecycle-management.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\model-lifecycle-management.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Model & Prompt Lifecycle Management"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Model & Prompt Lifecycle Management **Mission:** Provide a reusable SDLC for models, prompts, and agents covering build, deploy, monitor, and evolve phases with rigorous governance. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Centralised model registry | Single source of truth ",
    "content": "# Pattern: Model & Prompt Lifecycle Management **Mission:** Provide a reusable SDLC for models, prompts, and agents covering build, deploy, monitor, and evolve phases with rigorous governance. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Centralised model registry | Single source of truth for artefacts, approvals. | Time-to-promote, rollback frequency, audit completeness. | | Automated evaluation gates | Prevent regressions before release. | Gate pass rate, defect escape rate. | | Cost & latency optimisation | Control spend while maintaining UX. | Cost per request, latency SLO, utilisation. | | Incident management & retrospectives | Rapid response, learning loop. | MTTR, postmortem completion, recurrence. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Plan & Intake | Product/tech lead defines scope, risk tier. | Intake agent maps required controls, pipeline templates. | Governance registry, project tracker. | | Build & Evaluate | Engineers train/tune models/prompts, run tests. | Automation agent triggers CI/CD, Promptfoo suites, fairness checks. | Git, CI, evaluation harness, ML platform. | | Deploy & Release | Release manager approves promotion strategy. | Deployment agent handles canary/shadow, updates routing, logs metadata. | K8s/API gateway, feature flag platform. | | Monitor & Operate | Ops monitors metrics, costs, incidents. | Observability agent correlates telemetry, alerts, triggers incident workflows. | Langfuse, Prometheus, logging. | | Learn & Iterate | Team reviews metrics, captures insights. | Retro agent produces summaries, backlog items, storytelling assets. | Retrospective workflow, analytics. | ## Technical Architecture Stack 1. **Source Control & CI/CD:** Git, Actions/Jenkins, IaC for environments. 2. **Data & Feature Management:** Versioned data lake, feature store, lineage tracking. 3. **Experimentation & Registry:** MLflow/W&B, prompt registry, metadata tracking. 4. **"
  },
  {
    "id": "01-design-patterns\\orchestration-workflow.md",
    "title": "Pattern: Intelligent Orchestration Workflow",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/orchestration-workflow.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\orchestration-workflow.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Intelligent Orchestration Workflow"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: Intelligent Orchestration Workflow **Mission:** Coordinate human experts, AI agents, and legacy systems through resilient workflows with full observability and compliance. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Claims adjudication | Reduce cycle time, improve accuracy. | S",
    "content": "# Pattern: Intelligent Orchestration Workflow **Mission:** Coordinate human experts, AI agents, and legacy systems through resilient workflows with full observability and compliance. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Claims adjudication | Reduce cycle time, improve accuracy. | SLA compliance, rework %, customer CSAT. | | Customer onboarding | Automate document collection, checks, approvals. | Time-to-activate, drop-off rate. | | Content approval | Manage multi-stage creative review with guardrails. | Approval SLA, compliance violations. | | Back-office escalations | Route issues to experts with context. | First contact resolution, backlog size. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Intake & Classification | User submits request/task. | Ingestion agent tags, prioritises, routes, fetches context. | Intake UI, CRM, CMS. | | Task Decomposition | Ops lead defines required steps, resources. | Planner agent sequences tasks, assigns humans/bots, adds guardrails. | Workflow engine (Temporal/Camunda). | | Execution & Collaboration | SMEs, bots complete tasks. | Task agents call APIs, update records, generate summaries, escalate exceptions. | Service APIs, RPA, LLM tools. | | Review & Approval | Approvers review outcomes, provide feedback. | QA agent aggregates evidence, policy checks, diff view. | Review console. | | Completion & Feedback | Ops communicates outcomes, logs metrics. | Reporter agent updates systems, notifies stakeholders, triggers retros. | Ticketing, analytics. ## Technical Architecture Stack 1. **Orchestration Engine:** Temporal/Camunda/Airflow for long-running workflows, retries, schedules. 2. **Task Router:** Queue + rules engine, dynamic assignment, SLA monitoring. 3. **Context Store:** Shared case file (Postgres/Elastic), embeddings for knowledge retrieval. 4. **Agent Services:** Tool wrappers, LLM orchestrators, RPA integrations, deterministic microservices. "
  },
  {
    "id": "01-design-patterns\\performance-optimization.md",
    "title": "Pattern: AI Performance Optimisation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/performance-optimization.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\performance-optimization.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: AI Performance Optimisation"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Lab Insights (Sept 2025)"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: AI Performance Optimisation **Mission:** Maximise responsiveness and throughput while controlling cost and quality across AI workloads. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Latency tuning for chat/RAG | Deliver snappy UX, reduce drop-offs. | P95 latency, abandonment rate",
    "content": "# Pattern: AI Performance Optimisation **Mission:** Maximise responsiveness and throughput while controlling cost and quality across AI workloads. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Latency tuning for chat/RAG | Deliver snappy UX, reduce drop-offs. | P95 latency, abandonment rate. | | Cost governance | Prevent runaway spend, increase ROI. | Cost per 1K tokens/request, budget variance. | | Capacity planning | Ensure availability during peaks. | Utilisation, autoscale efficiency. | | Model optimisation | Deploy smaller/faster models without losing quality. | Accuracy delta, compute savings. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Baseline Assessment | SRE gathers metrics, identifies hotspots. | Observability agent aggregates logs, traces, cost data. | Langfuse, Prometheus, FinOps dashboards. | | Optimisation Strategy | Architects define targets, trade-offs. | Advisor agent recommends tactics (batching, caching, model tiers). | Knowledge base (playbooks, benchmarks). | | Implementation | Engineers deploy optimisations, test. | Automation agent runs load tests, prompt regression, capacity tests. | CI/CD, load test harness. | | Validation | QA validates latency, quality, cost. | Evaluation agent compares metrics vs baseline, surfaces regressions. | Promptfoo, metrics pipeline. | | Continuous Monitoring | Ops monitors and iterates. | Feedback agent alerts on drift, triggers playbook. | Alerting, retrospectives. ## Technical Architecture Stack 1. **Observability:** Centralised metrics (Prometheus/Grafana), tracing (OpenTelemetry/Langfuse), logging (ELK). 2. **Load & Regression Testing:** Tools like k6/Locust, Promptfoo, synthetic dataset generator. 3. **Optimisation Toolkit:** Model quantisation (ONNX/TensorRT), caching (vLLM, Redis), batching/proxy. 4. **Automation:** Blue/green, canary, shadow deployments, autoscaling policies (HPA/KEDA). 5. **FinOps:** Cost tracking (Cloud cost"
  },
  {
    "id": "01-design-patterns\\vulnerability-management.md",
    "title": "Pattern: AI-Powered Vulnerability Management",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/vulnerability-management.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\vulnerability-management.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: AI-Powered Vulnerability Management"
      },
      {
        "level": 2,
        "text": "High-Value Use Cases"
      },
      {
        "level": 2,
        "text": "Experience Blueprint"
      },
      {
        "level": 2,
        "text": "Technical Architecture Stack"
      },
      {
        "level": 2,
        "text": "Data & Models"
      },
      {
        "level": 2,
        "text": "Implementation Sprints"
      },
      {
        "level": 2,
        "text": "Agent Build Instructions"
      },
      {
        "level": 2,
        "text": "Evaluation & Observability"
      },
      {
        "level": 2,
        "text": "Governance & Controls"
      },
      {
        "level": 2,
        "text": "Deliverables & Templates"
      }
    ],
    "excerpt": "# Pattern: AI-Powered Vulnerability Management **Mission:** Prioritise, remediate, and communicate security vulnerabilities using AI-driven triage, context, and automation. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Risk-based prioritisation | Focus on most critical issues first. | MTTR ",
    "content": "# Pattern: AI-Powered Vulnerability Management **Mission:** Prioritise, remediate, and communicate security vulnerabilities using AI-driven triage, context, and automation. ## High-Value Use Cases | Use Case | Impact | KPIs | | --- | --- | --- | | Risk-based prioritisation | Focus on most critical issues first. | MTTR for critical vulns, risk reduction. | | Automated ticket enrichment | Give engineers actionable context fast. | Ticket acceptance, time-to-fix. | | Executive reporting | Summaries for leadership, compliance. | Report accuracy, delivery time. | | Exception management | Track risk acceptance and revalidation. | Exception count, review compliance. | ## Experience Blueprint | Stage | Human | AI/Agents | Systems | | --- | --- | --- | --- | | Ingest & Normalise | SecOps imports scanner data, asset inventory. | Data agent deduplicates, enriches with business context, threat intel. | Vulnerability scanners, CMDB, threat feeds. | | Prioritise | Security analysts review backlog, set strategy. | Scoring agent computes risk, exploit probability, business impact ranking. | Risk engine, ML models. | | Assign & Remediate | Engineers receive tickets with guidance. | Triage agent generates remediation steps, links knowledge base. | Ticketing (Jira/ServiceNow), RPA. | | Validate & Close | Security verifies fixes, runs scans. | QA agent validates, updates status, documents evidence. | Scanners, automation scripts. | | Report & Improve | Leadership receives metrics, exceptions tracked. | Narrative agent summarises posture, compliance, trend insights. | Dashboards, Langfuse. ## Technical Architecture Stack 1. **Data Integration:** Ingest pipeline consolidating scanners (Qualys, Tenable), CMDB, business metadata, threat intel. 2. **Risk Engine:** ML + rule-based scoring, asset criticality, exploit feeds. 3. **Automation Layer:** Ticket enrichment, workflow automation, reminders, knowledge base integration. 4. **Collaboration & Reporting:** Dashboards, exec brief generator, "
  },
  {
    "id": "02-learning-paths\\100-hour-ai-architect.md",
    "title": "100-Hour AI Architect Plan",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/100-hour-ai-architect.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\100-hour-ai-architect.md",
    "headings": [
      {
        "level": 1,
        "text": "100-Hour AI Architect Plan"
      },
      {
        "level": 2,
        "text": "Orientation (Week 0, 4h)"
      },
      {
        "level": 2,
        "text": "Week 1 — Retrieval Foundations (26h)"
      },
      {
        "level": 2,
        "text": "Week 2 — Agents, Tooling, and Evaluations (25h)"
      },
      {
        "level": 2,
        "text": "Week 3 — Governance, Cost, and Performance (24h)"
      },
      {
        "level": 2,
        "text": "Week 4 — Specialisation and Portfolio (25h)"
      },
      {
        "level": 2,
        "text": "Sustain & Teach Forward"
      }
    ],
    "excerpt": "# 100-Hour AI Architect Plan Time-boxed to four sprints (~25 hours each) with optional orientation prep. Every week pairs context, guided study, and a tangible artifact. ## Orientation (Week 0, 4h) - **Micro-learning boost:** Pick two modules from [Micro-Learning Atlas](micro-learning.md) to reinforce personal gaps. Lo",
    "content": "# 100-Hour AI Architect Plan Time-boxed to four sprints (~25 hours each) with optional orientation prep. Every week pairs context, guided study, and a tangible artifact. ## Orientation (Week 0, 4h) - **Micro-learning boost:** Pick two modules from [Micro-Learning Atlas](micro-learning.md) to reinforce personal gaps. Log outcomes in the [Learning Logbook](logbook.md). - **Setup:** Clone the repo, skim , bookmark the live site, and sync your AI tools. - **Baseline:** Document current strengths + gaps using (create if not already started). - **Intent:** Capture a one-page learning goal brief; agree with mentor/manager if you have one. ## Week 1 — Retrieval Foundations (26h) **North star:** Stand up trustworthy retrieval and answer flows. | Day | Focus | Hands-on Output | Key References | | --- | --- | --- | --- | | 1 | Tokenization, embeddings, vector math | Notebook comparing embedding models on sample corpus | , | | 2 | Chunking + hybrid search patterns | Chunk strategy doc with rationale and scoring table | , | | 3 | Build search service (pgvector/Qdrant) | Running index + search API with tests | | | 4 | RAG baseline with citations | CLI or notebook app answering top 5 domain questions | | | 5 | Retro + storytelling | 2-page architecture note + loom-style walk-through | , | **Assessment checklist** - Query latency < 1.5s for top 20 questions in sample set. - Demo recorded and linked in . - Risks & next bets logged in your project README. ## Week 2 — Agents, Tooling, and Evaluations (25h) **North star:** Chain tools together with observability you trust. | Day | Focus | Hands-on Output | Key References | | --- | --- | --- | --- | | 1 | Function calling + tool design | Agent spec describing tasks, tools, guardrails | , | | 2 | LangGraph / orchestration lab | Flow diagram + simulated run traces | | | 3 | Observability setup (Langfuse, metrics) | Project instrumented with traces + dashboards | , | | 4 | Eval harness + CI | Promptfoo suite covering faithfulness, toxicity"
  },
  {
    "id": "02-learning-paths\\agentic-code-swarms.md",
    "title": "Learning Path: Agentic Code Swarms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/agentic-code-swarms.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\agentic-code-swarms.md",
    "headings": [
      {
        "level": 1,
        "text": "Learning Path: Agentic Code Swarms"
      },
      {
        "level": 2,
        "text": "Prereqs"
      },
      {
        "level": 2,
        "text": "Modules"
      },
      {
        "level": 2,
        "text": "Hands-On Steps"
      },
      {
        "level": 2,
        "text": "Challenges"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "Grading Rubric"
      }
    ],
    "excerpt": "# Learning Path: Agentic Code Swarms Audience: AI Architects and Builders using Codex, Claude Code, or Cursor. Outcome: Ship a working multi-agent swarm, understand orchestration patterns, and build a visual explorer. Time: 10–16 hours (self-paced) ## Prereqs - Python 3.11 - Optional: OpenAI/Anthropic key (LiteLLM). Ot",
    "content": "# Learning Path: Agentic Code Swarms Audience: AI Architects and Builders using Codex, Claude Code, or Cursor. Outcome: Ship a working multi-agent swarm, understand orchestration patterns, and build a visual explorer. Time: 10–16 hours (self-paced) ## Prereqs - Python 3.11 - Optional: OpenAI/Anthropic key (LiteLLM). Otherwise runs offline. ## Modules 1) Concepts and Patterns (1h) - Roles, messages, tools, orchestrators - Coordination: sequential, round-robin, planner–worker–reviewer 2) Core Setup (1h) - , venv, run Hello Swarm 3) Orchestration (2h) - Inspect and extend P–W–R with retries and critiques 4) Tools (2h) - Add a and tool 5) Visual Explorer (2h) - Use , add controls and message tracing 6) SaaS Planner (2–4h) - Extend to output a BoM and architecture diagram 7) Evaluate (2h) - Design acceptance checks and simple evals (see ) ## Hands-On Steps ## Challenges - Add a agent that blocks the final answer if tests fail - Add tracing to show token usage per agent (mock values offline) - Swap providers (OpenAI ↔ Anthropic) via LiteLLM config only ## Deliverables - Working swarm with at least 3 roles - A recorded screen demo of the Streamlit explorer - A short doc: architecture, trade-offs, and next steps ## Grading Rubric - Functionality (40%): agents coordinate, produce coherent outputs - Clarity (30%): code readability, clear prompts, thoughtful defaults - UX (20%): UI communicates roles, history, and decisions visually - Rigor (10%): eval checks or tests"
  },
  {
    "id": "02-learning-paths\\beginner.md",
    "title": "Beginner Path (4 Weeks)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/beginner.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\beginner.md",
    "headings": [
      {
        "level": 1,
        "text": "Beginner Path (4 Weeks)"
      },
      {
        "level": 2,
        "text": "Weekly Flow"
      },
      {
        "level": 2,
        "text": "Practice Rituals"
      },
      {
        "level": 2,
        "text": "Graduation Checklist"
      }
    ],
    "excerpt": "# Beginner Path (4 Weeks) A ramp for builders spending ~6 hours/week. Pair it with the self-assessment to track progress. ## Weekly Flow | Week | Focus | Outcomes | Guided Assets | | --- | --- | --- | --- | | 1 | LLM fundamentals + safe prompting | Cheatsheet covering tokens, sampling, prompt patterns, safety guardrail",
    "content": "# Beginner Path (4 Weeks) A ramp for builders spending ~6 hours/week. Pair it with the self-assessment to track progress. ## Weekly Flow | Week | Focus | Outcomes | Guided Assets | | --- | --- | --- | --- | | 1 | LLM fundamentals + safe prompting | Cheatsheet covering tokens, sampling, prompt patterns, safety guardrails | , | | 2 | Retrieval + embeddings + pgvector basics | Hands-on notebook building hybrid retrieval over your own notes | , | | 3 | Intro agents + tool orchestration | Simple agent that calls two tools (search + summarise) with evaluation checklist | , | | 4 | Evaluations + observability | Promptfoo suite with faithfulness + toxicity checks, retro using | , | ## Practice Rituals - Assign one module per week from the [Micro-Learning Atlas](micro-learning.md) to reinforce hands-on practice. Log artefacts in . - Ship a tiny artifact each week (notebook, README, Loom) and store it in . - Use the as a visual anchor for study groups. - Run a 30-minute wrap-up retro every Friday using the prompts in . ## Graduation Checklist - You can explain how embeddings, prompts, and guardrails work together. - You have a working retrieval notebook and a basic agent script with evals. - You can run Langfuse locally and interpret trace metrics."
  },
  {
    "id": "02-learning-paths\\bootcamp.md",
    "title": "Bootcamp Path (AI CoE Inspired)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/bootcamp.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\bootcamp.md",
    "headings": [
      {
        "level": 1,
        "text": "Bootcamp Path (AI CoE Inspired)"
      },
      {
        "level": 2,
        "text": "Week 1 — Vision, Patterns, Customer Journeys"
      },
      {
        "level": 2,
        "text": "Week 2 — Architecture, Integration, Operations"
      },
      {
        "level": 2,
        "text": "Week 3 — Industry Focus, Launch, Storytelling"
      },
      {
        "level": 2,
        "text": "Final Gate Review"
      }
    ],
    "excerpt": "# Bootcamp Path (AI CoE Inspired) Three intense weeks (~35 hours/week) designed for leaders building or scaling internal AI programs. ## Week 1 — Vision, Patterns, Customer Journeys - Kick off with the [Micro-Learning Atlas](micro-learning.md) modules for Context Engineering and Executive Narrative to prime stakeholder",
    "content": "# Bootcamp Path (AI CoE Inspired) Three intense weeks (~35 hours/week) designed for leaders building or scaling internal AI programs. ## Week 1 — Vision, Patterns, Customer Journeys - Kick off with the [Micro-Learning Atlas](micro-learning.md) modules for Context Engineering and Executive Narrative to prime stakeholders. - **Discovery workshops:** Facilitate the questions in with stakeholders. - **Experience blueprint:** Pair with the hero image to outline end-to-end journeys. - **Outcome:** A signed-off narrative including personas, success metrics, and high-level roadmap stored in . ## Week 2 — Architecture, Integration, Operations - **Deep dives:** Map reference architectures using + . - **Implementation lab:** Stand up a production-ready retrieval service and an agent workflow with observability. - **Governance cockpit:** Complete DPIA, risk matrix, and policy guardrails; review with security using templates. - **Outcome:** Architecture dossier + cost model + operational checklists. ## Week 3 — Industry Focus, Launch, Storytelling - **Choose vertical:** Use and to pin down compliance + integration needs. - **Launch playbook:** Produce GTM briefs, enablement docs, and a walkthrough video starring the mentor persona. - **Community loop:** Publish a thought-leadership article in and schedule a governance retro. - **Outcome:** Executive-ready program brief, demo assets, and a 90-day execution plan. ## Final Gate Review Use the template. You are ready to scale if: 1. Strategy, architecture, and governance docs are linked from a single source of truth. 2. A working prototype demonstrates value + guardrails, with eval data attached. 3. Comms plan (slides, poster, article) is share-ready for leadership and customers."
  },
  {
    "id": "02-learning-paths\\logbook.md",
    "title": "Learning Logbook",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/logbook.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\logbook.md",
    "headings": [
      {
        "level": 1,
        "text": "Learning Logbook"
      },
      {
        "level": 2,
        "text": "Weekly Summary"
      },
      {
        "level": 2,
        "text": "Links"
      },
      {
        "level": 2,
        "text": "Reflection Prompts"
      }
    ],
    "excerpt": "# Learning Logbook Track daily progress, insights, and blockers. Copy this template per cohort or per individual. | Date | Focus | Time Spent | Key Insight | Blockers | Next Action | | --- | --- | --- | --- | --- | --- | | | | | | | | ## Weekly Summary - Highlights - Metrics movement (latency, cost, eval scores) - Stak",
    "content": "# Learning Logbook Track daily progress, insights, and blockers. Copy this template per cohort or per individual. | Date | Focus | Time Spent | Key Insight | Blockers | Next Action | | --- | --- | --- | --- | --- | --- | | | | | | | | ## Weekly Summary - Highlights - Metrics movement (latency, cost, eval scores) - Stakeholder feedback ## Links - Demos / notebooks - Slides / memos - Guardrail or governance updates ## Reflection Prompts - What pattern repeated this week? - What surprised you? - Where can you teach or document a new lesson?"
  },
  {
    "id": "02-learning-paths\\micro-learning.md",
    "title": "Micro-Learning Playlists",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-learning.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-learning.md",
    "headings": [
      {
        "level": 1,
        "text": "Micro-Learning Playlists"
      },
      {
        "level": 2,
        "text": "Week Sprint Templates"
      },
      {
        "level": 3,
        "text": "Sprint A — Retrieval Excellence"
      },
      {
        "level": 3,
        "text": "Sprint B — Agentic Delivery"
      },
      {
        "level": 3,
        "text": "Sprint C — Governance Ready"
      },
      {
        "level": 3,
        "text": "Sprint D — Storytelling & Adoption"
      },
      {
        "level": 3,
        "text": "Sprint E � Creator Studio Launch"
      },
      {
        "level": 2,
        "text": "Implementation Tips"
      }
    ],
    "excerpt": "# Micro-Learning Playlists Mix and match the [Micro-Learning Atlas](micro-modules/README.md) modules to build targeted sprints. Each playlist fits inside a week (~5 hours) and pairs with deliverables from the main curriculum. ## Week Sprint Templates ### Sprint A — Retrieval Excellence 1. [Context Engineering for LLMs]",
    "content": "# Micro-Learning Playlists Mix and match the [Micro-Learning Atlas](micro-modules/README.md) modules to build targeted sprints. Each playlist fits inside a week (~5 hours) and pairs with deliverables from the main curriculum. ## Week Sprint Templates ### Sprint A — Retrieval Excellence 1. [Context Engineering for LLMs](micro-modules/foundations-context-engineering.md) 2. [Hybrid Ranking Blueprint](micro-modules/retrieval-hybrid-ranking.md) 3. [RAG Guardrails Fast Track](micro-modules/retrieval-rag-guardrails.md) 4. [Domain RAG Clinic (Healthcare)](micro-modules/retrieval-domain-rag-healthcare.md) 5. **Ship:** Update your domain RAG README with guardrails + metrics. ### Sprint B — Agentic Delivery 1. [LangGraph Planner Loop](micro-modules/agents-langgraph-planner.md) 2. [Multi-Agent Handoff Lab](micro-modules/agents-multi-agent-handoff.md) 3. [Langfuse Telemetry Sprints](micro-modules/operations-langfuse-telemetry.md) 4. **Ship:** Record a demo of planner → reviewer flow and capture traces. ### Sprint C — Governance Ready 1. [Model Risk Review Sprint](micro-modules/governance-model-risk-review.md) 2. [Policy Automation Quick Start](micro-modules/governance-policy-automation.md) 3. [Cost & Latency Playbook](micro-modules/operations-cost-optimization.md) 4. **Ship:** Merge CI policy gate and circulate governance update via [Executive Narrative Builder](micro-modules/storytelling-exec-brief.md). ### Sprint D — Storytelling & Adoption 1. [Evaluation Signals Primer](micro-modules/foundations-evaluation-signals.md) 2. [Evaluation Automation Pipeline](micro-modules/evaluation-automation-pipeline.md) 3. [Adoption & ROI Metrics](micro-modules/storytelling-adoption-metrics.md) 4. [Executive Narrative Builder](micro-modules/storytelling-exec-brief.md) 5. **Ship:** Publish a metrics + narrative bundle in . ### Sprint E � Creator Studio Launch 1. [Creator Content Orchestration Sprint](micro-modules/creator-content-orchestration.md) 2. [Creator Evaluation Scorecards](micro-modules"
  },
  {
    "id": "02-learning-paths\\micro-modules\\README.md",
    "title": "Micro-Learning Atlas",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/README.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Micro-Learning Atlas"
      }
    ],
    "excerpt": "# Micro-Learning Atlas Rapid-fire modules designed for 45-minute deep dives. Each micro-lesson pairs a single outcome with hands-on artefacts, updated for the September 2025 AI architecture landscape. Use this index to stitch custom learning journeys or supplement the 100-Hour Plan. | Category | Module | Purpose | Dura",
    "content": "# Micro-Learning Atlas Rapid-fire modules designed for 45-minute deep dives. Each micro-lesson pairs a single outcome with hands-on artefacts, updated for the September 2025 AI architecture landscape. Use this index to stitch custom learning journeys or supplement the 100-Hour Plan. | Category | Module | Purpose | Duration | | --- | --- | --- | --- | | Foundations | [Context Engineering for LLMs](foundations-context-engineering.md) | Master prompt + retrieval scaffolding using 2025 best practices | 45 min | | Foundations | [Evaluation Signals Primer](foundations-evaluation-signals.md) | Build a minimal eval harness with production-grade metrics | 40 min | | Retrieval Systems | [Hybrid Ranking Blueprint](retrieval-hybrid-ranking.md) | Combine sparse/dense search with rerankers shipping in 2025 | 50 min | | Retrieval Systems | [RAG Guardrails Fast Track](retrieval-rag-guardrails.md) | Implement grounded answers with policy + factual checks | 45 min | | Domain & Vertical | [Domain RAG Clinic (Healthcare)](retrieval-domain-rag-healthcare.md) | Build a healthcare-focused RAG with compliance-ready datasets & evals | 55 min | | Agentic Patterns | [LangGraph Planner Loop](agents-langgraph-planner.md) | Orchestrate task decomposition using LangGraph 0.3 | 45 min | | Agentic Patterns | [Multi-Agent Handoff Lab](agents-multi-agent-handoff.md) | Execute cross-agent workflows with shared memory + audits | 55 min | | Operations & Observability | [Langfuse Telemetry Sprints](operations-langfuse-telemetry.md) | Wire traces, cost, and drift dashboards in one sprint | 40 min | | Operations & Observability | [Cost & Latency Playbook](operations-cost-optimization.md) | Apply batching, caching, and adaptive routing to trim spend | 50 min | | Operations & Observability | [Evaluation Automation Pipeline](evaluation-automation-pipeline.md) | Automate regression suites with promptfoo, Langfuse, and CI gates | 45 min | | Governance & Risk | [Model Risk Review Sprint](governance-model-risk-re"
  },
  {
    "id": "02-learning-paths\\micro-modules\\agents-langgraph-planner.md",
    "title": "LangGraph Planner Loop",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/agents-langgraph-planner.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\agents-langgraph-planner.md",
    "headings": [
      {
        "level": 1,
        "text": "LangGraph Planner Loop"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# LangGraph Planner Loop **Category:** Agentic Patterns **Duration:** 45 minutes **Outcome:** Build a LangGraph 0.3 planner that decomposes tasks, manages memory, and calls tools with guardrails. ## Why it matters - LangGraph 0.3 introduced native tool scheduling, retries, and graph visualisations. - Organisations rely",
    "content": "# LangGraph Planner Loop **Category:** Agentic Patterns **Duration:** 45 minutes **Outcome:** Build a LangGraph 0.3 planner that decomposes tasks, manages memory, and calls tools with guardrails. ## Why it matters - LangGraph 0.3 introduced native tool scheduling, retries, and graph visualisations. - Organisations rely on deterministic planners before enabling autonomous steps. ## Prerequisites - Python 3.11+ environment with LangChain/LangGraph installed. - Toolset: search function, retrieval tool, evaluator (could be simple mock). - Access to at least one high-signal dataset (e.g., company knowledge base). ## Step-by-step 1. **Define nodes:** Create planner, worker, evaluator, and reporter nodes as per . 2. **Configure memory:** Store intermediate states in or . Limit history to 5 steps. 3. **Implement planner:** Use with reasoning tokens enabled (OpenAI o4-mini or Claude 3.5 Sonnet) and define . 4. **Add guardrails:** Integrate the RAG guardrails module for citations + policy enforcement before responses exit the graph. 5. **Visualise:** Render the graph with LangGraph's built-in and save a PNG in . 6. **Validate:** Run three tasks (e.g., \"Draft migration plan\", \"Summarise governance policy\") and capture success/failure metrics in Langfuse. ## Deliverables - LangGraph graph definition file with planner loop and memory config. - PNG of the graph structure for storytelling decks. - Experiment notes in the Logbook + follow-up questions for the Multi-Agent Handoff module. ## References - LangGraph 0.3 release (July 2025) docs. - OpenAI o4 reasoning model best practices (August 2025). - for validating agent behaviours."
  },
  {
    "id": "02-learning-paths\\micro-modules\\agents-multi-agent-handoff.md",
    "title": "Multi-Agent Handoff Lab",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/agents-multi-agent-handoff.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\agents-multi-agent-handoff.md",
    "headings": [
      {
        "level": 1,
        "text": "Multi-Agent Handoff Lab"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Multi-Agent Handoff Lab **Category:** Agentic Patterns **Duration:** 55 minutes **Outcome:** Prototype a two-agent workflow (researcher + reviewer) with explicit handoffs, audit trails, and human-in-the-loop checkpoints. ## Why it matters - 2025 enterprise deployments favour collaborative agents with controlled auton",
    "content": "# Multi-Agent Handoff Lab **Category:** Agentic Patterns **Duration:** 55 minutes **Outcome:** Prototype a two-agent workflow (researcher + reviewer) with explicit handoffs, audit trails, and human-in-the-loop checkpoints. ## Why it matters - 2025 enterprise deployments favour collaborative agents with controlled autonomy. - Clear handoffs reduce hallucinations and improve compliance. ## Prerequisites - Completion of [LangGraph Planner Loop](agents-langgraph-planner.md) module. - Access to a shared datastore (Redis, Supabase) for passing artefacts. - Slack/Teams webhook or email integration for human approvals (optional but recommended). ## Step-by-step 1. **Define roles:** Researcher agent gathers evidence; Reviewer agent validates, adds citations, and triggers approvals. 2. **Model selection:** Use Claude 3.5 Sonnet for research (long context) and GPT-4.1 mini for reviewer (speed). Configure via OpenRouter if centralised billing needed. 3. **Artefact contract:** Create JSON schema (problem, evidence[], risks[], draft_response) stored in shared_state. 4. **Human gate:** Add a webhook step that posts reviewer output to Slack with Approve/Reject buttons (use Slack Workflow Builder or n8n). 5. **Logging:** Persist each handoff with Langfuse spans labelled handoff_researcher and handoff_reviewer. Attach metadata (latency, token cost, approved? yes/no). 6. **Post-mortem:** If rejection occurs, run [Retrospective with AI](../../15-workflows/retrospective-with-ai.md) to capture lessons and update prompts. ## Deliverables - LangGraph agent definitions + orchestration script. - Screenshot or recording of the Slack/Teams approval flow. - Updated governance log noting review cadence and escalation path. ## References - Slack Workflow Builder AI approvals (May 2025 release). - OpenAI Function Calling best practices (June 2025 update). - 16-collaboration/agentic-teams.md for collaboration patterns."
  },
  {
    "id": "02-learning-paths\\micro-modules\\creator-analytics-feedback-loop.md",
    "title": "Creator Analytics Feedback Loop",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/creator-analytics-feedback-loop.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\creator-analytics-feedback-loop.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Analytics Feedback Loop"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Creator Analytics Feedback Loop **Category:** Growth & Insights **Duration:** 45 minutes **Outcome:** Wire marketing analytics back into prompts, briefs, and experimentation plans to keep AI-assisted content improving. ## Why it matters - Performance data often lives in silos; connecting it to the creation pipeline u",
    "content": "# Creator Analytics Feedback Loop **Category:** Growth & Insights **Duration:** 45 minutes **Outcome:** Wire marketing analytics back into prompts, briefs, and experimentation plans to keep AI-assisted content improving. ## Why it matters - Performance data often lives in silos; connecting it to the creation pipeline unlocks rapid iteration. - Continuous learning avoids publishing fatigue and proves the ROI of creator automation investments. ## Prerequisites - Access to analytics exports (YouTube, TikTok, newsletter, web, CRM) with at least engagement and conversion metrics. - Basic familiarity with your data warehouse or BI tool (Mode, Looker, Metabase). - Outcome logs from recent campaigns (OKRs, narrative summaries). ## Step-by-step 1. **Collect key metrics:** Export last 4 weeks of channel performance and standardise columns (asset ID, publish date, watch time, CTR, conversions). 2. **Map to assets:** Link analytics rows to the AI-generated artefacts stored in your DAM or outputs. 3. **Derive insights:** Identify top/bottom performers and tag root causes (hook strength, distribution timing, CTA clarity). 4. **Feed prompts:** Update prompts/templates with insight snippets (e.g., �Lead with metric-driven hook referencing benefit X�). Store versions in . 5. **Schedule experiments:** Plan 2�3 tests for the next sprint (thumbnail variants, narrative angle). Document in . 6. **Automate refresh:** Add a job or reminder to pull analytics weekly and append to for trending dashboards. ## Deliverables - Normalised analytics dataset saved in . - Updated prompt variants capturing data-driven hooks. - Experiment plan summarising tests, success metrics, and owners. ## References - [ ](../../06-toolchains/stack-reference.md) for analytics connectors and warehouses. - [ ](../micro-learning.md) to slot this feedback loop into broader learning sequences. - [ ](../../16-collaboration/escalation-guide.md) for stakeholder communication during iteration."
  },
  {
    "id": "02-learning-paths\\micro-modules\\creator-content-orchestration.md",
    "title": "Creator Content Orchestration Sprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/creator-content-orchestration.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\creator-content-orchestration.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Content Orchestration Sprint"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Creator Content Orchestration Sprint **Category:** Creator Systems **Duration:** 50 minutes **Outcome:** Stand up an AI-assisted production board that takes briefs to multi-channel assets with brand-safe guardrails. ## Why it matters - Creator teams juggle ideation, scripting, editing, and publishing across multiple ",
    "content": "# Creator Content Orchestration Sprint **Category:** Creator Systems **Duration:** 50 minutes **Outcome:** Stand up an AI-assisted production board that takes briefs to multi-channel assets with brand-safe guardrails. ## Why it matters - Creator teams juggle ideation, scripting, editing, and publishing across multiple channels; automation keeps the cadence sustainable. - Structured hand-offs between humans and AI reduce brand risk while allowing experimentation with new formats. ## Prerequisites - Access to a campaign brief or editorial calendar (Notion/Asana export works). - Local copy of [ ](../../01-design-patterns/creator-studio-automation.md). - Prompt tooling (Promptfoo or LangChain notebooks) to test candidate prompts. ## Step-by-step 1. **Map the pipeline:** Sketch the stages (brief ? outline ? script ? edit ? publish) and assign human/AI responsibilities. 2. **Create prompt packs:** Draft structured prompts for outline, hook, CTA, and SEO blurb generation. Store in a folder. 3. **Wire data sources:** Connect brand voice snippets, product FAQs, and performance metrics into a lightweight retrieval layer (e.g., ). 4. **Set review gates:** Define human approval points and add checklist items (style, compliance, accessibility). Capture them in the orchestrator or a Kanban template. 5. **Automate publishing prep:** Generate distribution copy (YouTube description, LinkedIn post, newsletter teaser) reusing the same source snippets. 6. **Log metrics:** Decide which KPIs feed back (CTR, watch time, saves). Add placeholders for analytics ingestion in . ## Deliverables - Pipeline diagram or state machine exported to . - Prompt pack ( ) with rationale and guardrails. - Review checklist embedded in your project management tool or saved as . ## References - [ ](../../03-awesome/portfolio-examples.md) for storytelling inspiration. - [ ](../../06-toolchains/vercel-ai-sdk.md) for streaming editor experiences. - [ ](../../05-projects/eval-automation/README.md) to monitor tone"
  },
  {
    "id": "02-learning-paths\\micro-modules\\creator-evaluation-scorecards.md",
    "title": "Creator Evaluation Scorecards",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/creator-evaluation-scorecards.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\creator-evaluation-scorecards.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Evaluation Scorecards"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Creator Evaluation Scorecards **Category:** Evaluation & QA **Duration:** 40 minutes **Outcome:** Build a repeatable scoring flow that keeps AI-generated creative assets on-brand, accurate, and high-converting. ## Why it matters - Creative output fails fast when tone, metadata, or compliance slip. Quantitative scorec",
    "content": "# Creator Evaluation Scorecards **Category:** Evaluation & QA **Duration:** 40 minutes **Outcome:** Build a repeatable scoring flow that keeps AI-generated creative assets on-brand, accurate, and high-converting. ## Why it matters - Creative output fails fast when tone, metadata, or compliance slip. Quantitative scorecards keep stakeholders aligned. - Evaluations turn subjective feedback into measurable loops that improve prompts, datasets, and human guidance. ## Prerequisites - Access to the latest Promptfoo/Langfuse setup ( ). - Recently published assets or drafts ready for scoring (videos, blog posts, social variants). - Brand or compliance checklist (pull from or legal guidelines). ## Step-by-step 1. **Define success metrics:** Pick 3�5 signals (tone adherence, fact accuracy, CTA strength, SEO keyword coverage, safety). 2. **Instrument Promptfoo:** Add the signals to with expected outcomes, linking to specific tone rules or metadata requirements. 3. **Build human rubric:** Translate signals into a simple 1�5 scale spreadsheet or Notion template for reviewers. 4. **Automate ingestion:** Update to store outputs in and surface them in Langfuse dashboards. 5. **Close the loop:** Present results during weekly creative ops review; adjust prompts or guardrails based on missed thresholds. 6. **Document updates:** Log new findings in and share with stakeholders. ## Deliverables - Updated evaluation dataset targeting creator campaigns ( ). - Scorecard template for human reviewers saved in . - Retro notes summarising key improvements and action items. ## References - [ ](../../05-projects/domain-rag-healthcare/eval/healthcare.json) for structured expectation examples. - [ ](../../07-evaluation/metrics.md) to align score definitions. - [ ](../../15-workflows/retrospective-with-ai.md) for follow-up sessions."
  },
  {
    "id": "02-learning-paths\\micro-modules\\evaluation-automation-pipeline.md",
    "title": "Evaluation Automation Pipeline",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/evaluation-automation-pipeline.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\evaluation-automation-pipeline.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Automation Pipeline"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Evaluation Automation Pipeline **Category:** Operations & Observability **Duration:** 45 minutes **Outcome:** Automate regression evaluations with promptfoo, Langfuse, and CI gates to keep releases safe. ## Why it matters - AI teams now run nightly eval suites; automation prevents regressions from reaching prod. - Ti",
    "content": "# Evaluation Automation Pipeline **Category:** Operations & Observability **Duration:** 45 minutes **Outcome:** Automate regression evaluations with promptfoo, Langfuse, and CI gates to keep releases safe. ## Why it matters - AI teams now run nightly eval suites; automation prevents regressions from reaching prod. - Ties into policy automation requirements for regulated industries. ## Prerequisites - Node.js 18+, Python 3.11. - Langfuse project with API key. - [ ](../../05-projects/eval-automation/README.md) checked out. ## Step-by-step 1. **Define scenarios:** Populate with real-world prompts, expected behaviours, and critical metrics. 2. **Promptfoo config:** Edit to include accuracy, toxicity, and citation checks. Reference templates from [Evaluation Signals Primer](foundations-evaluation-signals.md). 3. **Langfuse integration:** Use to execute promptfoo and sync results to Langfuse synthetic evaluations. 4. **CI gate:** Enable the GitHub Action in to run on every PR. Block merges when metrics drop below thresholds. 5. **Reporting:** Generate the HTML summary via and attach to release notes / [Executive Narrative Builder](storytelling-exec-brief.md). 6. **Continuous improvement:** Schedule weekly evaluation reviews with the [Model Risk Review Sprint](governance-model-risk-review.md). ## Deliverables - Updated + pipeline. - CI job status badge linked in project README. - Langfuse dashboard showing trendlines for key metrics. ## References - Promptfoo 2.0 automation guide (August 2025). - Langfuse synthetic evaluation API docs (July 2025). - GitHub Actions best practices for AI eval gating."
  },
  {
    "id": "02-learning-paths\\micro-modules\\foundations-context-engineering.md",
    "title": "Context Engineering for LLMs",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/foundations-context-engineering.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\foundations-context-engineering.md",
    "headings": [
      {
        "level": 1,
        "text": "Context Engineering for LLMs"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Context Engineering for LLMs **Category:** Foundations **Duration:** 45 minutes **Outcome:** Produce a context blueprint that mixes system prompts, retrieval buffers, and grounding data tuned for enterprise-grade assistants in 2025. ## Why it matters - Llama-3.1, Claude 3.5, and GPT-4.1 ships alongside smaller edge m",
    "content": "# Context Engineering for LLMs **Category:** Foundations **Duration:** 45 minutes **Outcome:** Produce a context blueprint that mixes system prompts, retrieval buffers, and grounding data tuned for enterprise-grade assistants in 2025. ## Why it matters - Llama-3.1, Claude 3.5, and GPT-4.1 ships alongside smaller edge models—each needs explicit context allocation. - Poor prompt hygiene still causes hallucinations; context engineering is faster than model fine-tuning for many use cases. ## Prerequisites - Basic familiarity with prompt templates and retrieval-augmented generation. - Access to an embedding store (Supabase pgvector, Pinecone, or Qdrant) seeded with at least 50 documents. ## Step-by-step 1. **Capture intent:** Interview a stakeholder (or use the prompt in 15-workflows/ai-briefing.md) to extract success criteria, tone, and compliance requirements. 2. **Design the skeleton:** Use the framing in \u00001-design-patterns/content-generation.md to define system + developer + user message slots. 3. **Add retrieval buffers:** Pull top-8 documents via hybrid search (BM25 + dense). Keep the context_window_budget table from \u00005-projects/rag-on-supabase.md handy to stay under token limits. 4. **Layer guardrails:** Insert policy snippets from \u00008-governance/policy-packs.md (e.g., PII redaction or safety clauses). Add anti-hallucination checks referencing etrieval-rag-guardrails.md. 5. **Instrument:** Fire five representative queries through Langfuse using the template in \u00005-projects/evals-langfuse.md. Capture latency, cost, and hallucination metrics. 6. **Debrief:** Summarise findings in the [Learning Logbook](../logbook.md) and flag open risks in 16-collaboration/escalation-guide.md. ## Deliverables - Context blueprint (system/developer prompt plus retrieval plan) committed to your project repo. - Langfuse trace link showing before/after adjustments. - 3 lessons learned ready to share in a retrospective. ## References - Anthropic context engineering whitepaper (Aug 2025 upda"
  },
  {
    "id": "02-learning-paths\\micro-modules\\foundations-evaluation-signals.md",
    "title": "Evaluation Signals Primer",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/foundations-evaluation-signals.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\foundations-evaluation-signals.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Signals Primer"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Evaluation Signals Primer **Category:** Foundations **Duration:** 40 minutes **Outcome:** Stand up a lightweight evaluation harness that scores accuracy, safety, and UX signals for a single workflow. ## Why it matters - 2025-era teams track multi-dimensional signals (faithfulness, bias, latency) before shipping any a",
    "content": "# Evaluation Signals Primer **Category:** Foundations **Duration:** 40 minutes **Outcome:** Stand up a lightweight evaluation harness that scores accuracy, safety, and UX signals for a single workflow. ## Why it matters - 2025-era teams track multi-dimensional signals (faithfulness, bias, latency) before shipping any assistant. - Langfuse, Promptfoo, and Weights & Biases now interoperate—learn the minimal loop. ## Prerequisites - Working RAG or agent prototype (CLI or notebook). - Langfuse project + API key (free tier is fine). - Node.js 18+ installed for promptfoo. ## Step-by-step 1. **Define assertions:** Use \u00007-evaluation/metrics.md to pick 3 KPIs (e.g., faithfulness ≥ 0.7, response time < 2 s, tone = \"compliant\"). 2. **Assemble dataset:** Export 15 representative questions + references from \u00005-projects/rag-on-supabase.md or your own CSV. Store in evaluation/cases.yaml. 3. **Configure promptfoo:** Run px promptfoo init and wire Langfuse integration using the template in \u00005-projects/evals-langfuse.md. 4. **Add custom check:** Implement a short script that flags responses missing citations or violating safety rules (use etrieval-rag-guardrails.md as reference). 5. **Execute & log:** promptfoo eval with parallelism=3. Inspect Langfuse dashboard for latency + cost scatterplots. 6. **Snapshot results:** Export promptfoo report and attach to your project README. Log follow-up work in 15-workflows/postmortem.md if thresholds failed. ## Deliverables - promptfoo.yaml with assertions + custom check. - Langfuse dashboard link showing evaluation traces. - Summary paragraph for the [Executive Narrative Builder](storytelling-exec-brief.md) module. ## References - Promptfoo 2.0 evaluation recipes (July 2025). - Langfuse model quality scorecards blog (May 2025). - \u00007-evaluation/eval-harness.md for advanced setup."
  },
  {
    "id": "02-learning-paths\\micro-modules\\governance-model-risk-review.md",
    "title": "Model Risk Review Sprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/governance-model-risk-review.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\governance-model-risk-review.md",
    "headings": [
      {
        "level": 1,
        "text": "Model Risk Review Sprint"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Model Risk Review Sprint **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Facilitate a cross-functional risk review and produce an updated risk register entry with mitigations. ## Why it matters - Regulators (EU AI Act, UK FCA) expect documented risk reviews for high-impact systems. - Aligns prod",
    "content": "# Model Risk Review Sprint **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Facilitate a cross-functional risk review and produce an updated risk register entry with mitigations. ## Why it matters - Regulators (EU AI Act, UK FCA) expect documented risk reviews for high-impact systems. - Aligns product, legal, and engineering around a shared mitigation plan. ## Prerequisites - Current DPIA or risk log from \u00008-governance/. - Representatives from product, legal/compliance, and engineering (30-minute slot). - Latest telemetry snapshots (Langfuse dashboards, eval reports). ## Step-by-step 1. **Prep pack:** Gather metrics (latency, accuracy, failures) and policy documents. Use template in \u00008-governance/risk-review-template.md (create if missing). 2. **Run session:** Facilitate 20-minute workshop: identify new risks, rate severity/likelihood, assign owners. Use prompts in 16-collaboration/escalation-guide.md. 3. **Update register:** Document new risks (ID, description, severity, mitigation, owner, review date). Store in \u00008-governance/risk-register.csv. 4. **Mitigation backlog:** Create Jira or Linear tickets for critical mitigations. Link to [Policy Automation Quick Start](governance-policy-automation.md) for automation ideas. 5. **Communicate:** Summarise outcomes in the [Executive Narrative Builder](storytelling-exec-brief.md) and share with leadership. 6. **Schedule next review:** Set review cadence (monthly for high risk, quarterly otherwise) and log in the Learning Logbook. ## Deliverables - Updated risk register entry (CSV/Markdown). - Meeting notes with decisions + owners. - Follow-up backlog items for mitigations. ## References - EU AI Act compliance checklist (July 2025 version). - NIST AI RMF Playbook updates (May 2025). - \u00008-governance/model-risk.md for risk categories."
  },
  {
    "id": "02-learning-paths\\micro-modules\\governance-policy-automation.md",
    "title": "Policy Automation Quick Start",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/governance-policy-automation.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\governance-policy-automation.md",
    "headings": [
      {
        "level": 1,
        "text": "Policy Automation Quick Start"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Policy Automation Quick Start **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Automate DPIA/policy workflows using checklists, GitOps, and CI guards. ## Why it matters - Manual policy reviews don’t scale across hundreds of micro-features. - GitOps + policy-as-code ensures every release is tracea",
    "content": "# Policy Automation Quick Start **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Automate DPIA/policy workflows using checklists, GitOps, and CI guards. ## Why it matters - Manual policy reviews don’t scale across hundreds of micro-features. - GitOps + policy-as-code ensures every release is traceable. ## Prerequisites - Access to repo workflows (GitHub Actions or GitLab CI). - Governance templates under \u00008-governance/. - Optional: OpenPolicyAgent/Rego knowledge. ## Step-by-step 1. **Codify checklist:** Convert \u00008-governance/checklists.md into YAML (governance/checklist.yaml) capturing required artefacts (DPIA doc, risk score, Langfuse link). 2. **Create CI gate:** Build GitHub Action that blocks PR merge if checklist items missing (use `ctions/github-script or OPA conftest). 3. **Generate DPIA summary:** Use prompt-packs/governance/dpia.nlg.json (create if missing) to auto-generate summaries after each change. 4. **Notify stakeholders:** Integrate with Teams/Slack to ping policy owners when PR flagged. 5. **Archive proofs:** Store generated PDFs in governance/artifacts/ with release tags. 6. **Review impact:** Run the [Model Risk Review Sprint](governance-model-risk-review.md) to confirm automation coverage. ## Deliverables - YAML checklist committed to repo. - CI workflow file enforcing policy automation. - Example DPIA summary attached to release notes. ## References - GitHub Actions OPA policy enforcement (June 2025 blog). - OpenPolicyAgent policy-as-code patterns for AI workflows. - \u00008-governance/privacy-gdpr.md and \u00008-governance/checklists.md."
  },
  {
    "id": "02-learning-paths\\micro-modules\\operations-cost-optimization.md",
    "title": "Cost & Latency Playbook",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/operations-cost-optimization.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\operations-cost-optimization.md",
    "headings": [
      {
        "level": 1,
        "text": "Cost & Latency Playbook"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Cost & Latency Playbook **Category:** Operations & Observability **Duration:** 50 minutes **Outcome:** Reduce per-request cost and latency by at least 20% using batching, caching, and adaptive routing. ## Why it matters - Model pricing volatility in 2025 requires real-time routing to cheaper or faster models. - CFOs ",
    "content": "# Cost & Latency Playbook **Category:** Operations & Observability **Duration:** 50 minutes **Outcome:** Reduce per-request cost and latency by at least 20% using batching, caching, and adaptive routing. ## Why it matters - Model pricing volatility in 2025 requires real-time routing to cheaper or faster models. - CFOs expect cost-per-outcome metrics before approving scale. ## Prerequisites - Baseline metrics from the [Langfuse Telemetry Sprints](operations-langfuse-telemetry.md). - Access to multiple models (OpenAI, Anthropic, Groq, or open-source via OpenRouter). - Cache layer (Redis, Upstash, or Vercel KV). ## Step-by-step 1. **Baseline snapshot:** Export latency & cost per endpoint from Langfuse. 2. **Implement caching:** Cache prompt → response pairs for low-risk endpoints with 1-hour TTL. Invalidate on underlying data updates. 3. **Add batching:** For retrieval-heavy flows, batch embedding requests (OpenAI text-embedding-3-small) and log throughput gains. 4. **Adaptive routing:** Configure Router (e.g., Anyscale Endpoints or custom) to route to Groq Llama-3.1-8B for low complexity tasks, fallback to Claude 3.5 for high complexity (use heuristics/prompt foo scoring). 5. **Measure again:** Compare latency/cost improvements using Langfuse dashboards. Aim for ≥20% improvement. 6. **Document policy:** Update \u00006-toolchains/stack-reference.md with routing rules, and note governance implications in [Policy Automation Quick Start](governance-policy-automation.md). ## Deliverables - Routing + caching code snippet. - Before/after metrics table. - Communication note for exec stakeholders (tie into [Executive Narrative Builder](storytelling-exec-brief.md)). ## References - Groq LPU latency benchmarks (May 2025). - OpenAI API cost management guidelines (July 2025). - Anyscale Router release notes (August 2025)."
  },
  {
    "id": "02-learning-paths\\micro-modules\\operations-langfuse-telemetry.md",
    "title": "Langfuse Telemetry Sprints",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/operations-langfuse-telemetry.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\operations-langfuse-telemetry.md",
    "headings": [
      {
        "level": 1,
        "text": "Langfuse Telemetry Sprints"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Langfuse Telemetry Sprints **Category:** Operations & Observability **Duration:** 40 minutes **Outcome:** Stand up a telemetry baseline in Langfuse covering latency, cost, drift, and dataset health. ## Why it matters - 2025 exec dashboards expect real-time AI performance stats. - Langfuse 2.3 ships drift detection, c",
    "content": "# Langfuse Telemetry Sprints **Category:** Operations & Observability **Duration:** 40 minutes **Outcome:** Stand up a telemetry baseline in Langfuse covering latency, cost, drift, and dataset health. ## Why it matters - 2025 exec dashboards expect real-time AI performance stats. - Langfuse 2.3 ships drift detection, cost anomaly alerts, and synthetic eval hooks. ## Prerequisites - Langfuse project + API key. - Access to your application logs or ability to instrument requests. - Optional: Snowflake/BigQuery for warehouse sync. ## Step-by-step 1. **Instrument client & server:** Use Langfuse middleware for Vercel AI SDK (see \u00006-toolchains/vercel-ai-sdk.md). Ensure trace IDs propagate end-to-end. 2. **Capture metadata:** Log model, temperature, tool invocations, and token counts. Add custom properties for \business_unit, customer_tier. 3. **Enable alerts:** Configure cost anomaly alert (20% spike) and latency SLA alert (p95 > 2s). Route notifications to Slack/Teams. 4. **Drift monitors:** Upload evaluation datasets via Langfuse Synthetic Evaluations. Track accuracy vs last baseline (tie into [Evaluation Signals Primer](foundations-evaluation-signals.md)). 5. **Warehouse sync:** If using Snowflake, enable the Langfuse connector and schedule daily sync for historical analysis. 6. **Snapshot dashboard:** Export charts for latency, cost, and accuracy. Attach to your Logbook and share in the AI Briefing workflow. ## Deliverables - Instrumented code snippet + Langfuse dashboard link. - Alert configuration screenshot. - Drift evaluation report with action items. ## References - Langfuse 2.3 release notes (August 2025). - Vercel AI SDK observability guide (July 2025). - \u00007-evaluation/metrics.md for aligning evaluation fields."
  },
  {
    "id": "02-learning-paths\\micro-modules\\retrieval-domain-rag-healthcare.md",
    "title": "Domain RAG Clinic (Healthcare)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/retrieval-domain-rag-healthcare.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\retrieval-domain-rag-healthcare.md",
    "headings": [
      {
        "level": 1,
        "text": "Domain RAG Clinic (Healthcare)"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Domain RAG Clinic (Healthcare) **Category:** Domain & Vertical **Duration:** 55 minutes **Outcome:** Customise a retrieval-augmented system for healthcare knowledge bases with compliance-ready guardrails. ## Why it matters - Healthcare deployments demand HIPAA-aware pipelines with provenance, consent, and auditabilit",
    "content": "# Domain RAG Clinic (Healthcare) **Category:** Domain & Vertical **Duration:** 55 minutes **Outcome:** Customise a retrieval-augmented system for healthcare knowledge bases with compliance-ready guardrails. ## Why it matters - Healthcare deployments demand HIPAA-aware pipelines with provenance, consent, and auditability. - Domain-tuned retrievers outperform generic embeddings when combined with medical ontologies. ## Prerequisites - Access to de-identified healthcare documents (clinical guidelines, FAQs) in . - Running vector store (Supabase pgvector or Qdrant). - [ ](../../05-projects/domain-rag-healthcare/README.md) cloned locally. ## Step-by-step 1. **Prep corpus:** Load the sample dataset from . Chunk using UMLS-aware segmentation (see ). 2. **Embed with domain model:** Use BioClinicalBERT or OpenAI text-embedding-3-large as configured in . Store metadata (source, revision date, PHI flag). 3. **Hybrid search:** Enable BM25 + dense search with reciprocal rank fusion. Evaluate top-10 precision using the scripts provided. 4. **Guardrails:** Configure policy checks from (PHI redaction, contraindication warnings). Integrate with the [RAG Guardrails Fast Track](retrieval-rag-guardrails.md) steps. 5. **Context blueprint:** Update prompts in with clinician persona instructions and safety disclaimers. 6. **Evaluate:** Run or the CLI to measure factual accuracy and coverage. Log metrics in Langfuse. ## Deliverables - Updated with domain-specific embedding + guardrail settings. - Eval report saved to . - Risk log entry covering residual PHI risks (use [Model Risk Review Sprint](governance-model-risk-review.md)). ## References - NIH ClinicalTrials.gov ontology export (August 2025). - BioClinicalBERT 2025 embeddings performance benchmarks. - for extended walkthrough."
  },
  {
    "id": "02-learning-paths\\micro-modules\\retrieval-hybrid-ranking.md",
    "title": "Hybrid Ranking Blueprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/retrieval-hybrid-ranking.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\retrieval-hybrid-ranking.md",
    "headings": [
      {
        "level": 1,
        "text": "Hybrid Ranking Blueprint"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Hybrid Ranking Blueprint **Category:** Retrieval Systems **Duration:** 50 minutes **Outcome:** Deploy a hybrid (sparse + dense + rerank) pipeline tuned for multi-lingual corpora, aligned with 2025 search benchmarks. ## Why it matters - BM25 + dense embeddings still outperform fancy architectures when tuned together. ",
    "content": "# Hybrid Ranking Blueprint **Category:** Retrieval Systems **Duration:** 50 minutes **Outcome:** Deploy a hybrid (sparse + dense + rerank) pipeline tuned for multi-lingual corpora, aligned with 2025 search benchmarks. ## Why it matters - BM25 + dense embeddings still outperform fancy architectures when tuned together. - 2025 rerankers (Cohere Rerank 5, Voyage Fusion) close accuracy gaps with minimal cost. ## Prerequisites - Supabase with pgvector or a managed vector DB (Pinecone, Qdrant). - Access to Cohere Rerank v5 or Voyage Fusion API key. - Dataset of at least 200 docs spanning two languages. ## Step-by-step 1. **Baseline search:** Follow \u00005-projects/vector-search-pgvector.md to index documents with OpenAI text-embedding-3-large or Nomic v1.5 embeddings. 2. **Add BM25:** Enable pg_search (Supabase) or use Typesense for lexical search. Log top-10 results for 5 seed queries. 3. **Blend scores:** Implement reciprocal rank fusion (see \u00006-toolchains/stack-reference.md). Weight dense=0.6, sparse=0.4 to start. 4. **Apply reranker:** Call Cohere Rerank 5 (or Voyage Fusion) on the top 8 results. Capture latency + cost per query. 5. **Evaluate:** Use the [Evaluation Signals Primer](foundations-evaluation-signals.md) to score NDCG, recall@5, and hallucination rate across 30 queries. 6. **Optimise:** Experiment with multilingual embeddings (e.g., Jina Embeddings v2) and reranker thresholds. Document improvements in the logbook. ## Deliverables - Notebook or script showing fusion + rerank code. - Metrics table comparing baseline vs hybrid vs hybrid+rerank. - Recommendation memo for production rollout (drop into \u00003-awesome/portfolio-examples.md template). ## References - Cohere Rerank v5 changelog (June 2025). - Supabase hybrid search recipe (August 2025 blog). - Voyage Fusion evaluation on multilingual Wikipedia (July 2025)."
  },
  {
    "id": "02-learning-paths\\micro-modules\\retrieval-rag-guardrails.md",
    "title": "RAG Guardrails Fast Track",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/retrieval-rag-guardrails.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\retrieval-rag-guardrails.md",
    "headings": [
      {
        "level": 1,
        "text": "RAG Guardrails Fast Track"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# RAG Guardrails Fast Track **Category:** Retrieval Systems **Duration:** 45 minutes **Outcome:** Wrap a RAG endpoint with safety, grounding, and compliance guardrails ready for pilot launch. ## Why it matters - Regulators expect grounded responses, citation tracking, and audit logs even during pilots. - Tooling like G",
    "content": "# RAG Guardrails Fast Track **Category:** Retrieval Systems **Duration:** 45 minutes **Outcome:** Wrap a RAG endpoint with safety, grounding, and compliance guardrails ready for pilot launch. ## Why it matters - Regulators expect grounded responses, citation tracking, and audit logs even during pilots. - Tooling like Guardrails AI, Protect AI, and LangChain Guardrails matured dramatically in 2025. ## Prerequisites - Existing RAG service (REST or GraphQL) from \u00005-projects/rag-on-supabase.md. - Guardrails AI or OpenAI Moderation v2 API key. - Access to a policy checklist (see \u00008-governance/checklists.md). ## Step-by-step 1. **Catalogue risks:** Use \u00008-governance/model-risk.md to list top failure modes (hallucination, PII leakage, policy violation). 2. **Implement content filters:** Add Guardrails AI YAML config or OpenAI Moderation v2 call before returning answers. Log decisions. 3. **Enforce citation threshold:** Require ≥2 supporting references. If missing, return fallback message with confidence = LOW metadata. 4. **Add anti-hallucination check:** Compare answers with retrieved chunks using Semantic Similarity > 0.76 (Sentence Transformers `ll-MiniLM-L6-v3 works). 5. **Audit logging:** Emit JSON logs to Langfuse or your SIEM capturing prompt, retrieved docs, policy verdicts, and user ID. 6. **Drill compliance:** Run the [Model Risk Review Sprint](governance-model-risk-review.md) module to validate guardrail coverage. ## Deliverables - Guardrails config + enforcement middleware committed to your project. - Audit log sample annotated with pass/fail outcomes. - Updated DPIA or risk register entry. ## References - Guardrails AI 2025 policy pack launch notes. - Protect AI Playbook for RAG pipelines (April 2025). - \u00007-evaluation/metrics.md for grounding metrics."
  },
  {
    "id": "02-learning-paths\\micro-modules\\storytelling-adoption-metrics.md",
    "title": "Adoption & ROI Metrics",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/storytelling-adoption-metrics.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\storytelling-adoption-metrics.md",
    "headings": [
      {
        "level": 1,
        "text": "Adoption & ROI Metrics"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Adoption & ROI Metrics **Category:** Storytelling & Adoption **Duration:** 40 minutes **Outcome:** Define a metrics stack that links AI capability usage to business value. ## Why it matters - AI programs in 2025 succeed when tied to measurable ROI (hours saved, revenue, risk reduction). - Product + finance teams need",
    "content": "# Adoption & ROI Metrics **Category:** Storytelling & Adoption **Duration:** 40 minutes **Outcome:** Define a metrics stack that links AI capability usage to business value. ## Why it matters - AI programs in 2025 succeed when tied to measurable ROI (hours saved, revenue, risk reduction). - Product + finance teams need shared dashboards. ## Prerequisites - Access to product analytics (Amplitude, Mixpanel, internal warehouse) or support logs. - Baseline telemetry from [Langfuse Telemetry Sprints](../micro-modules/operations-langfuse-telemetry.md). - Collaboration with finance/ops partner. ## Step-by-step 1. **Map funnel:** Identify user journey stages (activation, task completion, escalation). Align with \u00002-learning-paths/100-hour-ai-architect.md deliverables. 2. **Select metrics:** Choose 4 metrics: adoption (weekly active users), productivity (tasks per user), quality (eval pass rate), risk (escalations per 100 tasks). 3. **Instrument events:** Add tracking to dashboard/workflows (e.g., log ask_completed, eval_passed). Route to analytics + Langfuse tags. 4. **Calculate ROI:** Collaborate with finance to estimate savings/revenue uplift. Update \u00003-awesome/portfolio-examples.md with case study snippet. 5. **Build dashboard:** Use Metabase, PowerBI, or Looker to visualise metrics. Include trendlines + target thresholds. 6. **Share cadence:** Present metrics in the [Executive Narrative Builder](storytelling-exec-brief.md) and log action items. ## Deliverables - Metrics definition doc (KPIs, formulas, owners). - Dashboard screenshot or share link. - ROI summary paragraph for stakeholder updates. ## References - Gartner AI Product Metrics 2025 report. - Airbnb AI adoption case study (April 2025). - 15-workflows/ai-briefing.md for reporting cadence."
  },
  {
    "id": "02-learning-paths\\micro-modules\\storytelling-exec-brief.md",
    "title": "Executive Narrative Builder",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/storytelling-exec-brief.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\storytelling-exec-brief.md",
    "headings": [
      {
        "level": 1,
        "text": "Executive Narrative Builder"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Executive Narrative Builder **Category:** Storytelling & Adoption **Duration:** 35 minutes **Outcome:** Produce a board-ready narrative that links AI delivery progress to business outcomes. ## Why it matters - Leaders need crisp proof of value, risks, and next bets each week. - Storytelling drives adoption and budget",
    "content": "# Executive Narrative Builder **Category:** Storytelling & Adoption **Duration:** 35 minutes **Outcome:** Produce a board-ready narrative that links AI delivery progress to business outcomes. ## Why it matters - Leaders need crisp proof of value, risks, and next bets each week. - Storytelling drives adoption and budget for AI programs. ## Prerequisites - Metrics snapshot from Langfuse or your telemetry stack. - Recent evaluation report (see [Evaluation Signals Primer](../micro-modules/foundations-evaluation-signals.md)). - Brand Voice guidance from BRAND-VOICE.md. ## Step-by-step 1. **Gather signals:** Extract top metrics (latency, cost, accuracy, adoption) and add qualitative wins + blockers. 2. **Structure brief:** Use the outline in 15-workflows/ai-briefing.md (headline, metrics pulse, highlights, blockers, next experiments). 3. **Add visuals:** Embed hero image (`ssets/ai-architect-campus.png) and relevant dashboards/screenshot. 4. **Highlight governance:** Summarise risk status + mitigations from [Model Risk Review Sprint](../micro-modules/governance-model-risk-review.md). 5. **Call to action:** Define decisions needed (budget, approvals) and deadlines. 6. **Distribute:** Package as PDF or Loom video; share via exec channel and archive in \u00009-articles/drafts/. ## Deliverables - 1-page executive brief (PDF/Markdown) referencing metrics + risks. - Key slide or graphic for town halls. - Log entry in the Learning Logbook capturing feedback. ## References - 2025 McKinsey AI adoption scorecard. - Stripe Radar storytelling examples (2024-2025). - \u00009-articles/templates/keynote.md for extended decks."
  },
  {
    "id": "02-learning-paths\\professional.md",
    "title": "Professional Path (6 Weeks)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/professional.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\professional.md",
    "headings": [
      {
        "level": 1,
        "text": "Professional Path (6 Weeks)"
      },
      {
        "level": 2,
        "text": "Structure Overview"
      },
      {
        "level": 2,
        "text": "Detailed Plan"
      },
      {
        "level": 2,
        "text": "Mentorship Moments"
      },
      {
        "level": 2,
        "text": "Portfolio & Promotion"
      }
    ],
    "excerpt": "# Professional Path (6 Weeks) Ideal for architects balancing delivery and leadership responsibilities (~12-15 hours/week). ## Structure Overview - **Weeks 1-2:** Solidify retrieval + agent systems while setting baselines for evals. - **Weeks 3-4:** Embed governance, risk, and cost practices into day-to-day operations. ",
    "content": "# Professional Path (6 Weeks) Ideal for architects balancing delivery and leadership responsibilities (~12-15 hours/week). ## Structure Overview - **Weeks 1-2:** Solidify retrieval + agent systems while setting baselines for evals. - **Weeks 3-4:** Embed governance, risk, and cost practices into day-to-day operations. - **Weeks 5-6:** Specialise by domain and create teaching material for your org. ## Detailed Plan | Week | Theme | Focus Areas | Deliverables | | --- | --- | --- | --- | | 1 | Retrieval Excellence | Hybrid search, chunking, multi-tenant design | Search service blueprint + benchmark report | | 2 | Agents & Automation | Tool choreography, LangGraph patterns, failure recovery | Agent flow deck + incident playbook | | 3 | Observability & Evals | Langfuse dashboards, promptfoo CI, data drift alerts | Eval summary + trace library | | 4 | Governance in Practice | DPIA updates, policy mapping, access reviews | Governance cockpit doc + risk board | | 5 | Industry Playbooks | Tailor patterns for a chosen vertical, map integrations | Domain playbook + partner alignment memo | | 6 | Teach & Scale | Build enablement assets, mentor peers, host workshop | Workshop agenda, recording, updated poster | ## Mentorship Moments - Add one [Micro-Learning Atlas](micro-learning.md) module per week to keep skills sharp (e.g., guardrails during Week 3, storytelling during Week 6). - Pair each week with a 30-minute mentor sync using the questions in . - Swap eval results and guardrail ideas with peers via . ## Portfolio & Promotion - Capture new visuals (screenshots, hero art) and append to + as you progress. - Maintain a change log in (create if needed) highlighting wins, metrics, and stories. - When ready, submit a PR to share your vertical playbook or workflow so others can build on it."
  },
  {
    "id": "02-learning-paths\\self-assessment.md",
    "title": "AI Architect Self-Assessment",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/self-assessment.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\self-assessment.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Architect Self-Assessment"
      },
      {
        "level": 2,
        "text": "Profile"
      },
      {
        "level": 2,
        "text": "Current Confidence (rate 1-5)"
      },
      {
        "level": 2,
        "text": "Strengths to Amplify"
      },
      {
        "level": 2,
        "text": "Gaps to Close"
      },
      {
        "level": 2,
        "text": "Learning Objectives (Top 3)"
      },
      {
        "level": 2,
        "text": "Success Signals"
      },
      {
        "level": 2,
        "text": "Support & Resources"
      },
      {
        "level": 2,
        "text": "Reflection Placeholder"
      }
    ],
    "excerpt": "# AI Architect Self-Assessment Capture a snapshot before starting any path, then revisit every 30 days. Copy this file, fill in the prompts, and store it alongside your learning journal. ## Profile - Name / Role / Team - Primary domain (product, data, infra, research, strategy) - Time available per week ## Current Conf",
    "content": "# AI Architect Self-Assessment Capture a snapshot before starting any path, then revisit every 30 days. Copy this file, fill in the prompts, and store it alongside your learning journal. ## Profile - Name / Role / Team - Primary domain (product, data, infra, research, strategy) - Time available per week ## Current Confidence (rate 1-5) | Area | Score | Notes | | --- | --- | --- | | Value framing & storytelling | | | | Retrieval & search systems | | | | Agents & orchestration | | | | Observability & evaluation | | | | Governance & policy collaboration | | | | Shipping to production | | | ## Strengths to Amplify - - ## Gaps to Close - - ## Learning Objectives (Top 3) 1. 2. 3. ## Success Signals - Example metric improvements or artifacts you want by the end of the program. - Stakeholder feedback you need. - Behaviours you want to see in your day-to-day work. ## Support & Resources - Peer / mentor / manager check-ins - Repos, talks, or courses to pair with this playbook - Risks or blockers that might slow progress ## Reflection Placeholder Revisit at the end of each week: What moved? What stayed stuck? What will you change?"
  },
  {
    "id": "03-awesome\\awesome-agents.md",
    "title": "Awesome Agents",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-agents.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-agents.md",
    "headings": [],
    "excerpt": "# Awesome Agents | Link | Why it matters | |---|---| | [microsoft/autogen](https://github.com/microsoft/autogen) | Multi-agent framework with tool support | | [joaomdmoura/crewai](https://github.com/joaomdmoura/crewai) | Agent teams and orchestration | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgrap",
    "content": "# Awesome Agents | Link | Why it matters | |---|---| | [microsoft/autogen](https://github.com/microsoft/autogen) | Multi-agent framework with tool support | | [joaomdmoura/crewai](https://github.com/joaomdmoura/crewai) | Agent teams and orchestration | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph) | Deterministic agent graphs for complex flows |"
  },
  {
    "id": "03-awesome\\awesome-aggregators.md",
    "title": "Awesome Aggregators (Meta Lists)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-aggregators.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-aggregators.md",
    "headings": [
      {
        "level": 1,
        "text": "Awesome Aggregators (Meta Lists)"
      }
    ],
    "excerpt": "# Awesome Aggregators (Meta Lists) High-signal GitHub repos that aggregate the best resources. - sindresorhus/awesome — The canonical index of curated “awesome” lists across topics - awesome-machine-learning/awesome-machine-learning — Curated machine learning resources and libraries - visenger/awesome-mlops — Curated M",
    "content": "# Awesome Aggregators (Meta Lists) High-signal GitHub repos that aggregate the best resources. - sindresorhus/awesome — The canonical index of curated “awesome” lists across topics - awesome-machine-learning/awesome-machine-learning — Curated machine learning resources and libraries - visenger/awesome-mlops — Curated MLOps tools, articles, and best practices - eugeneyan/applied-ml — Applied ML reading list (papers, blog posts, case studies) - papers-we-love/papers-we-love — Foundational computer science and systems papers - f/awesome-chatgpt-prompts — Community prompts and examples for ChatGPT-style models - e2b-dev/awesome-ai-agents — Curated list of AI agent frameworks and tooling Notes - Prefer recent, maintained lists; check stars and last update. - When adding links from these lists, include 1–2 lines on why they matter."
  },
  {
    "id": "03-awesome\\awesome-evals.md",
    "title": "Awesome Evals",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-evals.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-evals.md",
    "headings": [],
    "excerpt": "# Awesome Evals | Link | Why it matters | |---|---| | [langfuse/langfuse](https://github.com/langfuse/langfuse) | Traces, evaluations, and cost tracking | | [Arize-ai/phoenix](https://github.com/Arize-ai/phoenix) | Model and LLM analysis plus evaluation | | [promptfoo/promptfoo](https://github.com/promptfoo/promptfoo) ",
    "content": "# Awesome Evals | Link | Why it matters | |---|---| | [langfuse/langfuse](https://github.com/langfuse/langfuse) | Traces, evaluations, and cost tracking | | [Arize-ai/phoenix](https://github.com/Arize-ai/phoenix) | Model and LLM analysis plus evaluation | | [promptfoo/promptfoo](https://github.com/promptfoo/promptfoo) | Prompt evaluations in CI workflows | | [openai/evals](https://github.com/openai/evals) | Evaluation framework with reusable datasets | | [microsoft/promptflow](https://github.com/microsoft/promptflow) | Flow evaluation, orchestration, and tracking |"
  },
  {
    "id": "03-awesome\\awesome-llms.md",
    "title": "Awesome Llms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-llms.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-llms.md",
    "headings": [],
    "excerpt": "# Awesome LLMs | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Core models and tokenizers for NLP and LLM work | | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | Run models locally on CPU and edge devices | | [vllm-project/vllm](https://gi",
    "content": "# Awesome LLMs | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Core models and tokenizers for NLP and LLM work | | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | Run models locally on CPU and edge devices | | [vllm-project/vllm](https://github.com/vllm-project/vllm) | High-throughput, low-latency model serving | | [ollama/ollama](https://github.com/ollama/ollama) | Local model runtime and packaging | | [openai/openai-python](https://github.com/openai/openai-python) | Official Python SDK for OpenAI APIs | | [openai/openai-cookbook](https://github.com/openai/openai-cookbook) | Production patterns and practical examples |"
  },
  {
    "id": "03-awesome\\awesome-mlops.md",
    "title": "Awesome Mlops",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-mlops.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-mlops.md",
    "headings": [],
    "excerpt": "# Awesome MLOps | Link | Why it matters | |---|---| | [ray-project/ray](https://github.com/ray-project/ray) | Distributed compute for Python and LLM workloads | | [PrefectHQ/prefect](https://github.com/PrefectHQ/prefect) | Modern workflow orchestration | | [temporalio/sdk-typescript](https://github.com/temporalio/sdk-t",
    "content": "# Awesome MLOps | Link | Why it matters | |---|---| | [ray-project/ray](https://github.com/ray-project/ray) | Distributed compute for Python and LLM workloads | | [PrefectHQ/prefect](https://github.com/PrefectHQ/prefect) | Modern workflow orchestration | | [temporalio/sdk-typescript](https://github.com/temporalio/sdk-typescript) | Durable workflows with strong guarantees | | [flyteorg/flyte](https://github.com/flyteorg/flyte) | ML orchestrator built for scale | | [dagster-io/dagster](https://github.com/dagster-io/dagster) | Data and ML orchestration | | [mlflow/mlflow](https://github.com/mlflow/mlflow) | Experiment tracking and model registry | | [bentoml/BentoML](https://github.com/bentoml/BentoML) | Packaging and serving toolkit |"
  },
  {
    "id": "03-awesome\\awesome-rag.md",
    "title": "Awesome Rag",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-rag.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-rag.md",
    "headings": [],
    "excerpt": "# Awesome RAG | Link | Why it matters | |---|---| | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | Composable primitives for RAG and tool use | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph) | Graph and state orchestration for RAG and agents | | [run-llama/llama_index](http",
    "content": "# Awesome RAG | Link | Why it matters | |---|---| | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | Composable primitives for RAG and tool use | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph) | Graph and state orchestration for RAG and agents | | [run-llama/llama_index](https://github.com/run-llama/llama_index) | Data framework for indexing and retrieval | | [explodinggradients/ragas](https://github.com/explodinggradients/ragas) | Evaluate RAG quality and faithfulness | | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Vector similarity search inside Postgres | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Vector database with hybrid search |"
  },
  {
    "id": "03-awesome\\awesome-vector-dbs.md",
    "title": "Awesome Vector Dbs",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-vector-dbs.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-vector-dbs.md",
    "headings": [],
    "excerpt": "# Awesome Vector Databases | Link | Why it matters | |---|---| | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Postgres extension for vector similarity | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Hybrid",
    "content": "# Awesome Vector Databases | Link | Why it matters | |---|---| | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Postgres extension for vector similarity | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Hybrid search with modular extensions | | [milvus-io/milvus](https://github.com/milvus-io/milvus) | Distributed vector database | | [chroma-core/chroma](https://github.com/chroma-core/chroma) | Lightweight local vector store |"
  },
  {
    "id": "03-awesome\\portfolio-examples.md",
    "title": "Portfolio Examples to Model",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/portfolio-examples.md",
    "section": "Awesome",
    "path": "03-awesome\\portfolio-examples.md",
    "headings": [
      {
        "level": 1,
        "text": "Portfolio Examples to Model"
      },
      {
        "level": 2,
        "text": "Product Stories"
      },
      {
        "level": 2,
        "text": "Technical Deep Dives"
      },
      {
        "level": 2,
        "text": "Suggested Prompts"
      },
      {
        "level": 2,
        "text": "Assets to Capture"
      },
      {
        "level": 2,
        "text": "Share & Iterate"
      }
    ],
    "excerpt": "# Portfolio Examples to Model Use these prompts + resources when shaping your own case studies and demos. ## Product Stories - Stripe Radar launch memo (highlight alignment of metrics + narrative) - GitHub Copilot case study (focus on developer workflow impact) - Intercom Fin AI assistant release notes (clear guardrail",
    "content": "# Portfolio Examples to Model Use these prompts + resources when shaping your own case studies and demos. ## Product Stories - Stripe Radar launch memo (highlight alignment of metrics + narrative) - GitHub Copilot case study (focus on developer workflow impact) - Intercom Fin AI assistant release notes (clear guardrail messaging) ## Technical Deep Dives - OpenAI retrieval plugin walkthrough - Netflix personalization architecture notes - Datadog incident retrospectives (observability framing) ## Suggested Prompts 1. \"Draft a case study using the structure: problem, approach, architecture, metrics, next steps.\" 2. \"Summarise the top 3 stakeholder questions likely to appear for this launch.\" 3. \"Translate the architecture into a slide-friendly diagram narrative.\" ## Assets to Capture - Hero visual ( ) - Mentor persona ( ) - Curriculum poster ( ) - Live product screenshots (see ) ## Share & Iterate - Store drafts in - Use to reflect on feedback - Convert your best story into a talk or webinar outline"
  },
  {
    "id": "04-templates\\bom-template.md",
    "title": "Bill of Materials Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/bom-template.md",
    "section": "Templates",
    "path": "04-templates\\bom-template.md",
    "headings": [
      {
        "level": 1,
        "text": "Bill of Materials Template"
      }
    ],
    "excerpt": "# Bill of Materials Template - Services & SKUs - Usage assumptions - Monthly estimate - Notes & tradeoffs",
    "content": "# Bill of Materials Template - Services & SKUs - Usage assumptions - Monthly estimate - Notes & tradeoffs"
  },
  {
    "id": "04-templates\\discovery-questions.md",
    "title": "Discovery Questions Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/discovery-questions.md",
    "section": "Templates",
    "path": "04-templates\\discovery-questions.md",
    "headings": [
      {
        "level": 1,
        "text": "Discovery Questions Template"
      }
    ],
    "excerpt": "# Discovery Questions Template - Business goals, constraints, KPIs - Data sources, privacy, compliance - Users, workflows, SLAs - Risks and mitigations",
    "content": "# Discovery Questions Template - Business goals, constraints, KPIs - Data sources, privacy, compliance - Users, workflows, SLAs - Risks and mitigations"
  },
  {
    "id": "04-templates\\solution-doc.md",
    "title": "Solution Document Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/solution-doc.md",
    "section": "Templates",
    "path": "04-templates\\solution-doc.md",
    "headings": [
      {
        "level": 1,
        "text": "Solution Document Template"
      }
    ],
    "excerpt": "# Solution Document Template - Problem & value - Architecture overview - Data flows & components - Security & compliance - Rollout & costs",
    "content": "# Solution Document Template - Problem & value - Architecture overview - Data flows & components - Security & compliance - Rollout & costs"
  },
  {
    "id": "04-templates\\technical-architecture.md",
    "title": "Technical Architecture Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/technical-architecture.md",
    "section": "Templates",
    "path": "04-templates\\technical-architecture.md",
    "headings": [
      {
        "level": 1,
        "text": "Technical Architecture Template"
      }
    ],
    "excerpt": "# Technical Architecture Template - Context & assumptions - Component diagram - Data model - Scaling & SLOs - Observability",
    "content": "# Technical Architecture Template - Context & assumptions - Component diagram - Data model - Scaling & SLOs - Observability"
  },
  {
    "id": "04-templates\\workshop-agenda.md",
    "title": "Workshop Agenda Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/workshop-agenda.md",
    "section": "Templates",
    "path": "04-templates\\workshop-agenda.md",
    "headings": [
      {
        "level": 1,
        "text": "Workshop Agenda Template"
      }
    ],
    "excerpt": "# Workshop Agenda Template - Intro & goals - Discovery questions - Architecture co-design - Risks & next steps",
    "content": "# Workshop Agenda Template - Intro & goals - Discovery questions - Architecture co-design - Risks & next steps"
  },
  {
    "id": "05-projects\\100-projects.md",
    "title": "100 Projects for AI Architects (Starter Set)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/100-projects.md",
    "section": "Projects",
    "path": "05-projects\\100-projects.md",
    "headings": [
      {
        "level": 1,
        "text": "100 Projects for AI Architects (Starter Set)"
      }
    ],
    "excerpt": "# 100 Projects for AI Architects (Starter Set) Practical builds to master architecture by shipping. Sorted by theme; each project lists difficulty, estimated time, and links to guides or repos. Contribute: PR new projects with a short description, difficulty, est. hours, and links. Keep impact high and scope clear. | #",
    "content": "# 100 Projects for AI Architects (Starter Set) Practical builds to master architecture by shipping. Sorted by theme; each project lists difficulty, estimated time, and links to guides or repos. Contribute: PR new projects with a short description, difficulty, est. hours, and links. Keep impact high and scope clear. | # | Project | Theme | Difficulty | Est. Hours | Links | |---|---|---|---|---|---| | 1 | RAG on Supabase with Citations | RAG | Beginner | 8–12 | [Guide](rag-on-supabase.md) • [pgvector](https://github.com/pgvector/pgvector) | | 2 | Hybrid Search (BM25 + Vectors) | Retrieval | Intermediate | 6–10 | [Concepts](../03-awesome/awesome-vector-dbs.md) | | 3 | Chunking Strategies Benchmark | Retrieval | Intermediate | 6–10 | [RAGAS](https://github.com/explodinggradients/ragas) | | 4 | Evals Harness (Faithfulness, Coverage) | Evaluation | Intermediate | 6–10 | [Langfuse](https://github.com/langfuse/langfuse) | | 5 | Cost Guardrails & Caching Layer | Ops | Intermediate | 6–10 | [openai-cookbook](https://github.com/openai/openai-cookbook) | | 6 | Agent with Tools (Web + DB) | Agents | Intermediate | 10–14 | [LangGraph](https://github.com/langchain-ai/langgraph) | | 7 | Multi-Agent Workflow (Reviewer Loops) | Agents | Advanced | 12–18 | [AutoGen](https://github.com/microsoft/autogen) | | 8 | Observability Dash (Traces, Costs) | Observability | Intermediate | 6–10 | [Langfuse](https://github.com/langfuse/langfuse) | | 9 | Prompt Registry + Versioning | MLOps | Intermediate | 6–10 | [promptfoo](https://github.com/promptfoo/promptfoo) | | 10 | Model Serving with vLLM | Serving | Intermediate | 8–12 | [vLLM](https://github.com/vllm-project/vllm) | | 11 | Local Inference Prototype | Edge | Beginner | 4–8 | [llama.cpp](https://github.com/ggerganov/llama.cpp) | | 12 | Orchestration with Temporal | Orchestration | Intermediate | 8–12 | [Temporal TS](https://github.com/temporalio/sdk-typescript) | | 13 | Vector DB Benchmarks (Qdrant/Weaviate/Milvus) | Retrieval | Advanced |"
  },
  {
    "id": "05-projects\\agentic-saas-planner.md",
    "title": "Project: Agentic SaaS Planner",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/agentic-saas-planner.md",
    "section": "Projects",
    "path": "05-projects\\agentic-saas-planner.md",
    "headings": [
      {
        "level": 1,
        "text": "Project: Agentic SaaS Planner"
      },
      {
        "level": 2,
        "text": "Steps"
      },
      {
        "level": 2,
        "text": "Acceptance Criteria"
      },
      {
        "level": 2,
        "text": "Stretch"
      }
    ],
    "excerpt": "# Project: Agentic SaaS Planner Goal: Use a Planner → Worker → Reviewer swarm to produce a product plan, architecture, and build plan for a simple SaaS idea. ## Steps 1) Run the example 2) Add a agent to block release if risks are high 3) Extend output: Bill of Materials, API list, day-by-day build plan 4) Optional: St",
    "content": "# Project: Agentic SaaS Planner Goal: Use a Planner → Worker → Reviewer swarm to produce a product plan, architecture, and build plan for a simple SaaS idea. ## Steps 1) Run the example 2) Add a agent to block release if risks are high 3) Extend output: Bill of Materials, API list, day-by-day build plan 4) Optional: Streamlit UI button to export a PDF brief ## Acceptance Criteria - Outputs include target user, jobs-to-be-done, key features, risks, MVP outline - Technical plan includes components, data flow, APIs, eval plan - Reviewer summary is clear and actionable ## Stretch - Integrate LiteLLM to compare models (OpenAI vs Anthropic) - Add a test harness that verifies the plan includes eval metrics"
  },
  {
    "id": "05-projects\\creator-evals\\README.md",
    "title": "Creator Evaluation Harness",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/creator-evals/README.md",
    "section": "Projects",
    "path": "05-projects\\creator-evals\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Evaluation Harness"
      },
      {
        "level": 2,
        "text": "What's Inside"
      },
      {
        "level": 2,
        "text": "Quick Start"
      },
      {
        "level": 1,
        "text": "Reuse the shared automation tooling"
      },
      {
        "level": 2,
        "text": "Extend"
      },
      {
        "level": 2,
        "text": "Suggested Workflow"
      },
      {
        "level": 2,
        "text": "Related Resources"
      }
    ],
    "excerpt": "# Creator Evaluation Harness Opinionated scaffold for scoring AI-assisted creator workflows. It extends with creator-specific scenarios, datasets, and reporting conventions. ## What's Inside - � Promptfoo-ready creator prompts covering hooks, performance summaries, and next-step experiments. - � Sample analytics export",
    "content": "# Creator Evaluation Harness Opinionated scaffold for scoring AI-assisted creator workflows. It extends with creator-specific scenarios, datasets, and reporting conventions. ## What's Inside - � Promptfoo-ready creator prompts covering hooks, performance summaries, and next-step experiments. - � Sample analytics export used during evaluations. - � Drop evaluation outputs and retro notes here (gitignored by default). ## Quick Start Reports land in within the eval-automation project. Copy the relevant JSON/HTML into if you want creator-specific archives. ## Extend - Append new creator scenarios (e.g., newsletter variants, podcast scripts) to . - Mirror analytics exports from your BI stack into for reproducible tests. - Sync metrics with [ ](../../02-learning-paths/micro-modules/creator-analytics-feedback-loop.md) as you build longer feedback loops. ## Suggested Workflow 1. Run micro-modules: orchestration, scorecards, analytics loop. 2. Update Promptfoo configs to include creator guardrails (tone, compliance, CTA strength). 3. Execute evaluations pre- and post-campaign; log results in . 4. Share wins in the [Creator Studio Launch Guide](../../09-articles/creator-studio-launch-guide.md). ## Related Resources - [ ](../../01-design-patterns/creator-studio-automation.md) - [ ](../eval-automation/README.md) - [ ](../../02-learning-paths/micro-modules/creator-evaluation-scorecards.md)"
  },
  {
    "id": "05-projects\\domain-rag-healthcare\\README.md",
    "title": "Domain RAG Healthcare Blueprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/domain-rag-healthcare/README.md",
    "section": "Projects",
    "path": "05-projects\\domain-rag-healthcare\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Domain RAG Healthcare Blueprint"
      },
      {
        "level": 2,
        "text": "What's Inside"
      },
      {
        "level": 2,
        "text": "Quick Start"
      },
      {
        "level": 2,
        "text": "Configure"
      },
      {
        "level": 2,
        "text": "Evaluate & Share"
      },
      {
        "level": 2,
        "text": "Extend"
      }
    ],
    "excerpt": "# Domain RAG Healthcare Blueprint Opinionated scaffolding for building a healthcare-ready retrieval augmented generation (RAG) service tailored to AI Architect Health Alliance (AIHA) clinical teams. ## What's Inside - � AIHA hypertension, diabetes, telehealth, kidney, and medication safety playbooks. - � end-to-end ing",
    "content": "# Domain RAG Healthcare Blueprint Opinionated scaffolding for building a healthcare-ready retrieval augmented generation (RAG) service tailored to AI Architect Health Alliance (AIHA) clinical teams. ## What's Inside - � AIHA hypertension, diabetes, telehealth, kidney, and medication safety playbooks. - � end-to-end ingestion, embedding, and retrieval workflows with environment-tunable models. - � UMLS-aware chunker with sentence fallback for lightweight deployments. - � PHI redaction, contraindication alerts, and escalation triggers. - � CLI wrapper to score responses against and push metrics to Langfuse. - � drop evaluation outputs, risk reviews, and decision logs for audit. ## Quick Start ## Configure Create from and supply: Override or if you point ingest at alternate datasets. ## Evaluate & Share - Run after ingesting new knowledge. - Store summaries in (e.g., ). - Feed insights back into the [Domain RAG Clinic (Healthcare)](../../02-learning-paths/micro-modules/retrieval-domain-rag-healthcare.md) micro-module. ## Extend - Drop additional CSVs in (e.g., policy memos, care coordination scripts) and set to the combined export. - Enrich with terminology services (SNOMED, LOINC) for enterprise-grade retrieval. - Pair with to capture post-release retrospectives and risk mitigations."
  },
  {
    "id": "05-projects\\eval-automation\\README.md",
    "title": "Evaluation Automation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/eval-automation/README.md",
    "section": "Projects",
    "path": "05-projects\\eval-automation\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Automation"
      },
      {
        "level": 2,
        "text": "What's Inside"
      },
      {
        "level": 2,
        "text": "Quick Start"
      },
      {
        "level": 2,
        "text": "Live Eval Checklist"
      },
      {
        "level": 2,
        "text": "Wire It Into CI"
      },
      {
        "level": 2,
        "text": "Reports & Storytelling"
      },
      {
        "level": 2,
        "text": "Creator Ops Integration"
      }
    ],
    "excerpt": "# Evaluation Automation Scaffolding to run automated evaluation suites via Promptfoo and Langfuse with AIHA-specific scenarios. ## What's Inside - - accuracy, toxicity, and custom citation guardrails. - - curated prompts covering Langfuse telemetry, evaluation automation, and AIHA care protocols. - - orchestrates Promp",
    "content": "# Evaluation Automation Scaffolding to run automated evaluation suites via Promptfoo and Langfuse with AIHA-specific scenarios. ## What's Inside - - accuracy, toxicity, and custom citation guardrails. - - curated prompts covering Langfuse telemetry, evaluation automation, and AIHA care protocols. - - orchestrates Promptfoo runs and writes timestamped JSON/HTML reports. - - quickcheck for provider API keys before running live evaluations. - - GitHub Action template to block merges when guardrails regress. - - drop-off point for generated artefacts and exec-ready summaries. - - template for provider keys and optional Promptfoo overrides. ## Quick Start If your Promptfoo binary lives outside of , set in to the absolute path. ## Live Eval Checklist 1. Populate with production-ready and values. 2. Run ; address any missing variables before continuing. 3. (Optional) Override if you manage multiple Promptfoo installs. 4. Execute to generate live-model reports. When provider keys are absent, the script falls back to ungraded summaries tagged as . ## Wire It Into CI 1. Copy into . 2. Store / (and any LLM provider keys) in repository secrets. 3. Tune thresholds or providers inside to reflect your policies. ## Reports & Storytelling Generated HTML/JSON reports land in . Attach highlights to the [Executive Narrative Builder](../../02-learning-paths/micro-modules/storytelling-exec-brief.md) or share in leadership forums. Offline runs remain in the same directory but clearly indicate the fallback status. Complete the [Evaluation Automation Pipeline](../../02-learning-paths/micro-modules/evaluation-automation-pipeline.md) micro-module to unlock advanced workflows. ## Creator Ops Integration - Run creator-specific test sets from [ ](../creator-evals/README.md) with . - Schedule micro-modules for your ops team: [Creator Content Orchestration Sprint](../../02-learning-paths/micro-modules/creator-content-orchestration.md), [Creator Evaluation Scorecards](../../02-learning-paths/micro-"
  },
  {
    "id": "05-projects\\evals-langfuse.md",
    "title": "Evals Langfuse",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/evals-langfuse.md",
    "section": "Projects",
    "path": "05-projects\\evals-langfuse.md",
    "headings": [],
    "excerpt": "# Project: Evals with Langfuse - Instrument API endpoints with traces, latency, and cost spans - Create an evaluation dataset (Q/A with citations) - Implement scoring for faithfulness and helpfulness - Track versions and regressions with CI that fails on quality drops",
    "content": "# Project: Evals with Langfuse - Instrument API endpoints with traces, latency, and cost spans - Create an evaluation dataset (Q/A with citations) - Implement scoring for faithfulness and helpfulness - Track versions and regressions with CI that fails on quality drops"
  },
  {
    "id": "05-projects\\rag-on-supabase.md",
    "title": "Rag On Supabase",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/rag-on-supabase.md",
    "section": "Projects",
    "path": "05-projects\\rag-on-supabase.md",
    "headings": [],
    "excerpt": "# Project: RAG on Supabase + OpenAI (Step-by-Step) 1. **Setup** - Create a Supabase project (EU recommended) and enable - Create tables: , - Secure OpenAI and YouTube API keys 2. **Ingest** - Fetch video transcripts, clean them, and segment into 200-400 token chunks at semantic boundaries - Compute embeddings (small mo",
    "content": "# Project: RAG on Supabase + OpenAI (Step-by-Step) 1. **Setup** - Create a Supabase project (EU recommended) and enable - Create tables: , - Secure OpenAI and YouTube API keys 2. **Ingest** - Fetch video transcripts, clean them, and segment into 200-400 token chunks at semantic boundaries - Compute embeddings (small model) and store vectors in 3. **Retrieval API** - Combine vector similarity with keyword search and re-ranking - Return chunks with timestamps and resource metadata 4. **Tutor Endpoint** - Compose a system prompt with rules (require citations, abstain on low confidence) - Retrieve top-k chunks, call OpenAI, and return answers with citations 5. **Evals and Observability** - Add Langfuse traces and build a small evaluation dataset (questions plus expected sources) - Track faithfulness and citation coverage 6. **Hardening** - Apply rate limits per user or tier and cache frequent queries - Add cost guardrails and alerts"
  },
  {
    "id": "05-projects\\vector-search-pgvector.md",
    "title": "Vector Search Pgvector",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/vector-search-pgvector.md",
    "section": "Projects",
    "path": "05-projects\\vector-search-pgvector.md",
    "headings": [],
    "excerpt": "# Project: Vector Search with pgvector - Install the extension and prepare schema - Choose embedding model; test cosine versus inner product - Tune indexing (IVFFlat) and storage parameters - Benchmark recall and latency trade-offs, including hybrid search",
    "content": "# Project: Vector Search with pgvector - Install the extension and prepare schema - Choose embedding model; test cosine versus inner product - Tune indexing (IVFFlat) and storage parameters - Benchmark recall and latency trade-offs, including hybrid search"
  },
  {
    "id": "06-toolchains\\agentic-swarms-stack.md",
    "title": "Reference Stack: Agentic Code Swarms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/06-toolchains/agentic-swarms-stack.md",
    "section": "Toolchains",
    "path": "06-toolchains\\agentic-swarms-stack.md",
    "headings": [
      {
        "level": 1,
        "text": "Reference Stack: Agentic Code Swarms"
      },
      {
        "level": 2,
        "text": "Runtime"
      },
      {
        "level": 2,
        "text": "Providers"
      },
      {
        "level": 2,
        "text": "Data & Memory"
      },
      {
        "level": 2,
        "text": "Observability"
      },
      {
        "level": 2,
        "text": "Evals & Guardrails"
      },
      {
        "level": 2,
        "text": "Deployment"
      },
      {
        "level": 2,
        "text": "Security & Costs"
      }
    ],
    "excerpt": "# Reference Stack: Agentic Code Swarms ## Runtime - Python 3.11, , , - Orchestrations: sequential, P–W–R, round‑robin, map‑reduce - UI: Streamlit Explorer for learning and demos ## Providers - LiteLLM routing (OpenAI, Anthropic, etc.) - Offline mock provider for classrooms ## Data & Memory - Vector DB: pgvector or Qdra",
    "content": "# Reference Stack: Agentic Code Swarms ## Runtime - Python 3.11, , , - Orchestrations: sequential, P–W–R, round‑robin, map‑reduce - UI: Streamlit Explorer for learning and demos ## Providers - LiteLLM routing (OpenAI, Anthropic, etc.) - Offline mock provider for classrooms ## Data & Memory - Vector DB: pgvector or Qdrant (optional for advanced tools) - Cache: Redis for short‑term memory (optional) ## Observability - Tracing: OpenTelemetry export from agents (future) - Logs: structured JSON logs per agent step ## Evals & Guardrails - Metrics: task success, latency, cost - Harness: deterministic prompts + expected checks ## Deployment - Dockerfile provided; run Streamlit on 8501 - Suggested: Fly.io/Render for demos; Kubernetes for internal portals ## Security & Costs - Secrets via or platform secret store - Budget limits per run; circuit breakers on failure counts"
  },
  {
    "id": "06-toolchains\\stack-reference.md",
    "title": "Stack Reference",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/06-toolchains/stack-reference.md",
    "section": "Toolchains",
    "path": "06-toolchains\\stack-reference.md",
    "headings": [
      {
        "level": 1,
        "text": "Stack Reference"
      }
    ],
    "excerpt": "# Stack Reference - Frontend: Next.js, Tailwind - API: Node/Express - DB: Supabase (Postgres + pgvector) - AI: OpenAI (with OSS fallback) - Obs: Sentry, Langfuse",
    "content": "# Stack Reference - Frontend: Next.js, Tailwind - API: Node/Express - DB: Supabase (Postgres + pgvector) - AI: OpenAI (with OSS fallback) - Obs: Sentry, Langfuse"
  },
  {
    "id": "06-toolchains\\vercel-ai-sdk.md",
    "title": "Vercel AI SDK Integration Playbook",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/06-toolchains/vercel-ai-sdk.md",
    "section": "Toolchains",
    "path": "06-toolchains\\vercel-ai-sdk.md",
    "headings": [
      {
        "level": 1,
        "text": "Vercel AI SDK Integration Playbook"
      },
      {
        "level": 2,
        "text": "Why Use the SDK"
      },
      {
        "level": 2,
        "text": "Quick Start (Dashboard)"
      },
      {
        "level": 2,
        "text": "Pair With Repo Patterns"
      },
      {
        "level": 2,
        "text": "OpenRouter & Local Models"
      },
      {
        "level": 2,
        "text": "Observability Hooks"
      },
      {
        "level": 2,
        "text": "Deployment Checklist"
      },
      {
        "level": 2,
        "text": "Next Steps"
      }
    ],
    "excerpt": "# Vercel AI SDK Integration Playbook Leverage the Vercel AI SDK to power composable chat, RAG, and agent experiences across the AI Architect Academy ecosystem. This guide shows how to connect the SDK to the dashboard, reuse repo patterns, and stay compliant with governance expectations. ## Why Use the SDK - **Streaming",
    "content": "# Vercel AI SDK Integration Playbook Leverage the Vercel AI SDK to power composable chat, RAG, and agent experiences across the AI Architect Academy ecosystem. This guide shows how to connect the SDK to the dashboard, reuse repo patterns, and stay compliant with governance expectations. ## Why Use the SDK - **Streaming UX out of the box:** Token streaming, tool call events, and optimistic UI updates with minimal boilerplate. - **Provider flexibility:** Swap OpenAI, Anthropic, Cohere, Groq, or OpenRouter adapters without refactoring business logic. - **Edge-ready:** First-class support for Vercel Edge Functions and Next.js App Router (used in ). - **Trace hooks:** Easily attach Langfuse or custom telemetry, keeping eval and observability loops tight. ## Quick Start (Dashboard) 1. Install dependencies (already present in ). 2. Set provider keys in : 3. Use the SDK inside edge routes, e.g. : 4. Surface responses in the React client using the hook from for full streaming UX. ## Pair With Repo Patterns - **RAG Projects:** Use the SDK’s support to call retrieval endpoints built from . - **Agentic Swarms:** Map tool calls to orchestrations described in and log hand-offs via . - **Governance:** Pipe model + prompt metadata into and for traceability. ## OpenRouter & Local Models - Add an adapter (e.g. ) and set in . - For local models (Open WebUI, Big-AGI), point the SDK’s fetcher to your local endpoint: - Document the setup in [ ](../dashboard/AGENT.md) so teammates can replicate your environment. ## Observability Hooks - Wrap with Langfuse logging (see ) or add custom spans to track latency and cost. - Use the dashboards in Langfuse to validate evals from . ## Deployment Checklist - Secrets stored via Vercel environment variables or your secrets manager. - Feature flags ( , ) toggled per environment in . - Regression suite staged via (add tests as your flows mature). ## Next Steps - Extend the SDK usage to power the dashboard Playground and Builder modules. - Pair with to "
  },
  {
    "id": "07-evaluation\\eval-harness.md",
    "title": "Eval Harness",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/07-evaluation/eval-harness.md",
    "section": "Evaluation",
    "path": "07-evaluation\\eval-harness.md",
    "headings": [
      {
        "level": 1,
        "text": "Eval Harness"
      }
    ],
    "excerpt": "# Eval Harness - Dataset format - Scoring functions - CI integration",
    "content": "# Eval Harness - Dataset format - Scoring functions - CI integration"
  },
  {
    "id": "07-evaluation\\metrics.md",
    "title": "Evaluation Metrics",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/07-evaluation/metrics.md",
    "section": "Evaluation",
    "path": "07-evaluation\\metrics.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Metrics"
      }
    ],
    "excerpt": "# Evaluation Metrics - Faithfulness, groundedness, factuality - Relevance, recall@k, MRR - UX metrics: satisfaction, time-to-answer",
    "content": "# Evaluation Metrics - Faithfulness, groundedness, factuality - Relevance, recall@k, MRR - UX metrics: satisfaction, time-to-answer"
  },
  {
    "id": "08-governance\\AI-procurement-checklist.md",
    "title": "AI Procurement Checklist",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/AI-procurement-checklist.md",
    "section": "Governance",
    "path": "08-governance\\AI-procurement-checklist.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Procurement Checklist"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Readiness Questions"
      },
      {
        "level": 2,
        "text": "Checklist"
      },
      {
        "level": 2,
        "text": "Artefacts to produce"
      },
      {
        "level": 2,
        "text": "Related Modules"
      }
    ],
    "excerpt": "# AI Procurement Checklist Codify how your organisation sources, evaluates, and signs off on AI vendors and tooling. ## Why it matters - Aligns legal, security, and business stakeholders before pilots or procurement escalate cost. - Documents risk assessments, data-sharing terms, and evaluation benchmarks for third-par",
    "content": "# AI Procurement Checklist Codify how your organisation sources, evaluates, and signs off on AI vendors and tooling. ## Why it matters - Aligns legal, security, and business stakeholders before pilots or procurement escalate cost. - Documents risk assessments, data-sharing terms, and evaluation benchmarks for third-party models. ## Readiness Questions - What data will be shared with the vendor? Is it covered by existing DPAs? - Which regions and business units will use the service? Any residency constraints? - Does the vendor provide SOC2 / ISO 27001 / HIPAA certification or equivalent? - What metrics or proof-of-value must be delivered before renewal? ## Checklist 1. **Initiate**: Document business case, problem statement, and expected outcomes. 2. **Risk Intake**: Complete security/privacy questionnaire, classify data sensitivity, and log in risk register. 3. **Evaluation Plan**: Define success metrics, guardrail tests, and fallback options (tie to ). 4. **Contract Guardrails**: Capture usage caps, data retention promises, retraining rights, and audit clauses. 5. **Approval Workflow**: Route through procurement, security, legal, and finance sign-offs with timestamps. 6. **Operational Handover**: Share playbooks with the owning team (monitoring, incident contacts, renewal timeline). ## Artefacts to produce - Signed intake form stored in your governance workspace. - Evaluation summary + Promptfoo report proving the vendor meets thresholds. - Risk log entry linked to mitigation actions. - Renewal reminder + owner recorded in collaboration tools. ## Related Modules - [Policy Automation Quick Start](../02-learning-paths/micro-modules/governance-policy-automation.md) - [Model Risk Review Sprint](../02-learning-paths/micro-modules/governance-model-risk-review.md) - [Evaluation Automation Pipeline](../02-learning-paths/micro-modules/evaluation-automation-pipeline.md)"
  },
  {
    "id": "08-governance\\README.md",
    "title": "Governance & Compliance Library",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/README.md",
    "section": "Governance",
    "path": "08-governance\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Governance & Compliance Library"
      },
      {
        "level": 2,
        "text": "Quick Links"
      },
      {
        "level": 2,
        "text": "How to use"
      },
      {
        "level": 2,
        "text": "Related Playbooks"
      }
    ],
    "excerpt": "# Governance & Compliance Library Practical guides to keep AI initiatives compliant, auditable, and resilient. ## Quick Links - [AI Procurement Checklist](AI-procurement-checklist.md) - [Incident Response Playbook](incident-response-checklist.md) - [Human Review & Oversight Checklist](human-review-checklist.md) - [Mode",
    "content": "# Governance & Compliance Library Practical guides to keep AI initiatives compliant, auditable, and resilient. ## Quick Links - [AI Procurement Checklist](AI-procurement-checklist.md) - [Incident Response Playbook](incident-response-checklist.md) - [Human Review & Oversight Checklist](human-review-checklist.md) - [Model Risk Overview](model-risk.md) - [Privacy & GDPR Starter](privacy-gdpr.md) ## How to use 1. Pair each checklist with the relevant design patterns and workflows. 2. Embed outputs in your governance workspace (Confluence/Notion/SharePoint) with ownership. 3. Tie actions back to the Value Loop: Pattern ? Project ? Evaluation ? Story. 4. Update quarterly or after any incident/major deployment. ## Related Playbooks - [Creator Studio Automation](../01-design-patterns/creator-studio-automation.md) - [Model Lifecycle Management](../01-design-patterns/model-lifecycle-management.md) - [Retrospective with AI Workflow](../15-workflows/retrospective-with-ai.md)"
  },
  {
    "id": "08-governance\\human-review-checklist.md",
    "title": "Human Review & Oversight Checklist",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/human-review-checklist.md",
    "section": "Governance",
    "path": "08-governance\\human-review-checklist.md",
    "headings": [
      {
        "level": 1,
        "text": "Human Review & Oversight Checklist"
      },
      {
        "level": 2,
        "text": "When to Require Human Review"
      },
      {
        "level": 2,
        "text": "Review Stages"
      },
      {
        "level": 2,
        "text": "Checklist"
      },
      {
        "level": 2,
        "text": "Artefacts"
      },
      {
        "level": 2,
        "text": "Related Resources"
      }
    ],
    "excerpt": "# Human Review & Oversight Checklist Ensure human experts stay empowered during AI-assisted workflows. ## When to Require Human Review - High-risk decisions (medical, financial, legal recommendations). - Content with brand/regulatory implications (public statements, press, regulated marketing). - Model updates, prompt ",
    "content": "# Human Review & Oversight Checklist Ensure human experts stay empowered during AI-assisted workflows. ## When to Require Human Review - High-risk decisions (medical, financial, legal recommendations). - Content with brand/regulatory implications (public statements, press, regulated marketing). - Model updates, prompt changes, or data migrations affecting customer-facing outcomes. ## Review Stages 1. **Eligibility**: Define which tasks demand human approval vs. automated release. 2. **Reviewer Assignment**: Map reviewers to domains; ensure training and access rights. 3. **Context Delivery**: Provide AI output, source citations, metrics, and change diff. 4. **Decision Logging**: Capture approve/reject/needs changes with rationale. 5. **Feedback Loop**: Feed reviewer notes into prompt/data improvements and evaluation suites. ## Checklist - [ ] Decision matrix documented and accessible. - [ ] Reviewer rosters up to date with coverage across time zones. - [ ] Review UI surfaces context, guardrail scores, cost, and citations. - [ ] Audit log stores reviewer actions with timestamps and linked artefacts. - [ ] Workflow enforces segregation of duties (no self-approval for authors). - [ ] SLA tracking for review turnaround with escalation triggers. ## Artefacts - Review policy doc stored under governance workspace. - Example annotated output demonstrating correct reviewer feedback. - Dashboard tracking review throughput and override rates. - Continuous improvement backlog referencing reviewer insights. ## Related Resources - [Intelligent Orchestration Workflow Pattern](../01-design-patterns/orchestration-workflow.md) - [Creator Content Orchestration Sprint](../02-learning-paths/micro-modules/creator-content-orchestration.md) - [Evaluation Automation Pipeline](../02-learning-paths/micro-modules/evaluation-automation-pipeline.md)"
  },
  {
    "id": "08-governance\\incident-response-checklist.md",
    "title": "Incident Response Playbook for AI Systems",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/incident-response-checklist.md",
    "section": "Governance",
    "path": "08-governance\\incident-response-checklist.md",
    "headings": [
      {
        "level": 1,
        "text": "Incident Response Playbook for AI Systems"
      },
      {
        "level": 2,
        "text": "Signals to watch"
      },
      {
        "level": 2,
        "text": "Roles & Responsibilities"
      },
      {
        "level": 2,
        "text": "Response Checklist"
      },
      {
        "level": 2,
        "text": "Artefacts"
      },
      {
        "level": 2,
        "text": "Related Resources"
      }
    ],
    "excerpt": "# Incident Response Playbook for AI Systems Respond quickly when AI services misbehave, integrating human-in-the-loop controls and communications. ## Signals to watch - Guardrail breaches (toxicity, hallucinations, compliance infractions). - Latency or cost spikes from runaway prompts or agent loops. - Data exposure ev",
    "content": "# Incident Response Playbook for AI Systems Respond quickly when AI services misbehave, integrating human-in-the-loop controls and communications. ## Signals to watch - Guardrail breaches (toxicity, hallucinations, compliance infractions). - Latency or cost spikes from runaway prompts or agent loops. - Data exposure events (logs containing PII, model training on restricted data). ## Roles & Responsibilities | Role | Ownership | | --- | --- | | Incident Commander | Coordinates response, decision making, and status updates. | | AI Ops | Captures telemetry, triggers mitigations, performs rollbacks. | | Domain SME | Evaluates business impact, approves messaging to stakeholders. | | Communications | Notifies leadership, legal, and customers when applicable. | ## Response Checklist 1. **Detect & Acknowledge**: Auto-create ticket with severity, affected services, guardrail metrics, and last deploy ID. 2. **Stabilise**: Roll back to known good model/prompt, or route traffic to human fallback. 3. **Contain**: Disable risky features, rotate keys/tokens, purge compromised caches. 4. **Investigate**: Pull Langfuse traces, Promptfoo reports, deployment logs, and impacted dataset snapshots. 5. **Remediate**: Patch prompts/models, update guardrail rules, add regression tests. 6. **Communicate**: Issue updates every 30/60 minutes to stakeholders. Use templated incident briefs. 7. **Review**: Within 48 hours, run [Retrospective with AI](../15-workflows/retrospective-with-ai.md) and update risk registers / training. ## Artefacts - Incident timeline (timestamps, decisions, owners). - Root cause analysis with contributing factors and preventative actions. - Updated playbooks (alerts, thresholds, rollback runbooks). - Stakeholder comms archive (email/slack summaries). ## Related Resources - [Model Lifecycle Management Pattern](../01-design-patterns/model-lifecycle-management.md) - [AI Performance Optimisation Pattern](../01-design-patterns/performance-optimization.md) - [Creator Evaluat"
  },
  {
    "id": "08-governance\\model-risk.md",
    "title": "Model Risk Management",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/model-risk.md",
    "section": "Governance",
    "path": "08-governance\\model-risk.md",
    "headings": [
      {
        "level": 1,
        "text": "Model Risk Management"
      }
    ],
    "excerpt": "# Model Risk Management - Risk taxonomy - Controls & monitoring - Review cadence",
    "content": "# Model Risk Management - Risk taxonomy - Controls & monitoring - Review cadence"
  },
  {
    "id": "08-governance\\privacy-gdpr.md",
    "title": "Privacy & GDPR",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/privacy-gdpr.md",
    "section": "Governance",
    "path": "08-governance\\privacy-gdpr.md",
    "headings": [
      {
        "level": 1,
        "text": "Privacy & GDPR"
      }
    ],
    "excerpt": "# Privacy & GDPR - Data residency - Consent & retention - Data subject rights",
    "content": "# Privacy & GDPR - Data residency - Consent & retention - Data subject rights"
  },
  {
    "id": "09-articles\\README.md",
    "title": "Articles",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/09-articles/README.md",
    "section": "Articles",
    "path": "09-articles\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Articles"
      }
    ],
    "excerpt": "# Articles - RAG in Production: Chunking, Hybrid Search, Evals - Grounded Tutoring: Enforcing Citations - Design Patterns for Enterprise AI - Agentic Code Swarms: Orchestration patterns, UX for agents, and pedagogy - Creator Studio Launch Guide: Automating briefs-to-publish workflows with eval loops",
    "content": "# Articles - RAG in Production: Chunking, Hybrid Search, Evals - Grounded Tutoring: Enforcing Citations - Design Patterns for Enterprise AI - Agentic Code Swarms: Orchestration patterns, UX for agents, and pedagogy - Creator Studio Launch Guide: Automating briefs-to-publish workflows with eval loops"
  },
  {
    "id": "09-articles\\creator-studio-launch-guide.md",
    "title": "Creator Studio Launch Guide",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/09-articles/creator-studio-launch-guide.md",
    "section": "Articles",
    "path": "09-articles\\creator-studio-launch-guide.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Studio Launch Guide"
      },
      {
        "level": 2,
        "text": "1. Anchor on Outcomes"
      },
      {
        "level": 2,
        "text": "2. Structure the Production Pipeline"
      },
      {
        "level": 2,
        "text": "3. Assign Roles & Rituals"
      },
      {
        "level": 2,
        "text": "4. Build the Tech Stack"
      },
      {
        "level": 2,
        "text": "5. Implement the Feedback Loop"
      },
      {
        "level": 2,
        "text": "6. Operational Excellence"
      },
      {
        "level": 2,
        "text": "7. Launch Checklist"
      },
      {
        "level": 2,
        "text": "8. Next Steps"
      }
    ],
    "excerpt": "# Creator Studio Launch Guide Launch a high-velocity creator studio that blends human taste with AI copilots. This guide distils best practices from the new Creator Studio Automation pattern, micro-learning modules, and evaluation workflows so you can move from concept to multi-channel distribution in days, not months.",
    "content": "# Creator Studio Launch Guide Launch a high-velocity creator studio that blends human taste with AI copilots. This guide distils best practices from the new Creator Studio Automation pattern, micro-learning modules, and evaluation workflows so you can move from concept to multi-channel distribution in days, not months. ## 1. Anchor on Outcomes - **North-star metrics:** retention, audience growth, revenue per campaign, turnaround time. - **Guardrails:** brand voice, compliance, accessibility, and regional requirements. - **Success definition:** a single campaign should demonstrate faster production, higher engagement, or better reuse of assets. ## 2. Structure the Production Pipeline 1. **Brief intake:** capture persona, offer, CTA, channel mix, and success metrics. Use forms with schema validation. 2. **Knowledge graph:** store brand voice snippets, value props, FAQs, and evergreen offers in a vector store. 3. **Generation tiers:** split prompts for outlines, long-form scripts, short-form hooks, thumbnails, and distribution copy. 4. **Guardrail service:** enforce tone/style rules, run compliance linting, and attach citations for claims. 5. **Review lanes:** surface diffs, highlight AI-generated sections, and collect reviewer notes in one dashboard. 6. **Publish & measure:** push to CMS, social schedulers, newsletters, and update analytics warehouse for tracking. ## 3. Assign Roles & Rituals | Persona | Responsibility | AI Assistants | | --- | --- | --- | | Creative Director | Approves campaign brief, calibrates tone, manages backlog | Outline generator, persona reminder bot | | Editor / Producer | Refines scripts, ensures compliance, orchestrates publishing | Fact-check agent, compliance lint tool | | Analyst | Monitors performance, designs experiments, communicates insights | Analytics notebook agent, cohort analysis prompts | | Marketing Ops | Maintains workflow automation, env secrets, CI checks | Promptfoo regression suite, Langfuse dashboards | ## 4. Build the "
  },
  {
    "id": "09-articles\\drafts\\README.md",
    "title": "Draft Articles",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/09-articles/drafts/README.md",
    "section": "Articles",
    "path": "09-articles\\drafts\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Draft Articles"
      }
    ],
    "excerpt": "# Draft Articles Drop in-progress narratives here. Use the prompts from and the templates in to accelerate publishing.",
    "content": "# Draft Articles Drop in-progress narratives here. Use the prompts from and the templates in to accelerate publishing."
  },
  {
    "id": "10-resources\\channels.md",
    "title": "Channels",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/channels.md",
    "section": "Resources",
    "path": "10-resources\\channels.md",
    "headings": [],
    "excerpt": "# YouTube Channels (Curated) - Andrej Karpathy - https://www.youtube.com/@AndrejKarpathy - Sebastian Raschka - https://www.youtube.com/@SebastianRaschka - Hugging Face - https://www.youtube.com/@HuggingFace - LangChain - https://www.youtube.com/@LangChain - OpenAI - https://www.youtube.com/@OpenAI - MLOps Community - h",
    "content": "# YouTube Channels (Curated) - Andrej Karpathy - https://www.youtube.com/@AndrejKarpathy - Sebastian Raschka - https://www.youtube.com/@SebastianRaschka - Hugging Face - https://www.youtube.com/@HuggingFace - LangChain - https://www.youtube.com/@LangChain - OpenAI - https://www.youtube.com/@OpenAI - MLOps Community - https://www.youtube.com/@MLOps - Two Minute Papers (curate for depth) - https://www.youtube.com/@TwoMinutePapers"
  },
  {
    "id": "10-resources\\papers.md",
    "title": "Papers & Reading",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/papers.md",
    "section": "Resources",
    "path": "10-resources\\papers.md",
    "headings": [
      {
        "level": 1,
        "text": "Papers & Reading"
      }
    ],
    "excerpt": "# Papers & Reading - Attention Is All You Need - Rethinking RAG architectures (surveys) - Hallucination evaluation methods",
    "content": "# Papers & Reading - Attention Is All You Need - Rethinking RAG architectures (surveys) - Hallucination evaluation methods"
  },
  {
    "id": "10-resources\\platforms.md",
    "title": "Platforms & Tools",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/platforms.md",
    "section": "Resources",
    "path": "10-resources\\platforms.md",
    "headings": [
      {
        "level": 1,
        "text": "Platforms & Tools"
      }
    ],
    "excerpt": "# Platforms & Tools - Supabase — https://supabase.com/ - Vercel — https://vercel.com/ - Railway — https://railway.app/ - Fly.io — https://fly.io/ - Cloudflare — https://www.cloudflare.com/ - Stripe — https://stripe.com/ - Sentry — https://sentry.io/ - Langfuse — https://langfuse.com/",
    "content": "# Platforms & Tools - Supabase — https://supabase.com/ - Vercel — https://vercel.com/ - Railway — https://railway.app/ - Fly.io — https://fly.io/ - Cloudflare — https://www.cloudflare.com/ - Stripe — https://stripe.com/ - Sentry — https://sentry.io/ - Langfuse — https://langfuse.com/"
  },
  {
    "id": "10-resources\\playlists.md",
    "title": "Playlists (First Pull Targets)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/playlists.md",
    "section": "Resources",
    "path": "10-resources\\playlists.md",
    "headings": [
      {
        "level": 1,
        "text": "Playlists (First Pull Targets)"
      }
    ],
    "excerpt": "# Playlists (First Pull Targets) - Karpathy: Neural Networks, LLMs — https://www.youtube.com/@AndrejKarpathy/playlists - Hugging Face: Transformers — https://www.youtube.com/@HuggingFace/playlists - LangChain: RAG & Agents — https://www.youtube.com/@LangChain/playlists - OpenAI Dev — https://www.youtube.com/@OpenAI/pla",
    "content": "# Playlists (First Pull Targets) - Karpathy: Neural Networks, LLMs — https://www.youtube.com/@AndrejKarpathy/playlists - Hugging Face: Transformers — https://www.youtube.com/@HuggingFace/playlists - LangChain: RAG & Agents — https://www.youtube.com/@LangChain/playlists - OpenAI Dev — https://www.youtube.com/@OpenAI/playlists - MLOps Community Sessions — https://www.youtube.com/@MLOps/playlists"
  },
  {
    "id": "10-resources\\repos.md",
    "title": "Repos",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/repos.md",
    "section": "Resources",
    "path": "10-resources\\repos.md",
    "headings": [
      {
        "level": 2,
        "text": "Meta Aggregators"
      }
    ],
    "excerpt": "# Repositories to Know (With Links) | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Models and tokenizers: foundation for NLP and LLM projects | | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | RAG and tool orchestration; fast from p",
    "content": "# Repositories to Know (With Links) | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Models and tokenizers: foundation for NLP and LLM projects | | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | RAG and tool orchestration; fast from prototype to production | | [run-llama/llama_index](https://github.com/run-llama/llama_index) | Data framework for LLM applications | | [vllm-project/vllm](https://github.com/vllm-project/vllm) | High-throughput, low-latency serving | | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | Local inference on CPU and edge devices | | [ollama/ollama](https://github.com/ollama/ollama) | Local model runner and packaging | | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Vector similarity inside Postgres | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Production-ready open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Vector database with hybrid search | | [milvus-io/milvus](https://github.com/milvus-io/milvus) | Distributed vector database | | [langfuse/langfuse](https://github.com/langfuse/langfuse) | Observability for LLM applications | | [Arize-ai/phoenix](https://github.com/Arize-ai/phoenix) | Observability and analysis toolkit | | [promptfoo/promptfoo](https://github.com/promptfoo/promptfoo) | Prompt evaluations in CI | | [microsoft/autogen](https://github.com/microsoft/autogen) | Multi-agent framework | | [joaomdmoura/crewai](https://github.com/joaomdmoura/crewai) | Agent teams and workflows | ## Meta Aggregators | Link | Why it matters | |---|---| | [sindresorhus/awesome](https://github.com/sindresorhus/awesome) | Canonical index of awesome lists | | [awesome-machine-learning/awesome-machine-learning](https://github.com/awesome-machine-learning/awesome-machine-learning) | Broad machine learning resources | | [visenger/awesome-mlops](https://github.com/visenger/awesome-mlops) | Cu"
  },
  {
    "id": "10-resources\\videos.md",
    "title": "Videos (Selected)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/videos.md",
    "section": "Resources",
    "path": "10-resources\\videos.md",
    "headings": [
      {
        "level": 1,
        "text": "Videos (Selected)"
      }
    ],
    "excerpt": "# Videos (Selected) - Karpathy: Intro to LLMs — https://www.youtube.com/watch?v=zjkBMFhNj_g - Raschka: LLM Evaluation — https://www.youtube.com/watch?v=Rr8a8Y3G9Vw - LangChain: RAG with pgvector — https://www.youtube.com/watch?v=b8bXqSxTq0E - Hugging Face: Transformers Intro — https://www.youtube.com/watch?v=G5RY_SUJih",
    "content": "# Videos (Selected) - Karpathy: Intro to LLMs — https://www.youtube.com/watch?v=zjkBMFhNj_g - Raschka: LLM Evaluation — https://www.youtube.com/watch?v=Rr8a8Y3G9Vw - LangChain: RAG with pgvector — https://www.youtube.com/watch?v=b8bXqSxTq0E - Hugging Face: Transformers Intro — https://www.youtube.com/watch?v=G5RY_SUJih4 - OpenAI: Dev Day Highlights — https://www.youtube.com/watch?v=U9mJuUkhUzk"
  },
  {
    "id": "11-hyperscalers\\README.md",
    "title": "Hyperscalers (OCI • AWS • GCP • Azure)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Hyperscalers (OCI • AWS • GCP • Azure)"
      }
    ],
    "excerpt": "# Hyperscalers (OCI • AWS • GCP • Azure) Decision-focused guides to build GenAI/RAG on each cloud with links to official docs, vector options, and quick starts. - Oracle Cloud Infrastructure (OCI): - Amazon Web Services (AWS): - Google Cloud (GCP): - Microsoft Azure: See also: [Choose a Platform](choose-platform.md)",
    "content": "# Hyperscalers (OCI • AWS • GCP • Azure) Decision-focused guides to build GenAI/RAG on each cloud with links to official docs, vector options, and quick starts. - Oracle Cloud Infrastructure (OCI): - Amazon Web Services (AWS): - Google Cloud (GCP): - Microsoft Azure: See also: [Choose a Platform](choose-platform.md)"
  },
  {
    "id": "11-hyperscalers\\aws\\README.md",
    "title": "AWS for AI Architects",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/aws/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\aws\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "AWS for AI Architects"
      }
    ],
    "excerpt": "# AWS for AI Architects Key Services - Amazon Bedrock: managed GenAI (model providers) - OpenSearch Serverless: vector search; filters - Aurora/ RDS Postgres: pgvector support - Lambda, API Gateway, ECS/EKS for serving - Observability: CloudWatch + 3rd‑party Vector Options - OpenSearch kNN / Faiss under the hood - Auro",
    "content": "# AWS for AI Architects Key Services - Amazon Bedrock: managed GenAI (model providers) - OpenSearch Serverless: vector search; filters - Aurora/ RDS Postgres: pgvector support - Lambda, API Gateway, ECS/EKS for serving - Observability: CloudWatch + 3rd‑party Vector Options - OpenSearch kNN / Faiss under the hood - Aurora Postgres + pgvector Quick Start (RAG) 1) Ingest → embeddings → OpenSearch index or pgvector 2) Retrieval API: vector + BM25 hybrid; re‑rank if needed 3) Tutor API: Bedrock (or OpenAI) with citations 4) Logs & metrics; costs via CloudWatch + tags Docs & Links - Amazon Bedrock — https://aws.amazon.com/bedrock/ - OpenSearch Vector Search — https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html - Aurora pgvector — https://aws.amazon.com/blogs/database/amazon-aurora-postgresql-adhere-pgvector/"
  },
  {
    "id": "11-hyperscalers\\azure\\README.md",
    "title": "Azure for AI Architects",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/azure/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\azure\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Azure for AI Architects"
      }
    ],
    "excerpt": "# Azure for AI Architects Key Services - Azure OpenAI Service: GPT models with enterprise controls - Azure AI Search (Cognitive Search) with vectors - Cosmos DB vector, Azure Postgres + pgvector - Azure Functions / AKS for serving - Observability: App Insights + 3rd‑party Vector Options - Azure AI Search (vector) - Cos",
    "content": "# Azure for AI Architects Key Services - Azure OpenAI Service: GPT models with enterprise controls - Azure AI Search (Cognitive Search) with vectors - Cosmos DB vector, Azure Postgres + pgvector - Azure Functions / AKS for serving - Observability: App Insights + 3rd‑party Vector Options - Azure AI Search (vector) - Cosmos DB vector - Azure Postgres + pgvector Quick Start (RAG) 1) Choose vector: AI Search or pgvector 2) Retrieval API: vector + filters; optional rerank 3) Tutor API: Azure OpenAI with citations + safety 4) Monitor with App Insights; track costs Docs & Links - Azure OpenAI — https://learn.microsoft.com/azure/ai-services/openai/ - Azure AI Search Vector — https://learn.microsoft.com/azure/search/vector-search-overview - Cosmos DB Vector — https://learn.microsoft.com/azure/cosmos-db/vector-database"
  },
  {
    "id": "11-hyperscalers\\choose-platform.md",
    "title": "Choose a Platform (Decision Guide)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/choose-platform.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\choose-platform.md",
    "headings": [
      {
        "level": 1,
        "text": "Choose a Platform (Decision Guide)"
      }
    ],
    "excerpt": "# Choose a Platform (Decision Guide) Criteria - Data residency & compliance (EU, HIPAA, ISO) - Managed vector search options and performance - GenAI service availability (models, safety) - Integration with data sources and identity - Cost transparency and predictable budgets - Lock‑in vs. portability (pgvector, open SD",
    "content": "# Choose a Platform (Decision Guide) Criteria - Data residency & compliance (EU, HIPAA, ISO) - Managed vector search options and performance - GenAI service availability (models, safety) - Integration with data sources and identity - Cost transparency and predictable budgets - Lock‑in vs. portability (pgvector, open SDKs) Quick Take - If you already use a cloud: pick native services (pgvector/managed vector + GenAI) and keep portability via open embeddings and RAG patterns. - If you need maximum portability and EU control: Postgres + pgvector + OpenAI/Anthropic with OSS fallbacks. Baseline Stack (All Clouds) - Postgres + pgvector or managed vector DB - Retrieval API (hybrid search), Tutor API (RAG with citations) - Observability (Langfuse/Phoenix), Evals (promptfoo/ragas) - AuthZ, rate limits, and cost guardrails"
  },
  {
    "id": "11-hyperscalers\\gcp\\README.md",
    "title": "Google Cloud for AI Architects",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/gcp/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\gcp\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Google Cloud for AI Architects"
      }
    ],
    "excerpt": "# Google Cloud for AI Architects Key Services - Vertex AI (GenAI Studio, endpoints) - AlloyDB AI: integrated vector embeddings; Cloud SQL pgvector - Cloud Run / GKE for serving - Observability: Cloud Logging/Trace + 3rd‑party Vector Options - AlloyDB AI vector; Cloud SQL pgvector - OpenSearch-compatible via partners Qu",
    "content": "# Google Cloud for AI Architects Key Services - Vertex AI (GenAI Studio, endpoints) - AlloyDB AI: integrated vector embeddings; Cloud SQL pgvector - Cloud Run / GKE for serving - Observability: Cloud Logging/Trace + 3rd‑party Vector Options - AlloyDB AI vector; Cloud SQL pgvector - OpenSearch-compatible via partners Quick Start (RAG) 1) Store embeddings in AlloyDB AI or Cloud SQL pgvector 2) Retrieval API: vector + keyword hybrid 3) Tutor API: Vertex AI models (or OpenAI) with citations 4) Traces/evals; pub/sub for ingestion Docs & Links - Vertex AI — https://cloud.google.com/vertex-ai - AlloyDB AI — https://cloud.google.com/alloydb/docs/ai - Cloud SQL pgvector — https://cloud.google.com/sql/docs/postgres/extensions/pgvector"
  },
  {
    "id": "11-hyperscalers\\oci\\README.md",
    "title": "OCI for AI Architects (Oracle Cloud Infrastructure)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/oci/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\oci\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "OCI for AI Architects (Oracle Cloud Infrastructure)"
      }
    ],
    "excerpt": "# OCI for AI Architects (Oracle Cloud Infrastructure) Key Services - GenAI Service: managed access to foundation models - Oracle Database 23ai: AI Vector Search + JSON Relational Duality - Autonomous Database: managed Oracle DB with vector - Functions/OKE: serverless and Kubernetes - Observability: Logging, APM; 3rd‑pa",
    "content": "# OCI for AI Architects (Oracle Cloud Infrastructure) Key Services - GenAI Service: managed access to foundation models - Oracle Database 23ai: AI Vector Search + JSON Relational Duality - Autonomous Database: managed Oracle DB with vector - Functions/OKE: serverless and Kubernetes - Observability: Logging, APM; 3rd‑party Langfuse/Sentry Vector Options - Oracle DB 23ai AI Vector Search (native) - Postgres (OCI) + pgvector Quick Start (RAG) 1) Store docs in Oracle DB 23ai (vector index) 2) Retrieval API: cosine similarity + lexical fallback 3) Tutor API: call GenAI Service/OpenAI with citations 4) Observability: traces, evals, cost budgets Docs & Links - Oracle GenAI Service — https://docs.oracle.com/en-us/iaas/Content/generative-ai/overview.htm - Database 23ai Vector Search — https://www.oracle.com/database/ai/ - JSON Relational Duality — https://www.oracle.com/database/json/"
  },
  {
    "id": "12-concepts\\caching-and-observability.md",
    "title": "Caching & Observability",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/caching-and-observability.md",
    "section": "Concepts",
    "path": "12-concepts\\caching-and-observability.md",
    "headings": [
      {
        "level": 1,
        "text": "Caching & Observability"
      }
    ],
    "excerpt": "# Caching & Observability - Semantic caches; TTL strategies; invalidation - Traces and spans for prompts, retrieval, answers - Cost tracking and dashboards",
    "content": "# Caching & Observability - Semantic caches; TTL strategies; invalidation - Traces and spans for prompts, retrieval, answers - Cost tracking and dashboards"
  },
  {
    "id": "12-concepts\\cost-and-latency-slos.md",
    "title": "Cost & Latency SLOs",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/cost-and-latency-slos.md",
    "section": "Concepts",
    "path": "12-concepts\\cost-and-latency-slos.md",
    "headings": [
      {
        "level": 1,
        "text": "Cost & Latency SLOs"
      }
    ],
    "excerpt": "# Cost & Latency SLOs - Budgets per feature/tier; alerts - Latency budgets per step; caching and batch - Model choice tradeoffs; fallback strategies",
    "content": "# Cost & Latency SLOs - Budgets per feature/tier; alerts - Latency budgets per step; caching and batch - Model choice tradeoffs; fallback strategies"
  },
  {
    "id": "12-concepts\\embeddings-and-chunking.md",
    "title": "Embeddings & Chunking",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/embeddings-and-chunking.md",
    "section": "Concepts",
    "path": "12-concepts\\embeddings-and-chunking.md",
    "headings": [
      {
        "level": 1,
        "text": "Embeddings & Chunking"
      }
    ],
    "excerpt": "# Embeddings & Chunking - Embedding model selection: cost vs quality - Chunking: semantic boundaries, window overlap, structure aware - Metadata: titles, headings, source, timestamps - Vector hygiene: dedupe, normalization, index tuning",
    "content": "# Embeddings & Chunking - Embedding model selection: cost vs quality - Chunking: semantic boundaries, window overlap, structure aware - Metadata: titles, headings, source, timestamps - Vector hygiene: dedupe, normalization, index tuning"
  },
  {
    "id": "12-concepts\\eval-and-guardrails.md",
    "title": "Evaluation & Guardrails",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/eval-and-guardrails.md",
    "section": "Concepts",
    "path": "12-concepts\\eval-and-guardrails.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation & Guardrails"
      }
    ],
    "excerpt": "# Evaluation & Guardrails - Datasets: Q/A with citations; negative tests - Metrics: faithfulness, coverage, relevance, MRR - Guardrails: safety filters, policy checks, PII redaction",
    "content": "# Evaluation & Guardrails - Datasets: Q/A with citations; negative tests - Metrics: faithfulness, coverage, relevance, MRR - Guardrails: safety filters, policy checks, PII redaction"
  },
  {
    "id": "12-concepts\\hybrid-search-and-reranking.md",
    "title": "Hybrid Search & Reranking",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/hybrid-search-and-reranking.md",
    "section": "Concepts",
    "path": "12-concepts\\hybrid-search-and-reranking.md",
    "headings": [
      {
        "level": 1,
        "text": "Hybrid Search & Reranking"
      }
    ],
    "excerpt": "# Hybrid Search & Reranking - Combine BM25 and vectors for recall and precision - Reranking: cross-encoders or LLM rerankers - Filters and faceting; scoring and thresholds",
    "content": "# Hybrid Search & Reranking - Combine BM25 and vectors for recall and precision - Reranking: cross-encoders or LLM rerankers - Filters and faceting; scoring and thresholds"
  },
  {
    "id": "12-concepts\\prompt-injection-and-security.md",
    "title": "Prompt Injection & Security",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/prompt-injection-and-security.md",
    "section": "Concepts",
    "path": "12-concepts\\prompt-injection-and-security.md",
    "headings": [
      {
        "level": 1,
        "text": "Prompt Injection & Security"
      }
    ],
    "excerpt": "# Prompt Injection & Security - Threats: injection, data exfiltration, jailbreaking - Controls: isolation, allowlists, output validation, sandboxing - Monitoring: anomaly detection, logging, forensics",
    "content": "# Prompt Injection & Security - Threats: injection, data exfiltration, jailbreaking - Controls: isolation, allowlists, output validation, sandboxing - Monitoring: anomaly detection, logging, forensics"
  },
  {
    "id": "12-concepts\\rag-architecture.md",
    "title": "RAG Architecture (Deep Dive)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/rag-architecture.md",
    "section": "Concepts",
    "path": "12-concepts\\rag-architecture.md",
    "headings": [
      {
        "level": 1,
        "text": "RAG Architecture (Deep Dive)"
      }
    ],
    "excerpt": "# RAG Architecture (Deep Dive) - Indexing pipeline: cleaning, chunking, embeddings, metadata - Retrieval: vector similarity + lexical; filters; rerankers - Answering: prompts with rules (citations), abstain paths - Quality: faithfulness, coverage, latency, cost",
    "content": "# RAG Architecture (Deep Dive) - Indexing pipeline: cleaning, chunking, embeddings, metadata - Retrieval: vector similarity + lexical; filters; rerankers - Answering: prompts with rules (citations), abstain paths - Quality: faithfulness, coverage, latency, cost"
  },
  {
    "id": "13-platforms\\README.md",
    "title": "Platforms to Know",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/README.md",
    "section": "Platforms",
    "path": "13-platforms\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Platforms to Know"
      }
    ],
    "excerpt": "# Platforms to Know - Databricks: Lakehouse, MLflow, vector search integrations - Snowflake: Cortex, Snowflake Native Apps, pgvector via partners - Elastic: Elasticsearch vector, hybrid search, relevance - Neo4j: Knowledge graphs and RAG enrichment - Pinecone/Weaviate/Qdrant/Milvus: managed and OSS vector DBs - Redis: ",
    "content": "# Platforms to Know - Databricks: Lakehouse, MLflow, vector search integrations - Snowflake: Cortex, Snowflake Native Apps, pgvector via partners - Elastic: Elasticsearch vector, hybrid search, relevance - Neo4j: Knowledge graphs and RAG enrichment - Pinecone/Weaviate/Qdrant/Milvus: managed and OSS vector DBs - Redis: Redis Stack with vectors, caching for LLM apps"
  },
  {
    "id": "13-platforms\\databricks.md",
    "title": "Databricks",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/databricks.md",
    "section": "Platforms",
    "path": "13-platforms\\databricks.md",
    "headings": [
      {
        "level": 1,
        "text": "Databricks"
      }
    ],
    "excerpt": "# Databricks - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Databricks - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\elasticsearch.md",
    "title": "Elasticsearch",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/elasticsearch.md",
    "section": "Platforms",
    "path": "13-platforms\\elasticsearch.md",
    "headings": [
      {
        "level": 1,
        "text": "Elasticsearch"
      }
    ],
    "excerpt": "# Elasticsearch - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Elasticsearch - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\milvus.md",
    "title": "Milvus",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/milvus.md",
    "section": "Platforms",
    "path": "13-platforms\\milvus.md",
    "headings": [
      {
        "level": 1,
        "text": "Milvus"
      }
    ],
    "excerpt": "# Milvus - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Milvus - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\neo4j.md",
    "title": "Neo4j",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/neo4j.md",
    "section": "Platforms",
    "path": "13-platforms\\neo4j.md",
    "headings": [
      {
        "level": 1,
        "text": "Neo4j"
      }
    ],
    "excerpt": "# Neo4j - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Neo4j - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\pinecone.md",
    "title": "Pinecone",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/pinecone.md",
    "section": "Platforms",
    "path": "13-platforms\\pinecone.md",
    "headings": [
      {
        "level": 1,
        "text": "Pinecone"
      }
    ],
    "excerpt": "# Pinecone - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Pinecone - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\qdrant.md",
    "title": "Qdrant",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/qdrant.md",
    "section": "Platforms",
    "path": "13-platforms\\qdrant.md",
    "headings": [
      {
        "level": 1,
        "text": "Qdrant"
      }
    ],
    "excerpt": "# Qdrant - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Qdrant - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\redis.md",
    "title": "Redis",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/redis.md",
    "section": "Platforms",
    "path": "13-platforms\\redis.md",
    "headings": [
      {
        "level": 1,
        "text": "Redis"
      }
    ],
    "excerpt": "# Redis - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Redis - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\snowflake.md",
    "title": "Snowflake",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/snowflake.md",
    "section": "Platforms",
    "path": "13-platforms\\snowflake.md",
    "headings": [
      {
        "level": 1,
        "text": "Snowflake"
      }
    ],
    "excerpt": "# Snowflake - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Snowflake - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\weaviate.md",
    "title": "Weaviate",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/weaviate.md",
    "section": "Platforms",
    "path": "13-platforms\\weaviate.md",
    "headings": [
      {
        "level": 1,
        "text": "Weaviate"
      }
    ],
    "excerpt": "# Weaviate - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Weaviate - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "14-ai-tools\\README.md",
    "title": "AI Coding Tools & Agents",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/README.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Coding Tools & Agents"
      }
    ],
    "excerpt": "# AI Coding Tools & Agents Practical guidance for using AI coding assistants and agent tools effectively as an AI Architect. Covers setup, strengths/limits, and playbooks for pairing with your repo. Tools covered: - Claude Code (Anthropic) — deep reasoning in code; inline edits - Gemini Code Assist (Google) — cloud-nat",
    "content": "# AI Coding Tools & Agents Practical guidance for using AI coding assistants and agent tools effectively as an AI Architect. Covers setup, strengths/limits, and playbooks for pairing with your repo. Tools covered: - Claude Code (Anthropic) — deep reasoning in code; inline edits - Gemini Code Assist (Google) — cloud-native integrations - Codex CLI (open-source agentic interface) — repo-aware CLI workflows - Cerebras (Model Studio/Inference) — cost/perf options and CLI use - Aider (OSS) — Git-aware assistant via diffs/commits - Continue (OSS) — local/remote models inside VS Code/JetBrains - Cursor (editor) — AI-native IDE (commercial) - Devin (agent) — long-running coding agent (commercial; conceptual workflow) See also: and ."
  },
  {
    "id": "14-ai-tools\\aider.md",
    "title": "Aider (OSS)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/aider.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\aider.md",
    "headings": [
      {
        "level": 1,
        "text": "Aider (OSS)"
      }
    ],
    "excerpt": "# Aider (OSS) - Repo: https://github.com/Aider-AI/aider - What: Git-aware CLI pair programmer; proposes diffs and commits. - Use: → in repo; ask for changes; review patches. - Tips: Use small, focused asks; rely on branch flows; run tests between steps.",
    "content": "# Aider (OSS) - Repo: https://github.com/Aider-AI/aider - What: Git-aware CLI pair programmer; proposes diffs and commits. - Use: → in repo; ask for changes; review patches. - Tips: Use small, focused asks; rely on branch flows; run tests between steps."
  },
  {
    "id": "14-ai-tools\\cerebras.md",
    "title": "Cerebras (Models & CLI)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/cerebras.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\cerebras.md",
    "headings": [
      {
        "level": 1,
        "text": "Cerebras (Models & CLI)"
      }
    ],
    "excerpt": "# Cerebras (Models & CLI) - What: Efficient LLM inference and training offerings; explore CLI/SDK for cost/perf. - Info: https://www.cerebras.net/ and https://github.com/Cerebras - Use: For workloads needing lower-cost inference at scale; integrate via standard SDKs.",
    "content": "# Cerebras (Models & CLI) - What: Efficient LLM inference and training offerings; explore CLI/SDK for cost/perf. - Info: https://www.cerebras.net/ and https://github.com/Cerebras - Use: For workloads needing lower-cost inference at scale; integrate via standard SDKs."
  },
  {
    "id": "14-ai-tools\\claude-code.md",
    "title": "Claude Code (Anthropic)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/claude-code.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\claude-code.md",
    "headings": [
      {
        "level": 1,
        "text": "Claude Code (Anthropic)"
      }
    ],
    "excerpt": "# Claude Code (Anthropic) - What: IDE and web assistant for code with strong reasoning and refactoring. - Setup: https://www.anthropic.com/claude - Use: Pair for big-picture refactors, doc generation, and design. Provide repo context (files, errors) and ask for patch diffs. - Tips: Prefer \"show me a minimal diff\"; iter",
    "content": "# Claude Code (Anthropic) - What: IDE and web assistant for code with strong reasoning and refactoring. - Setup: https://www.anthropic.com/claude - Use: Pair for big-picture refactors, doc generation, and design. Provide repo context (files, errors) and ask for patch diffs. - Tips: Prefer \"show me a minimal diff\"; iterate with test stubs; paste stack traces and failing tests."
  },
  {
    "id": "14-ai-tools\\codex-cli.md",
    "title": "Codex CLI (Open-source agentic coding interface)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/codex-cli.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\codex-cli.md",
    "headings": [
      {
        "level": 1,
        "text": "Codex CLI (Open-source agentic coding interface)"
      }
    ],
    "excerpt": "# Codex CLI (Open-source agentic coding interface) - Repo: https://github.com/openai/codex - What: Terminal-centric agent that can plan, run commands, and patch files with your approval. - Use: Explain goals → let it propose a plan → approve tool calls → review diffs before applying. - Tips: Keep tasks atomic; commit o",
    "content": "# Codex CLI (Open-source agentic coding interface) - Repo: https://github.com/openai/codex - What: Terminal-centric agent that can plan, run commands, and patch files with your approval. - Use: Explain goals → let it propose a plan → approve tool calls → review diffs before applying. - Tips: Keep tasks atomic; commit often; use a separate branch."
  },
  {
    "id": "14-ai-tools\\continue.md",
    "title": "Continue (OSS)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/continue.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\continue.md",
    "headings": [
      {
        "level": 1,
        "text": "Continue (OSS)"
      }
    ],
    "excerpt": "# Continue (OSS) - Repo: https://github.com/continuedev/continue - What: Open-source AI assistant inside VS Code/JetBrains; supports local/remote models. - Use: Install extension → configure providers → chat over code, run quick edits. - Tips: Add a context window with key files; save prompts as recipes.",
    "content": "# Continue (OSS) - Repo: https://github.com/continuedev/continue - What: Open-source AI assistant inside VS Code/JetBrains; supports local/remote models. - Use: Install extension → configure providers → chat over code, run quick edits. - Tips: Add a context window with key files; save prompts as recipes."
  },
  {
    "id": "14-ai-tools\\cursor.md",
    "title": "Cursor (Editor)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/cursor.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\cursor.md",
    "headings": [
      {
        "level": 1,
        "text": "Cursor (Editor)"
      }
    ],
    "excerpt": "# Cursor (Editor) - Site: https://www.cursor.com/ - What: AI-first IDE with strong inline and repo-wide refactors (commercial). - Use: Great for day-to-day implementation; keep commits small; use integrated diff view.",
    "content": "# Cursor (Editor) - Site: https://www.cursor.com/ - What: AI-first IDE with strong inline and repo-wide refactors (commercial). - Use: Great for day-to-day implementation; keep commits small; use integrated diff view."
  },
  {
    "id": "14-ai-tools\\devin.md",
    "title": "Devin (Agent)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/devin.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\devin.md",
    "headings": [
      {
        "level": 1,
        "text": "Devin (Agent)"
      }
    ],
    "excerpt": "# Devin (Agent) - Site: https://www.cognition.ai/ - What: Autonomous coding agent (commercial, limited access). Use this as a conceptual workflow: plan tasks → monitor progress → validate outputs. - Guidance: Even with autonomous agents, keep tight scopes, observable outputs, and manual review gates.",
    "content": "# Devin (Agent) - Site: https://www.cognition.ai/ - What: Autonomous coding agent (commercial, limited access). Use this as a conceptual workflow: plan tasks → monitor progress → validate outputs. - Guidance: Even with autonomous agents, keep tight scopes, observable outputs, and manual review gates."
  },
  {
    "id": "14-ai-tools\\gemini-code-assist.md",
    "title": "Gemini Code Assist (Google)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/gemini-code-assist.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\gemini-code-assist.md",
    "headings": [
      {
        "level": 1,
        "text": "Gemini Code Assist (Google)"
      }
    ],
    "excerpt": "# Gemini Code Assist (Google) - What: Code assistance integrated with Google Cloud and Vertex AI. - Setup: https://cloud.google.com/products/code-assist - Use: Cloud-native integrations (Vertex, GKE); ask for infra snippets and policy checks. - Tips: Keep prompts grounded with exact service names and links to docs.",
    "content": "# Gemini Code Assist (Google) - What: Code assistance integrated with Google Cloud and Vertex AI. - Setup: https://cloud.google.com/products/code-assist - Use: Cloud-native integrations (Vertex, GKE); ask for infra snippets and policy checks. - Tips: Keep prompts grounded with exact service names and links to docs."
  },
  {
    "id": "15-workflows\\agentic-swarms.md",
    "title": "Workflows: Agentic Code Swarms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/agentic-swarms.md",
    "section": "Workflows",
    "path": "15-workflows\\agentic-swarms.md",
    "headings": [
      {
        "level": 1,
        "text": "Workflows: Agentic Code Swarms"
      },
      {
        "level": 2,
        "text": "Planner → Worker → Reviewer (P–W–R)"
      },
      {
        "level": 2,
        "text": "Round‑Robin with Ownership"
      },
      {
        "level": 2,
        "text": "Map‑Reduce"
      },
      {
        "level": 2,
        "text": "Operational Flow"
      }
    ],
    "excerpt": "# Workflows: Agentic Code Swarms ## Planner → Worker → Reviewer (P–W–R) - Use for small, well‑scoped tasks - Add for gated release when safety critical ## Round‑Robin with Ownership - Each agent updates a shared plan; ownership fields resolve conflicts - Good for brainstorming or multi‑discipline drafts ## Map‑Reduce -",
    "content": "# Workflows: Agentic Code Swarms ## Planner → Worker → Reviewer (P–W–R) - Use for small, well‑scoped tasks - Add for gated release when safety critical ## Round‑Robin with Ownership - Each agent updates a shared plan; ownership fields resolve conflicts - Good for brainstorming or multi‑discipline drafts ## Map‑Reduce - Fan‑out to N workers and aggregate; add deduplication and scoring ## Operational Flow 1) Goal defined with acceptance criteria 2) Orchestrator runs; traces captured 3) Evals check outputs; failures routed to triage 4) Approved outputs promoted to deliverables"
  },
  {
    "id": "15-workflows\\ai-briefing.md",
    "title": "AI Briefing Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/ai-briefing.md",
    "section": "Workflows",
    "path": "15-workflows\\ai-briefing.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Briefing Template"
      },
      {
        "level": 2,
        "text": "Executive Summary"
      },
      {
        "level": 2,
        "text": "Metrics Pulse"
      },
      {
        "level": 2,
        "text": "Highlights"
      },
      {
        "level": 2,
        "text": "Blockers & Risks"
      },
      {
        "level": 2,
        "text": "Next Experiments"
      },
      {
        "level": 2,
        "text": "Appendices"
      }
    ],
    "excerpt": "# AI Briefing Template Use for weekly leadership touchpoints or launch reviews. Keep it to 10 minutes live with a pre-read. ## Executive Summary - Headline win this week - Biggest risk / decision needed - Upcoming launch date or milestone ## Metrics Pulse | Metric | Target | Latest | Commentary | | --- | --- | --- | --",
    "content": "# AI Briefing Template Use for weekly leadership touchpoints or launch reviews. Keep it to 10 minutes live with a pre-read. ## Executive Summary - Headline win this week - Biggest risk / decision needed - Upcoming launch date or milestone ## Metrics Pulse | Metric | Target | Latest | Commentary | | --- | --- | --- | --- | | Latency | | | | | Cost per call | | | | | Eval score (faithfulness/toxicity/etc.) | | | | | Adoption / usage | | | | ## Highlights - Customer or stakeholder quote - Visual (drop screenshot or reference / Langfuse chart) ## Blockers & Risks - Policy / compliance - Data or infra - Staffing / skills ## Next Experiments - Experiment | Owner | ETA | Success Criteria | ## Appendices - Links to demos, dashboards, notebooks - Guardrail updates or incidents log"
  },
  {
    "id": "15-workflows\\ai-pair-programming.md",
    "title": "AI Pair Programming (Daily Flow)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/ai-pair-programming.md",
    "section": "Workflows",
    "path": "15-workflows\\ai-pair-programming.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Pair Programming (Daily Flow)"
      }
    ],
    "excerpt": "# AI Pair Programming (Daily Flow) 1) Plan: Write a short plan in the issue; define acceptance criteria. 2) Context: Share key files/errors; ask for a minimal diff. 3) Apply: Review patches; run tests; iterate. 4) Document: Update README/docs; generate changelog. 5) PR: Ask AI to draft PR summary; link to issues; reque",
    "content": "# AI Pair Programming (Daily Flow) 1) Plan: Write a short plan in the issue; define acceptance criteria. 2) Context: Share key files/errors; ask for a minimal diff. 3) Apply: Review patches; run tests; iterate. 4) Document: Update README/docs; generate changelog. 5) PR: Ask AI to draft PR summary; link to issues; request reviews. Tools: Aider/Continue/Claude Code. Keep changes small; commit early/often."
  },
  {
    "id": "15-workflows\\issue-triage-with-linear.md",
    "title": "Issue Triage with Linear/GitHub",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/issue-triage-with-linear.md",
    "section": "Workflows",
    "path": "15-workflows\\issue-triage-with-linear.md",
    "headings": [
      {
        "level": 1,
        "text": "Issue Triage with Linear/GitHub"
      }
    ],
    "excerpt": "# Issue Triage with Linear/GitHub - Triage template: priority, scope, acceptance tests - Use AI to cluster issues and propose epics - Auto-draft tasks from 100 Projects with tags and timeboxes",
    "content": "# Issue Triage with Linear/GitHub - Triage template: priority, scope, acceptance tests - Use AI to cluster issues and propose epics - Auto-draft tasks from 100 Projects with tags and timeboxes"
  },
  {
    "id": "15-workflows\\peer-review.md",
    "title": "Peer Review Ritual",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/peer-review.md",
    "section": "Workflows",
    "path": "15-workflows\\peer-review.md",
    "headings": [
      {
        "level": 1,
        "text": "Peer Review Ritual"
      },
      {
        "level": 2,
        "text": "Preparation"
      },
      {
        "level": 2,
        "text": "Live Agenda"
      },
      {
        "level": 2,
        "text": "Prompt Pack"
      },
      {
        "level": 2,
        "text": "Follow-up"
      }
    ],
    "excerpt": "# Peer Review Ritual Use this 20-minute loop with another architect or engineer to stress-test work before shipping. ## Preparation - Share artifact in advance (notebook, PR, deck). - Provide context: goal, audience, current risks. ## Live Agenda 1. **Orientation (2 min):** Presenter frames scope + desired feedback. 2.",
    "content": "# Peer Review Ritual Use this 20-minute loop with another architect or engineer to stress-test work before shipping. ## Preparation - Share artifact in advance (notebook, PR, deck). - Provide context: goal, audience, current risks. ## Live Agenda 1. **Orientation (2 min):** Presenter frames scope + desired feedback. 2. **Walkthrough (6 min):** Presenter demos while reviewer captures questions. 3. **Deep dive (8 min):** Reviewer probes on assumptions, eval coverage, governance, comms. 4. **Decide (2 min):** Agree on ship/no-ship, outstanding tasks, owners. 5. **Log (2 min):** Update or issue tracker. ## Prompt Pack - \"List the top three failure modes for this design.\" - \"Suggest guardrails or evals missing from this plan.\" - \"Draft a one-line executive summary of this artifact.\" ## Follow-up - Capture decisions in PR comments or shared doc. - Book a follow-up if high-risk items remain."
  },
  {
    "id": "15-workflows\\postmortem.md",
    "title": "Postmortem Template for AI Initiatives",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/postmortem.md",
    "section": "Workflows",
    "path": "15-workflows\\postmortem.md",
    "headings": [
      {
        "level": 1,
        "text": "Postmortem Template for AI Initiatives"
      },
      {
        "level": 2,
        "text": "1. Snapshot"
      },
      {
        "level": 2,
        "text": "2. Timeline"
      },
      {
        "level": 2,
        "text": "3. What Went Well"
      },
      {
        "level": 2,
        "text": "4. What Went Wrong / Gaps"
      },
      {
        "level": 2,
        "text": "5. Metrics"
      },
      {
        "level": 2,
        "text": "6. Root Causes"
      },
      {
        "level": 2,
        "text": "7. Action Items"
      },
      {
        "level": 2,
        "text": "8. Follow-up"
      }
    ],
    "excerpt": "# Postmortem Template for AI Initiatives Use after a major milestone, production incident, or learning sprint. ## 1. Snapshot - Event name + date - Participants - Systems / repos touched ## 2. Timeline | Time | Event | Notes | | --- | --- | --- | | | | | ## 3. What Went Well - - ## 4. What Went Wrong / Gaps - Model per",
    "content": "# Postmortem Template for AI Initiatives Use after a major milestone, production incident, or learning sprint. ## 1. Snapshot - Event name + date - Participants - Systems / repos touched ## 2. Timeline | Time | Event | Notes | | --- | --- | --- | | | | | ## 3. What Went Well - - ## 4. What Went Wrong / Gaps - Model performance - Data quality issues - Process or communication misses ## 5. Metrics | Metric | Target | Actual | Delta | Notes | | --- | --- | --- | --- | --- | | Latency | | | | | | Cost | | | | | | Eval Score | | | | | ## 6. Root Causes Break down with 5 Whys or fishbone-style categories (People, Process, Tech, Data, Policy). ## 7. Action Items | Item | Owner | Due Date | Status | | --- | --- | --- | --- | | | | | | ## 8. Follow-up - Schedule review date - Link to updated docs, dashboards, or guardrails - Capture short Loom/Deck summary if useful for stakeholders"
  },
  {
    "id": "15-workflows\\pr-review-with-agents.md",
    "title": "PR Review with AI Agents",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/pr-review-with-agents.md",
    "section": "Workflows",
    "path": "15-workflows\\pr-review-with-agents.md",
    "headings": [
      {
        "level": 1,
        "text": "PR Review with AI Agents"
      }
    ],
    "excerpt": "# PR Review with AI Agents - Generate PR descriptions with context and risk notes. - Ask AI for test cases and edge cases; run suggested tests. - Use prompt pack: and .",
    "content": "# PR Review with AI Agents - Generate PR descriptions with context and risk notes. - Ask AI for test cases and edge cases; run suggested tests. - Use prompt pack: and ."
  },
  {
    "id": "15-workflows\\repo-maintenance-with-ai.md",
    "title": "Repo Maintenance with AI",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/repo-maintenance-with-ai.md",
    "section": "Workflows",
    "path": "15-workflows\\repo-maintenance-with-ai.md",
    "headings": [
      {
        "level": 1,
        "text": "Repo Maintenance with AI"
      }
    ],
    "excerpt": "# Repo Maintenance with AI - Link checks (CI): keep external links fresh - Auto-generate TOCs and badges as needed - Prompt pack: - Batch sanitize links and add context blurbs per link",
    "content": "# Repo Maintenance with AI - Link checks (CI): keep external links fresh - Auto-generate TOCs and badges as needed - Prompt pack: - Batch sanitize links and add context blurbs per link"
  },
  {
    "id": "15-workflows\\retrospective-with-ai.md",
    "title": "Retrospective with AI Assistant",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/retrospective-with-ai.md",
    "section": "Workflows",
    "path": "15-workflows\\retrospective-with-ai.md",
    "headings": [
      {
        "level": 1,
        "text": "Retrospective with AI Assistant"
      },
      {
        "level": 2,
        "text": "Prep"
      },
      {
        "level": 2,
        "text": "Script"
      },
      {
        "level": 2,
        "text": "Capture Template"
      },
      {
        "level": 2,
        "text": "Share-out"
      }
    ],
    "excerpt": "# Retrospective with AI Assistant 15-minute loop to capture learnings with your AI copilot after every sprint or study week. ## Prep - Share the latest metrics, eval results, and guardrail incidents. - Keep the screenshot from Langfuse or your dashboard ready. ## Script 1. **Win scan (3 min):** Prompt the assistant to ",
    "content": "# Retrospective with AI Assistant 15-minute loop to capture learnings with your AI copilot after every sprint or study week. ## Prep - Share the latest metrics, eval results, and guardrail incidents. - Keep the screenshot from Langfuse or your dashboard ready. ## Script 1. **Win scan (3 min):** Prompt the assistant to list outcomes, surprises, and high-signal artefacts. Validate and add context. 2. **Metric review (4 min):** Inspect latency, cost, eval scores. Ask \"What changed?\" and \"What pattern do we see?\" 3. **Risk radar (3 min):** Have the assistant enumerate new risks, policy changes, or data quality issues. 4. **Next experiments (3 min):** Co-draft the next two experiments, including success criteria. 5. **Commitments (2 min):** Log owners, dates, and supporting assets. ## Capture Template ## Share-out - Push notes to or your team workspace. - Attach visual assets (screens, charts, posters) to reinforce narrative."
  },
  {
    "id": "16-collaboration\\agentic-teams.md",
    "title": "Agentic Teams: Roles, RACI, and Operating Model",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/agentic-teams.md",
    "section": "Collaboration",
    "path": "16-collaboration\\agentic-teams.md",
    "headings": [
      {
        "level": 1,
        "text": "Agentic Teams: Roles, RACI, and Operating Model"
      },
      {
        "level": 2,
        "text": "Roles"
      },
      {
        "level": 2,
        "text": "RACI (example)"
      },
      {
        "level": 2,
        "text": "Cadence"
      },
      {
        "level": 2,
        "text": "Guardrails"
      },
      {
        "level": 2,
        "text": "Artifacts"
      }
    ],
    "excerpt": "# Agentic Teams: Roles, RACI, and Operating Model ## Roles - Orchestrator (Architect): defines goals, orchestrations, and acceptance criteria - Agent Lead (Tech): designs agents, tools, error handling, and eval hooks - Data/Evals: defines metrics, datasets, and guardrails; triages failures - Platform: CI/CD, secrets, c",
    "content": "# Agentic Teams: Roles, RACI, and Operating Model ## Roles - Orchestrator (Architect): defines goals, orchestrations, and acceptance criteria - Agent Lead (Tech): designs agents, tools, error handling, and eval hooks - Data/Evals: defines metrics, datasets, and guardrails; triages failures - Platform: CI/CD, secrets, cost controls, observability - Product: scope, risks, UX, compliance sign‑offs ## RACI (example) - Plan: Product (A), Architect (R), Tech (C), Evals (C) - Build: Tech (A/R), Platform (C), Architect (C) - Evaluate: Evals (A/R), Tech (C), Product (C) - Ship: Platform (A/R), Product (C), Architect (C) ## Cadence - Daily: 15‑min swarm review (top failures, costs, blocked items) - Weekly: model/agent perf review; update evals and guardrails - Monthly: pattern retro; consolidate lessons into the playbook ## Guardrails - Budget envelopes per run and per environment - Red‑team prompts and jailbreak checks in CI - PII scans, citations required for claims, source attribution ## Artifacts - Goals → plans → traces → final deliverables - Architecture documents and BoMs - Eval dashboards with trend lines and incidents"
  },
  {
    "id": "16-collaboration\\checklists.md",
    "title": "Checklists (Before/After)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/checklists.md",
    "section": "Collaboration",
    "path": "16-collaboration\\checklists.md",
    "headings": [
      {
        "level": 1,
        "text": "Checklists (Before/After)"
      }
    ],
    "excerpt": "# Checklists (Before/After) Before Asking - [ ] Define success in 1–2 sentences - [ ] Gather context (files, logs, env) - [ ] Decide output format (diff, plan, code, table) - [ ] Note constraints (performance, security, style) After Receiving - [ ] Sanity check: does it meet acceptance criteria? - [ ] Apply minimal dif",
    "content": "# Checklists (Before/After) Before Asking - [ ] Define success in 1–2 sentences - [ ] Gather context (files, logs, env) - [ ] Decide output format (diff, plan, code, table) - [ ] Note constraints (performance, security, style) After Receiving - [ ] Sanity check: does it meet acceptance criteria? - [ ] Apply minimal diff; run tests; lint - [ ] Add/adjust docs; create PR with summary - [ ] Capture follow‑ups as issues"
  },
  {
    "id": "16-collaboration\\escalation-guide.md",
    "title": "Escalation Guide for AI Architect Teams",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/escalation-guide.md",
    "section": "Collaboration",
    "path": "16-collaboration\\escalation-guide.md",
    "headings": [
      {
        "level": 1,
        "text": "Escalation Guide for AI Architect Teams"
      },
      {
        "level": 2,
        "text": "Levels"
      },
      {
        "level": 2,
        "text": "Escalation Checklist"
      },
      {
        "level": 2,
        "text": "Communication Templates"
      },
      {
        "level": 2,
        "text": "Prevention Loops"
      }
    ],
    "excerpt": "# Escalation Guide for AI Architect Teams Use when incidents, policy risks, or critical blockers arise. Keep it visible next to your runbook. ## Levels | Level | Trigger | Who to Engage | Response Time | | --- | --- | --- | --- | | P0 | Production outage, customer impact, critical security issue | Engineering lead, sec",
    "content": "# Escalation Guide for AI Architect Teams Use when incidents, policy risks, or critical blockers arise. Keep it visible next to your runbook. ## Levels | Level | Trigger | Who to Engage | Response Time | | --- | --- | --- | --- | | P0 | Production outage, customer impact, critical security issue | Engineering lead, security, exec sponsor | Immediate | | P1 | Major degradation (latency, cost spikes), policy breach | Tech lead, product lead, legal/compliance | < 2 hours | | P2 | Evaluation failure, new risk identified, missing dependency | Team core, governance partner | < 1 business day | | P3 | FYI / observation | Team async channel | Asynchronous | ## Escalation Checklist 1. Capture context (time, system, version, owner). 2. Attach observability evidence (Langfuse trace, logs, eval results). 3. Identify customer or stakeholder impact. 4. Propose immediate mitigation + longer-term fix. 5. Book follow-up with postmortem owner ( ). ## Communication Templates ## Prevention Loops - Track incidents in a shared doc; review trends monthly. - Review guardrail dashboards weekly with governance partners. - Rotate incident commander role to build muscle across the team."
  },
  {
    "id": "16-collaboration\\issue-templates.md",
    "title": "Issue Templates for AI Pairing",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/issue-templates.md",
    "section": "Collaboration",
    "path": "16-collaboration\\issue-templates.md",
    "headings": [
      {
        "level": 1,
        "text": "Issue Templates for AI Pairing"
      }
    ],
    "excerpt": "# Issue Templates for AI Pairing Feature - Objective - Acceptance criteria - Constraints / non‑goals - Context (links, files) - Definition of done Bug - Observed behavior & logs - Expected behavior - Repro steps - Env (OS, versions) - Hypothesis (optional)",
    "content": "# Issue Templates for AI Pairing Feature - Objective - Acceptance criteria - Constraints / non‑goals - Context (links, files) - Definition of done Bug - Observed behavior & logs - Expected behavior - Repro steps - Env (OS, versions) - Hypothesis (optional)"
  },
  {
    "id": "16-collaboration\\prompting-guide.md",
    "title": "Prompting Guide (Structure & Recipes)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/prompting-guide.md",
    "section": "Collaboration",
    "path": "16-collaboration\\prompting-guide.md",
    "headings": [
      {
        "level": 1,
        "text": "Prompting Guide (Structure & Recipes)"
      }
    ],
    "excerpt": "# Prompting Guide (Structure & Recipes) Prompt Canvas - Role: what assistant is (e.g., senior TS engineer) - Task: the concrete outcome - Context: files, errors, versions, constraints - Output: format requirements (diff, table, steps) - Checks: tests, constraints, guardrails Example (Patch‑First) Recipes - Code Review:",
    "content": "# Prompting Guide (Structure & Recipes) Prompt Canvas - Role: what assistant is (e.g., senior TS engineer) - Task: the concrete outcome - Context: files, errors, versions, constraints - Output: format requirements (diff, table, steps) - Checks: tests, constraints, guardrails Example (Patch‑First) Recipes - Code Review: see - Test Generator: see - Refactor: see - Docs Writer: see - Pattern Drafter: see - RAG Eval: see"
  },
  {
    "id": "16-collaboration\\working-with-ai.md",
    "title": "Working With AI Assistants (Playbook)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/working-with-ai.md",
    "section": "Collaboration",
    "path": "16-collaboration\\working-with-ai.md",
    "headings": [
      {
        "level": 1,
        "text": "Working With AI Assistants (Playbook)"
      }
    ],
    "excerpt": "# Working With AI Assistants (Playbook) Goal: Get high‑quality, verifiable outcomes quickly while keeping control over scope, cost, and quality. Principles - Be specific: State goal, constraints, acceptance criteria, and non‑goals. - Provide context: File paths, snippets, errors, env, versions. - Ask for a plan: Have t",
    "content": "# Working With AI Assistants (Playbook) Goal: Get high‑quality, verifiable outcomes quickly while keeping control over scope, cost, and quality. Principles - Be specific: State goal, constraints, acceptance criteria, and non‑goals. - Provide context: File paths, snippets, errors, env, versions. - Ask for a plan: Have the AI outline steps; approve/refine. - Prefer minimal diffs: Request patch/diff instead of full files. - Verify: Run tests, check outputs, and ask for self‑checks. - Iterate: Small scopes → apply → re‑ask with updated context. When to use which tool - Editor‑native (Claude Code, Continue, Cursor): fast inline edits and refactors. - Git‑aware CLI (Aider, Codex CLI): patch‑first changes with review and commits. - Long‑running agents (Devin): exploratory tasks; keep guardrails and review gates. Common patterns - Spec → Tests → Code: Ask for acceptance tests before implementation. - Investigate → Hypothesize → Patch: Paste stack traces; ask for debugging steps. - Design Doc → Skeleton → Fill‑in: Draft and iterate on design before coding. Anti‑patterns - Vague asks; no acceptance criteria. - Oversized changes in one shot. - Blindly pasting generated code without review."
  },
  {
    "id": "README.md",
    "title": "AI Architect Academy � Command Center for Visionary Builders",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/README.md",
    "section": "README.md",
    "path": "README.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Architect Academy � Command Center for Visionary Builders"
      },
      {
        "level": 2,
        "text": "Why This Playbook Wins"
      },
      {
        "level": 2,
        "text": "Search-Optimized Highlights"
      },
      {
        "level": 2,
        "text": "What You Will Learn"
      },
      {
        "level": 2,
        "text": "Run the Experience"
      },
      {
        "level": 2,
        "text": "Launch Tracks"
      },
      {
        "level": 2,
        "text": "Curriculum Builder (Teach & Scale)"
      },
      {
        "level": 2,
        "text": "Micro-Learning Atlas"
      },
      {
        "level": 2,
        "text": "Tooling & Integrations"
      },
      {
        "level": 2,
        "text": "Operate with Confidence"
      },
      {
        "level": 2,
        "text": "Dashboard & Agents"
      },
      {
        "level": 2,
        "text": "Tell the Story"
      },
      {
        "level": 2,
        "text": "Contribute & Extend"
      }
    ],
    "excerpt": "<p align=\"center\"><img src=\"assets/logo.svg\" width=\"420\" alt=\"AI Architect Academy\"></p> <p align=\"center\"> <a href=\"https://github.com/frankxai/ai-architect-academy/stargazers\"><img alt=\"Stars\" src=\"https://img.shields.io/github/stars/frankxai/ai-architect-academy?style=flat-square\"></a> <a href=\"https://github.com/fr",
    "content": "<p align=\"center\"><img src=\"assets/logo.svg\" width=\"420\" alt=\"AI Architect Academy\"></p> <p align=\"center\"> <a href=\"https://github.com/frankxai/ai-architect-academy/stargazers\"><img alt=\"Stars\" src=\"https://img.shields.io/github/stars/frankxai/ai-architect-academy?style=flat-square\"></a> <a href=\"https://github.com/frankxai/ai-architect-academy/pulls\"><img alt=\"PRs\" src=\"https://img.shields.io/badge/PRs-welcome-cyan?style=flat-square\"></a> <a href=\"https://ai-architect-academy.github.io/ai-architect-academy/\"><img alt=\"Pages\" src=\"https://img.shields.io/badge/Pages-live-green?style=flat-square\"></a> </p> # AI Architect Academy � Command Center for Visionary Builders Design, ship, and operate AI systems with confidence. This open playbook gives you the artefacts, visuals, and workflows to lead conversations, execute fast, and tell a world-class story. <div align=\"center\"> <a href=\"START-HERE.md\"><img alt=\"Start Here\" src=\"https://img.shields.io/badge/Start-Now-cyan?style=for-the-badge\"></a> <a href=\"docs/experience.html\"><img alt=\"Explore the Experience\" src=\"https://img.shields.io/badge/Explore-Experience-purple?style=for-the-badge\"></a> <a href=\"https://github.com/frankxai/ai-architect-academy/archive/refs/heads/main.zip\"><img alt=\"Download\" src=\"https://img.shields.io/badge/Clone-Repo-black?style=for-the-badge\"></a> </div> ![AI Architect Academy](assets/ai-architect-campus.png) <p align=\"center\"> <img src=\"assets/ai-architect-professor.png\" alt=\"AI Architect mentor persona using AI co-creation tools\" width=\"320\"> <img src=\"assets/ai-architect-education-poster.png\" alt=\"Think like an AI Architect curriculum poster\" width=\"320\"> </p> ## Why This Playbook Wins - **Everything is cross-linked.** Jump between repo, GitHub Pages, and dashboard without losing context. - **Production-calibre patterns.** Value framing, discovery questions, architecture diagrams, evaluation harnesses, and governance packs sit side-by-side. - **Launch-ready visuals.** Hero art, poster layout"
  },
  {
    "id": "START-HERE.md",
    "title": "Start Here — Build Your AI Architect Command Center",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/START-HERE.md",
    "section": "START HERE.md",
    "path": "START-HERE.md",
    "headings": [
      {
        "level": 1,
        "text": "Start Here — Build Your AI Architect Command Center"
      },
      {
        "level": 2,
        "text": "1. Choose Your Mission Profile"
      },
      {
        "level": 3,
        "text": "Launchpad — First 100 Hours"
      },
      {
        "level": 3,
        "text": "Creator & Influencer Track"
      },
      {
        "level": 3,
        "text": "Enterprise & AI CoE Leadership"
      },
      {
        "level": 3,
        "text": "Advisors, Clients, Friends & Family"
      },
      {
        "level": 2,
        "text": "2. Map Your Architecture Playbook"
      },
      {
        "level": 2,
        "text": "3. Run Value Loops Fast"
      },
      {
        "level": 2,
        "text": "4. Instrument, Govern, and Operate"
      },
      {
        "level": 2,
        "text": "5. Launch the Dashboard & Agents"
      },
      {
        "level": 2,
        "text": "6. Amplify and Share the Story"
      },
      {
        "level": 2,
        "text": "Clone & Personalise the Repo"
      },
      {
        "level": 2,
        "text": "Keep Exploring"
      }
    ],
    "excerpt": "# Start Here — Build Your AI Architect Command Center You’re here because you want to ship real AI value, guide stakeholders with confidence, and make your work simple to share. This guide orients you across the repo, the live site, and the dashboard so you can get momentum within minutes. ## 1. Choose Your Mission Pro",
    "content": "# Start Here — Build Your AI Architect Command Center You’re here because you want to ship real AI value, guide stakeholders with confidence, and make your work simple to share. This guide orients you across the repo, the live site, and the dashboard so you can get momentum within minutes. ## 1. Choose Your Mission Profile ### Launchpad — First 100 Hours - Follow the [100-Hour AI Architect Plan](02-learning-paths/100-hour-ai-architect.md) to sprint through retrieval, agents, governance, and storytelling. - Pair the plan with three anchor projects: [RAG on Supabase](05-projects/rag-on-supabase.md), [Langfuse eval harness](05-projects/evals-langfuse.md), and [Vector search benchmarks](05-projects/vector-search-pgvector.md). ### Creator & Influencer Track - Use the [Beginner](02-learning-paths/beginner.md) and [Professional](02-learning-paths/professional.md) paths to pace long-form content, workshops, and community sessions. - Pull examples and citations from [Awesome collections](03-awesome/) and [Articles](09-articles/) to fuel newsletters, talks, and threads. ### Enterprise & AI CoE Leadership - Run the [Bootcamp](02-learning-paths/bootcamp.md) to align product, platform, risk, and ops teams. - Combine architecture patterns ( ) with the [Governance toolkit](08-governance/) and [Collaboration playbooks](16-collaboration/) for executive visibility. ### Advisors, Clients, Friends & Family - Share the [Experience guide](docs/experience.html) when introducing the hub to new collaborators. - Use to spin up proposals, meeting notes, and recap reports fast. ## 2. Map Your Architecture Playbook - Start with three core patterns: [Content Generation](01-design-patterns/content-generation.md), [Decision Support](01-design-patterns/decision-support.md), and [Model Lifecycle Management](01-design-patterns/model-lifecycle-management.md). - Explore the [Concept decks](12-concepts/) for mental models (retrieval, evaluation, observability, safety) you can present to stakeholders. - "
  }
]