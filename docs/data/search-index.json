[
  {
    "id": "00-roadmap\\ROADMAP.md",
    "title": "Experience Roadmap",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/00-roadmap/ROADMAP.md",
    "section": "Roadmap",
    "path": "00-roadmap\\ROADMAP.md",
    "headings": [
      {
        "level": 1,
        "text": "Experience Roadmap"
      },
      {
        "level": 2,
        "text": "Shipping Now"
      },
      {
        "level": 2,
        "text": "Next 30 Days"
      },
      {
        "level": 2,
        "text": "60–90 Day Horizon"
      },
      {
        "level": 2,
        "text": "Guiding Principles"
      }
    ],
    "excerpt": "# Experience Roadmap ## Shipping Now - **Unified navigation + experience hub.** Refreshed with an Experience page, consistent nav, and persona-first copy. - **Brand voice alignment.** Added and updated + to reflect the new narrative. - **Visual system.** New SVG assets ( , ) mirrored under for the live site. ## Next 30",
    "content": "# Experience Roadmap ## Shipping Now - **Unified navigation + experience hub.** Refreshed with an Experience page, consistent nav, and persona-first copy. - **Brand voice alignment.** Added and updated + to reflect the new narrative. - **Visual system.** New SVG assets ( , ) mirrored under for the live site. ## Next 30 Days - **Persona dashboards.** Build dynamic cards on the Experience page that auto-pull highlights from learning paths, projects, and governance modules. - **Interactive project filters.** Extend with saved views (RAG, Agentic, Ops) and top-tier search presets. - **Governance deep dives.** Expand with checklists for procurement, incident response, and human-in-the-loop reviews. ## 60–90 Day Horizon - **Playground templates.** Bundle minimal repos for each flagship project with dockerised quick starts and CI-ready eval harnesses. - **Persona-specific onboarding emails.** Ship optional email sequences that reference repo content and live site milestones. - **Metrics dashboard.** Surface repo commit velocity, project freshness, and evaluation coverage on the homepage pulse cards. ## Guiding Principles - **Lead with clarity.** Every page answers “what value do I get in five minutes?” - **Ship in loops.** Pattern → Project → Eval → Story is the default development cadence. - **Stay truthful.** Document only what exists or is in flight. Ship fast, update often. - **Accessible to all.** Friends, family, enterprise buyers, and influencers should find a path immediately."
  },
  {
    "id": "01-design-patterns\\README.md",
    "title": "Design Pattern Library",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/README.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Design Pattern Library"
      }
    ],
    "excerpt": "# Design Pattern Library - **Customer Experience**: Content Generation, Language Understanding, Personalization, Conversational Commerce, Creator Studio Automation (new). - **Decision Support & Workflow**: Decision Support, Intelligent Orchestration Workflow, Predictive Operations, Autonomous Optimisation. - **Platform",
    "content": "# Design Pattern Library - **Customer Experience**: Content Generation, Language Understanding, Personalization, Conversational Commerce, Creator Studio Automation (new). - **Decision Support & Workflow**: Decision Support, Intelligent Orchestration Workflow, Predictive Operations, Autonomous Optimisation. - **Platform Enablement**: Rapid Innovation, Security & Compliance Automation, Synthetic Data Generation for experimentation. - **AI Infrastructure**: Multicloud Orchestration, Model Lifecycle Management, Governance & Compliance Automation, AI Performance Optimisation. - **Industry Blueprints**: Insurance Rate Modelling, Energy Trading Optimisation, Cyber Vulnerability Management, Genomic Analytics, GIS Intelligence. Each pattern file documents: 1. Business value narratives that clarify why the pattern matters. 2. A reference architecture with reasoning behind major components. 3. Discovery questions to align stakeholders and surface constraints early. 4. A bill of materials to accelerate vendor/tool selection and delivery planning. 5. Key risks with pragmatic controls informed by real-world deployments. Use the patterns as starting points�extend them with your organisation's context, decision gates, and compliance requirements before implementation."
  },
  {
    "id": "01-design-patterns\\content-generation.md",
    "title": "Pattern: Content Generation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/content-generation.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\content-generation.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Content Generation"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Content Generation ## Business Value - Compress campaign and asset production cycles from weeks to hours, enabling faster GTM iterations and multivariate experimentation. - Produce hyper-personalized content that reflects customer intent, compliance posture, and brand voice while scaling to long-tail segment",
    "content": "# Pattern: Content Generation ## Business Value - Compress campaign and asset production cycles from weeks to hours, enabling faster GTM iterations and multivariate experimentation. - Produce hyper-personalized content that reflects customer intent, compliance posture, and brand voice while scaling to long-tail segments. - Free subject-matter experts from repetitive drafting so they can focus on high-signal review, governance, and performance improvement. ## Technical Architecture 1. **Brief & Context Intake**: Marketing brief, product catalogue, historical performance metrics, and tone guidance enter an orchestrated workflow (Temporal/Prefect) that normalizes metadata. 2. **Knowledge Retrieval**: A hybrid search (vector + keyword) layer over product knowledge, FAQs, and policy memos retrieves grounding snippets with provenance tags. 3. **Generation & Adaptation**: Prompt templates select the appropriate foundation or fine-tuned model (e.g., GPT-4o, Claude Sonnet) with controllable decoding for long-form vs. short-form assets. 4. **Guardrails & Review**: Automatic redaction, toxicity checks, policy linting, and side-by-side diff tooling feed a reviewer dashboard with accept/reject/annotate actions before CMS publish. 5. **Observability & Feedback**: Langfuse/Weights & Biases capture latency, cost, and evaluation metrics (groundedness, tone, conversion lift) for continuous tuning. ## Discovery Questions - Which gold-standard assets best represent brand voice, and how frequently do guidelines change? - What regulatory frameworks (GDPR, FINRA, HIPAA) constrain language, disclosures, or data flows? - How will performance be measured (CTR, pipeline contribution, NPS) and fed back into prompts or selection strategies? - What human review SLA is acceptable, and who owns final approval for different campaign tiers? ## Bill of Materials - Orchestrator (Temporal/Prefect), feature store for campaign metadata, secure prompt store, and CMS connectors (Contentful, Adobe Experienc"
  },
  {
    "id": "01-design-patterns\\creator-studio-automation.md",
    "title": "Pattern: Creator Studio Automation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/creator-studio-automation.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\creator-studio-automation.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Creator Studio Automation"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      },
      {
        "level": 2,
        "text": "Metrics & Signals"
      },
      {
        "level": 2,
        "text": "Adoption Playbook"
      }
    ],
    "excerpt": "# Pattern: Creator Studio Automation ## Business Value - Scale high-quality multimedia production (video, blog, social, email) without burning out creative teams or diluting brand consistency. - Turn briefs, scripts, editing, and distribution into measurable pipelines that surface performance insights within hours. - L",
    "content": "# Pattern: Creator Studio Automation ## Business Value - Scale high-quality multimedia production (video, blog, social, email) without burning out creative teams or diluting brand consistency. - Turn briefs, scripts, editing, and distribution into measurable pipelines that surface performance insights within hours. - Level up client services and monetisation by offering AI-assisted packages with transparent governance and attribution. ## Technical Architecture 1. **Ideation Intake**: Capture campaign briefs, audience personas, and tone guidance through structured forms or Notion/Asana connectors that feed a workflow engine (Temporal/Camunda). 2. **Knowledge Layer**: Maintain brand voice, product facts, SEO keywords, and past winning assets in a vector-enabled content lake (pgvector on Postgres/Weaviate) with metadata tagging. 3. **Generation Tier**: Orchestrate specialised models for script drafting (LLMs), storyboard assistance (image diffusion), and headline experimentation (lightweight smaller models) with automatic citation back to the knowledge layer. 4. **Review & QA**: Route drafts through guardrails�style linting, compliance checks, SEO scoring, and fact validation�before human reviewers approve via a creator console. 5. **Publishing & Analytics**: Push approved assets to CMS, YouTube, newsletter, and social schedulers; stream engagement metrics back into analytics warehouse (Snowflake/BigQuery) for continuous optimisation. 6. **Observability & Feedback**: Langfuse/Weights & Biases capture latency, quality annotations, and cost per asset; promptfoo/DeepEval monitor hallucinations and tone adherence. ## Discovery Questions - Which content formats and channels (short-form video, blogs, ads, podcasts, email) are highest priority? What SLAs are required? - How strict are brand, legal, or regional compliance requirements? Are there approval gates by region or customer segment? - What performance metrics matter most�CTR, watch time, conversion, revenue attribution"
  },
  {
    "id": "01-design-patterns\\decision-support.md",
    "title": "Pattern: Decision Support",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/decision-support.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\decision-support.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Decision Support"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Decision Support ## Business Value - Shorten the time from data availability to defensible action by surfacing curated evidence, policy constraints, and confidence signals in one workspace. - Improve consistency of high-impact decisions (credit, underwriting, staffing) by encoding playbooks and risk toleranc",
    "content": "# Pattern: Decision Support ## Business Value - Shorten the time from data availability to defensible action by surfacing curated evidence, policy constraints, and confidence signals in one workspace. - Improve consistency of high-impact decisions (credit, underwriting, staffing) by encoding playbooks and risk tolerances directly into the reasoning flow. - Capture the organisational learning loop by tying decisions to outcomes and producing auditable records for regulators and internal governance. ## Technical Architecture 1. **Data Harmonisation**: ELT pipelines feed a governed warehouse and feature store with mastered entities, policy rules, and reference data. 2. **Context Retrieval**: Semantic and rules-based retrieval assemble facts, comparable cases, and relevant regulations per decision context. 3. **Reasoning & Tooling**: An LLM agent orchestrates deterministic calculators, simulation engines, and optimisation solvers to generate scenarios and recommendations. 4. **Explanation Layer**: The system produces narratives with citations, highlights trade-offs, enumerates assumptions, and exposes model scores. 5. **Decision Capture**: Review UI records human overrides, notes, approvals, and downstream execution triggers while streaming telemetry for model monitoring. ## Discovery Questions - What is the decision frequency, latency requirement, and acceptable residual risk for each decision class? - Which simulation models, heuristics, or playbooks must be preserved and how often are they recalibrated? - How should conflicting data sources be reconciled, and who is the source-of-truth owner? - What retention, audit, and explainability obligations exist across jurisdictions or business units? ## Bill of Materials - Data stack: Warehouse (Snowflake/BigQuery), feature store (Feast/Tecton), policy repository (OPA or Decision Model & Notation). - Retrieval/search layer (pgvector, Elasticsearch) paired with agent runtime (LangChain, Semantic Kernel) and tool execution san"
  },
  {
    "id": "01-design-patterns\\energy-market-trading-optimization.md",
    "title": "Pattern: Energy Market Trading & Optimization",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/energy-market-trading-optimization.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\energy-market-trading-optimization.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Energy Market Trading & Optimization"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Energy Market Trading & Optimization ## Business Value - Increase gross margin by optimising bid strategies across day-ahead, intraday, and balancing markets while respecting regulatory constraints. - Minimise imbalance penalties by pairing real-time telemetry with probabilistic forecasts of demand, supply, ",
    "content": "# Pattern: Energy Market Trading & Optimization ## Business Value - Increase gross margin by optimising bid strategies across day-ahead, intraday, and balancing markets while respecting regulatory constraints. - Minimise imbalance penalties by pairing real-time telemetry with probabilistic forecasts of demand, supply, and renewable intermittency. - Enable traders to scenario-plan around weather, fuel, and outage events using transparent narratives anchored in quantitative models. ## Technical Architecture 1. **Data Foundation**: Stream ingestion of SCADA metrics, weather feeds, fuel prices, and market data into a time-series lakehouse with quality SLAs. 2. **Forecasting Ensemble**: Combine statistical (ARIMA, Prophet) and ML (gradient boosting, deep temporal models) predictors with quantile outputs for uncertainty. 3. **Optimisation Engine**: Solver (Gurobi/CPLEX/OR-Tools) enforces physical constraints, ramp rates, and risk limits to generate bid curves and dispatch schedules. 4. **Agentic Advisory Layer**: LLM orchestrator translates optimisation outputs into trader-friendly narratives, highlights sensitivities, and surfaces compliance checks. 5. **Monitoring & Feedback**: Backtesting harness compares realised vs. forecasted outcomes, recalibrates models, and logs rationale for audit trails. ## Discovery Questions - Which market products (energy, capacity, ancillary services) and geographies are in scope, and what are their clearing deadlines? - What telemetry exists for asset availability, and how is maintenance or forced outage data captured today? - How are risk tolerances defined (VaR, CVaR, position limits), and who approves overrides? - What regulatory reporting obligations apply (REMIT, FERC), and how must recommendations be recorded? ## Bill of Materials - Time-series storage (InfluxDB, TimescaleDB) feeding a lakehouse (Delta/Snowflake) with data quality monitors. - Forecasting stack (tsfresh, GluonTS, Prophet) orchestrated via MLFlow plus optimisation solv"
  },
  {
    "id": "01-design-patterns\\genomic-sequence-processing.md",
    "title": "Pattern: Genomic Sequence Processing & Public Health",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/genomic-sequence-processing.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\genomic-sequence-processing.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Genomic Sequence Processing & Public Health"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Genomic Sequence Processing & Public Health ## Business Value - Compress turnaround from sample collection to actionable variant insights, enabling faster outbreak detection and clinical decision-making. - Support cross-institutional collaboration with governed data sharing that balances research needs again",
    "content": "# Pattern: Genomic Sequence Processing & Public Health ## Business Value - Compress turnaround from sample collection to actionable variant insights, enabling faster outbreak detection and clinical decision-making. - Support cross-institutional collaboration with governed data sharing that balances research needs against patient privacy obligations. - Provide epidemiologists and clinicians with contextualised summaries, lineage tracking, and therapeutic impact assessments. ## Technical Architecture 1. **Secure Data Intake**: Biospecimen metadata and raw sequencing files (FASTQ/BAM) land in encrypted, access-controlled storage with QC checks. 2. **Analysis Pipelines**: Workflow engines (Nextflow/Snakemake) orchestrate alignment, variant calling, annotation, and lineage assignment using validated reference genomes. 3. **Knowledge Integration**: Link variants to public databases (ClinVar, GISAID) and institutional knowledge graphs, capturing pathogenicity and therapeutic implications. 4. **Narrative & Reporting**: LLM assistant generates clinician-friendly briefs grounded in verified facts, highlighting confidence, quality metrics, and recommended follow-up. 5. **Governance Layer**: Audit trails, consent management, and data retention policies enforce compliance with HIPAA/GDPR and local bioethics reviews. ## Discovery Questions - What sequencing platforms are in use, and what are the expected daily/weekly sample volumes and SLAs? - Which regulatory regimes (HIPAA, GDPR, CCPA) and data residency constraints dictate storage and processing choices? - How will results be consumed (clinical dashboards, public health alerts, research portals), and what format is required? - What collaboration agreements exist with external labs or agencies, and how are data access requests approved? ## Bill of Materials - Workflow tooling (Nextflow, Cromwell) backed by HPC clusters or managed services (AWS Batch, Google Cloud Life Sciences). - Domain toolchain: BWA/GATK for alignment and va"
  },
  {
    "id": "01-design-patterns\\gis-intelligence.md",
    "title": "Pattern: Geographic Information Systems (GIS) Intelligence",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/gis-intelligence.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\gis-intelligence.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Geographic Information Systems (GIS) Intelligence"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Geographic Information Systems (GIS) Intelligence ## Business Value - Provide planners and field operators with timely spatial insights that optimise asset placement, maintenance schedules, and emergency response routing. - Fuse heterogeneous geospatial datasets to uncover growth opportunities, risk hotspots",
    "content": "# Pattern: Geographic Information Systems (GIS) Intelligence ## Business Value - Provide planners and field operators with timely spatial insights that optimise asset placement, maintenance schedules, and emergency response routing. - Fuse heterogeneous geospatial datasets to uncover growth opportunities, risk hotspots, and environmental impacts with defensible evidence. - Reduce manual GIS workload by exposing natural-language interfaces that democratise access to complex spatial analytics. ## Technical Architecture 1. **Data Ingestion & Curation**: Stream satellite imagery, lidar, IoT sensors, cadastral records, and mobility data into a spatial data lake with licence metadata. 2. **Spatial Indexing**: Generate vector tiles, raster pyramids, and geohash indexes using PostGIS/BigQuery GIS to power fast proximity and overlay queries. 3. **Analytical Services**: Blend traditional spatial joins with ML models (segmentation, object detection) and route optimisation engines. 4. **Conversational Layer**: LLM interface grounded in metadata schemas translates natural-language questions into SQL/GeoSPARQL while enforcing permission checks. 5. **Visualisation & Actioning**: Interactive maps (Mapbox/Deck.gl) integrate with existing GIS platforms (Esri, QGIS) and trigger downstream workflows (work orders, alerts). ## Discovery Questions - Which data providers and licences govern usage, and how frequently must layers be refreshed to stay decision-grade? - What spatial resolution, coordinate systems, and accuracy tolerances are required for regulatory or engineering sign-off? - How will outputs integrate with current GIS tooling, mobile field apps, or ERP systems powering operations? - What access controls and anonymisation are needed to protect sensitive locations (critical infrastructure, personal address data)? ## Bill of Materials - Spatial storage/compute: PostGIS, BigQuery GIS, or GeoMesa on a data lake; tile rendering (Mapbox GL Native, Tegola). - ML pipeline: Raster analy"
  },
  {
    "id": "01-design-patterns\\governance-compliance.md",
    "title": "Pattern: Governance & Compliance Automation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/governance-compliance.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\governance-compliance.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Governance & Compliance Automation"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Governance & Compliance Automation ## Business Value - Provide end-to-end traceability for AI systems, ensuring policies, risk controls, and regulatory obligations are satisfied without slowing delivery teams. - Reduce manual evidence collection and audit preparation by codifying governance workflows, checkl",
    "content": "# Pattern: Governance & Compliance Automation ## Business Value - Provide end-to-end traceability for AI systems, ensuring policies, risk controls, and regulatory obligations are satisfied without slowing delivery teams. - Reduce manual evidence collection and audit preparation by codifying governance workflows, checklists, and automated controls. - Increase organisational trust in AI deployments through transparent reporting of model lineage, performance, and policy adherence. ## Technical Architecture 1. **Policy Repository**: Structured catalog of regulations, internal standards, and control templates linked to products and models. 2. **Workflow Orchestration**: Automated gating (ServiceNow/Jira) that enforces risk assessments, approvals, and segregation-of-duties before promotion. 3. **Evidence Collection**: Integrations with MLOps, data platforms, and observability tools to ingest artefacts (datasets, model cards, evaluation reports). 4. **Continuous Monitoring**: Real-time checks on drift, bias, privacy, and access logs with escalations to compliance officers. 5. **Reporting & Audit**: Dashboards and exportable audit packages summarising status by regulation, business unit, and control effectiveness. ## Discovery Questions - Which external regulations (EU AI Act, GDPR, SOX, HIPAA) and internal policies apply to the AI portfolio? - How are models currently catalogued, and what metadata is missing to satisfy governance requirements? - Who are the control owners, reviewers, and approvers, and what SLAs must workflows meet? - What evidence is required for audits, and how long must it be retained or made discoverable? ## Bill of Materials - Governance platform (Azure AI Studio governance, Credo AI, or custom) with metadata catalog (DataHub, Collibra) and policy engine (OPA). - Integration adapters for ML pipelines (MLflow, SageMaker), data lineage (OpenLineage), and observability (Langfuse, EvidentlyAI). - Workflow automation (ServiceNow, Jira, Workato) connecting "
  },
  {
    "id": "01-design-patterns\\insurance-rate-modeling.md",
    "title": "Pattern: Insurance Rate Modeling",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/insurance-rate-modeling.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\insurance-rate-modeling.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Insurance Rate Modeling"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Insurance Rate Modeling ## Business Value - Achieve profitable growth by aligning underwriting prices with granular risk signals while maintaining competitiveness within regulatory bounds. - Accelerate rate filing cycles through reusable actuarial assets, automated documentation, and transparent justificatio",
    "content": "# Pattern: Insurance Rate Modeling ## Business Value - Achieve profitable growth by aligning underwriting prices with granular risk signals while maintaining competitiveness within regulatory bounds. - Accelerate rate filing cycles through reusable actuarial assets, automated documentation, and transparent justification of pricing changes. - Enhance cross-functional trust by exposing clear explanations of model drivers, fairness metrics, and compliance attestations. ## Technical Architecture 1. **Data Foundation**: Consolidate policy, claims, exposure, and third-party datasets (credit, weather, telematics) into a governed warehouse and feature store. 2. **Model Development**: Train GLMs, gradient boosting, or hybrid models with monotonic constraints; use LLMs to extract structured features from unstructured submissions. 3. **Validation & Explainability**: Apply cross-validation, stress tests, SHAP values, and fairness assessments across protected classes and geographies. 4. **Deployment & Governance**: Register approved models, manage champion/challenger rollouts, and integrate with workflow systems for actuarial and regulatory approvals. 5. **Monitoring & Feedback**: Continuously track loss ratio, hit ratio, drift, and regulatory KPIs; feed performance back into retraining pipelines. ## Discovery Questions - Which product lines and jurisdictions are in scope, and what specific regulatory filings (SERFF, PRA) govern them? - What external data sources add underwriting lift, and what contracts or privacy constraints accompany them? - How frequently must rates be refreshed, and what is the change management process for approvals and communication to distribution partners? - What level of explainability is required for regulators, and how will narratives be generated for filings or producer communications? ## Bill of Materials - Data platform (Snowflake, Databricks) with feature store (Feast/Tecton) and lineage tracking (OpenLineage). - Model development stack: scikit-l"
  },
  {
    "id": "01-design-patterns\\model-lifecycle-management.md",
    "title": "Pattern: Model Lifecycle Management (MLOps)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/model-lifecycle-management.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\model-lifecycle-management.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Model Lifecycle Management (MLOps)"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Model Lifecycle Management (MLOps) ## Business Value - Enable teams to deploy AI services rapidly and safely by providing consistent tooling for data prep, experimentation, deployment, and monitoring. - Reduce outages and performance regressions through automated testing, observability, and rollback strategi",
    "content": "# Pattern: Model Lifecycle Management (MLOps) ## Business Value - Enable teams to deploy AI services rapidly and safely by providing consistent tooling for data prep, experimentation, deployment, and monitoring. - Reduce outages and performance regressions through automated testing, observability, and rollback strategies anchored in measurable SLOs. - Maintain regulatory and organisational trust by ensuring every model or prompt change is auditable, reproducible, and cost-aware. ## Technical Architecture 1. **Data & Artefact Versioning**: Source-controlled datasets (DVC/LakeFS), feature stores, and prompt repositories capture provenance of training signals. 2. **Training & Evaluation Pipelines**: CI/CD orchestrates model training, fine-tuning, or prompt tuning with automated regression, bias, and robustness tests. 3. **Registry & Promotion**: Approved artefacts stored in MLflow/W&B with metadata (datasets, hyperparameters, eval scores) and staged promotion workflows. 4. **Deployment Engine**: Supports multiple targets (REST, streaming, batch, function-as-a-service) with canary/shadow rollouts and policy-aware routing. 5. **Monitoring & Feedback**: Unified observability pipeline tracks latency, cost, hallucination rates, and business KPIs; triggers retraining/reversion when thresholds breach. ## Discovery Questions - What environments (dev/test/prod) exist and how are deployment approvals or rollback actions governed across them? - Which compliance requirements mandate lineage tracking, data retention, or redaction for training and inference logs? - How are latency, reliability, and cost budgets defined for each product tier, and who owns them? - What is the current incident response process for model failures, and how will AI-specific runbooks integrate? ## Bill of Materials - Source control & CI (GitHub/GitLab, Jenkins) with pipeline orchestration (Argo Workflows, GitHub Actions) and IaC (Terraform). - Experimentation stack: MLflow/Weights & Biases, prompt manageme"
  },
  {
    "id": "01-design-patterns\\orchestration-workflow.md",
    "title": "Pattern: Intelligent Orchestration Workflow",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/orchestration-workflow.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\orchestration-workflow.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: Intelligent Orchestration Workflow"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: Intelligent Orchestration Workflow ## Business Value - Break complex, multi-step AI tasks into reliable, reusable workflows that coordinate humans, classical services, and AI agents. - Increase throughput and consistency of operations (claims handling, onboarding, content approvals) without losing governance",
    "content": "# Pattern: Intelligent Orchestration Workflow ## Business Value - Break complex, multi-step AI tasks into reliable, reusable workflows that coordinate humans, classical services, and AI agents. - Increase throughput and consistency of operations (claims handling, onboarding, content approvals) without losing governance checkpoints. - Provide visibility into end-to-end process health, service levels, and bottlenecks for continuous improvement. ## Technical Architecture 1. **Process Modelling**: Declarative workflow definitions (BPMN, state machines) capture tasks, branching logic, SLAs, and escalation paths. 2. **Task Execution Layer**: Mix of deterministic microservices, human tasks, and AI agents executed via orchestration runtime (Temporal, Camunda, Airflow). 3. **Context Management**: Shared data layer maintains case files, documents, and embeddings accessible to each step with strict access policies. 4. **Monitoring & SLAs**: Real-time metrics on task durations, queue lengths, and outcomes feed dashboards and automated escalation policies. 5. **Change & Experimentation**: Feature flags and A/B testing frameworks allow incremental rollout of new agents or task strategies. ## Discovery Questions - Which business processes are in scope, and what are the critical SLAs or compliance checkpoints within them? - How will human workers interact with the workflow (inbox, email, custom UI), and what training or change management is required? - What systems of record must be integrated, and how are credentials or API access managed? - What auditability is expected (event logs, approvals, artefact retention), and who consumes the reports? ## Bill of Materials - Orchestration engine (Temporal, Camunda, Airflow) with scalable task queue and retry semantics. - Integration services: API gateway, connector library, RPA bots if legacy screen automation is required. - AI components: LLM agent framework (LangChain, Semantic Kernel), RAG knowledge base, evaluation harness for agent p"
  },
  {
    "id": "01-design-patterns\\performance-optimization.md",
    "title": "Pattern: AI Performance Optimization",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/performance-optimization.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\performance-optimization.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: AI Performance Optimization"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: AI Performance Optimization ## Business Value - Maximise responsiveness and throughput of AI services, improving user experiences and unlocking higher transaction volumes at lower unit cost. - Enable teams to experiment rapidly by providing predictable performance envelopes and capacity planning insights. - ",
    "content": "# Pattern: AI Performance Optimization ## Business Value - Maximise responsiveness and throughput of AI services, improving user experiences and unlocking higher transaction volumes at lower unit cost. - Enable teams to experiment rapidly by providing predictable performance envelopes and capacity planning insights. - Reduce infrastructure spend by right-sizing models, leveraging hardware acceleration, and caching intelligently. ## Technical Architecture 1. **Profiling & Benchmarking**: Automated load tests measure latency, throughput, and token compute across representative workloads. 2. **Model Optimisation**: Techniques such as quantisation, distillation, sparse attention, or retrieval caching reduce inference costs while maintaining quality. 3. **Serving Infrastructure**: Scalable serving stack (Kubernetes + KServe/Triton) with autoscaling, request batching, and multi-model hosting. 4. **Edge & Caching Strategies**: Response caching, embedding reuse, and on-device inference for low-latency scenarios where feasible. 5. **Observability & Feedback**: Real-time telemetry for latency, cost per request, and quality metrics drives dynamic load shedding and routing decisions. ## Discovery Questions - What latency and throughput targets are required for each user journey, and how do they vary by customer segment or geography? - Which hardware platforms (CPU, GPU, TPU) are available, and what is the cost profile or utilisation targets for each? - How frequently do models or prompts change, and how will performance regressions be detected during rollout? - Where are caching or edge deployments acceptable considering data residency, privacy, and update cadences? ## Bill of Materials - Benchmarking tools (Locust, k6), profiling suites (PyTorch Profiler, Nvidia Nsight) and synthetic workload generators. - Optimisation libraries: ONNX Runtime, TensorRT, DeepSpeed, vLLM for context caching and parallelism. - Serving platform: Kubernetes with auto-scaling (HPA/KEDA), load balanc"
  },
  {
    "id": "01-design-patterns\\vulnerability-management.md",
    "title": "Pattern: AI-Powered Vulnerability Management",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/01-design-patterns/vulnerability-management.md",
    "section": "Design Patterns",
    "path": "01-design-patterns\\vulnerability-management.md",
    "headings": [
      {
        "level": 1,
        "text": "Pattern: AI-Powered Vulnerability Management"
      },
      {
        "level": 2,
        "text": "Business Value"
      },
      {
        "level": 2,
        "text": "Technical Architecture"
      },
      {
        "level": 2,
        "text": "Discovery Questions"
      },
      {
        "level": 2,
        "text": "Bill of Materials"
      },
      {
        "level": 2,
        "text": "Risks & Controls"
      }
    ],
    "excerpt": "# Pattern: AI-Powered Vulnerability Management ## Business Value - Shrink mean time to remediate critical vulnerabilities by continuously triaging findings against business impact, exploitability, and exposure. - Provide security and engineering teams with contextual narratives, remediation guidance, and automated work",
    "content": "# Pattern: AI-Powered Vulnerability Management ## Business Value - Shrink mean time to remediate critical vulnerabilities by continuously triaging findings against business impact, exploitability, and exposure. - Provide security and engineering teams with contextual narratives, remediation guidance, and automated workflow routing to accelerate closure. - Improve compliance posture by generating defensible evidence of coverage, prioritisation rationale, and SLA adherence. ## Technical Architecture 1. **Data Aggregation**: Normalise scanner outputs (Qualys, Tenable), asset inventory, CMDB, threat intel, and business metadata into a unified knowledge graph. 2. **Risk Scoring Engine**: Combine rule-based scoring (CVSS, EPSS) with ML models that account for asset criticality, compensating controls, and exploit chatter. 3. **AI Triage Assistant**: LLM-generated summaries highlight potential impact, recommended fixes, and link to patches or knowledge docs with citations. 4. **Workflow Integration**: Automated ticketing (Jira/ServiceNow), change window validation, and exception management with approval workflows. 5. **Monitoring & Reporting**: Dashboards track coverage, SLA compliance, trending vulnerabilities, and residual risk by business unit. ## Discovery Questions - How accurate and complete is the asset inventory, and how are orphaned systems discovered or reconciled? - What patching cadences and maintenance windows exist across infrastructure types (cloud, OT, endpoints)? - Which regulatory or certification frameworks (SOC 2, ISO 27001, NIST 800-53) dictate reporting frequencies and evidence requirements? - How are vulnerability exceptions approved, reviewed, and revisited over time? ## Bill of Materials - Data platform: graph or document store (Neo4j, Elastic Common Schema) with streaming ingestion (Kafka) from scanners and CMDB. - Threat intelligence enrichers (MISP, Recorded Future), ML pipeline for prioritisation, and LLM service (Azure OpenAI, Bedrock) for summ"
  },
  {
    "id": "02-learning-paths\\100-hour-ai-architect.md",
    "title": "100-Hour AI Architect Plan",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/100-hour-ai-architect.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\100-hour-ai-architect.md",
    "headings": [
      {
        "level": 1,
        "text": "100-Hour AI Architect Plan"
      },
      {
        "level": 2,
        "text": "Orientation (Week 0, 4h)"
      },
      {
        "level": 2,
        "text": "Week 1 — Retrieval Foundations (26h)"
      },
      {
        "level": 2,
        "text": "Week 2 — Agents, Tooling, and Evaluations (25h)"
      },
      {
        "level": 2,
        "text": "Week 3 — Governance, Cost, and Performance (24h)"
      },
      {
        "level": 2,
        "text": "Week 4 — Specialisation and Portfolio (25h)"
      },
      {
        "level": 2,
        "text": "Sustain & Teach Forward"
      }
    ],
    "excerpt": "# 100-Hour AI Architect Plan Time-boxed to four sprints (~25 hours each) with optional orientation prep. Every week pairs context, guided study, and a tangible artifact. ## Orientation (Week 0, 4h) - **Micro-learning boost:** Pick two modules from [Micro-Learning Atlas](micro-learning.md) to reinforce personal gaps. Lo",
    "content": "# 100-Hour AI Architect Plan Time-boxed to four sprints (~25 hours each) with optional orientation prep. Every week pairs context, guided study, and a tangible artifact. ## Orientation (Week 0, 4h) - **Micro-learning boost:** Pick two modules from [Micro-Learning Atlas](micro-learning.md) to reinforce personal gaps. Log outcomes in the [Learning Logbook](logbook.md). - **Setup:** Clone the repo, skim , bookmark the live site, and sync your AI tools. - **Baseline:** Document current strengths + gaps using (create if not already started). - **Intent:** Capture a one-page learning goal brief; agree with mentor/manager if you have one. ## Week 1 — Retrieval Foundations (26h) **North star:** Stand up trustworthy retrieval and answer flows. | Day | Focus | Hands-on Output | Key References | | --- | --- | --- | --- | | 1 | Tokenization, embeddings, vector math | Notebook comparing embedding models on sample corpus | , | | 2 | Chunking + hybrid search patterns | Chunk strategy doc with rationale and scoring table | , | | 3 | Build search service (pgvector/Qdrant) | Running index + search API with tests | | | 4 | RAG baseline with citations | CLI or notebook app answering top 5 domain questions | | | 5 | Retro + storytelling | 2-page architecture note + loom-style walk-through | , | **Assessment checklist** - Query latency < 1.5s for top 20 questions in sample set. - Demo recorded and linked in . - Risks & next bets logged in your project README. ## Week 2 — Agents, Tooling, and Evaluations (25h) **North star:** Chain tools together with observability you trust. | Day | Focus | Hands-on Output | Key References | | --- | --- | --- | --- | | 1 | Function calling + tool design | Agent spec describing tasks, tools, guardrails | , | | 2 | LangGraph / orchestration lab | Flow diagram + simulated run traces | | | 3 | Observability setup (Langfuse, metrics) | Project instrumented with traces + dashboards | , | | 4 | Eval harness + CI | Promptfoo suite covering faithfulness, toxicity"
  },
  {
    "id": "02-learning-paths\\agentic-code-swarms.md",
    "title": "Learning Path: Agentic Code Swarms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/agentic-code-swarms.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\agentic-code-swarms.md",
    "headings": [
      {
        "level": 1,
        "text": "Learning Path: Agentic Code Swarms"
      },
      {
        "level": 2,
        "text": "Prereqs"
      },
      {
        "level": 2,
        "text": "Modules"
      },
      {
        "level": 2,
        "text": "Hands-On Steps"
      },
      {
        "level": 2,
        "text": "Challenges"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "Grading Rubric"
      }
    ],
    "excerpt": "# Learning Path: Agentic Code Swarms Audience: AI Architects and Builders using Codex, Claude Code, or Cursor. Outcome: Ship a working multi-agent swarm, understand orchestration patterns, and build a visual explorer. Time: 10–16 hours (self-paced) ## Prereqs - Python 3.11 - Optional: OpenAI/Anthropic key (LiteLLM). Ot",
    "content": "# Learning Path: Agentic Code Swarms Audience: AI Architects and Builders using Codex, Claude Code, or Cursor. Outcome: Ship a working multi-agent swarm, understand orchestration patterns, and build a visual explorer. Time: 10–16 hours (self-paced) ## Prereqs - Python 3.11 - Optional: OpenAI/Anthropic key (LiteLLM). Otherwise runs offline. ## Modules 1) Concepts and Patterns (1h) - Roles, messages, tools, orchestrators - Coordination: sequential, round-robin, planner–worker–reviewer 2) Core Setup (1h) - , venv, run Hello Swarm 3) Orchestration (2h) - Inspect and extend P–W–R with retries and critiques 4) Tools (2h) - Add a and tool 5) Visual Explorer (2h) - Use , add controls and message tracing 6) SaaS Planner (2–4h) - Extend to output a BoM and architecture diagram 7) Evaluate (2h) - Design acceptance checks and simple evals (see ) ## Hands-On Steps ## Challenges - Add a agent that blocks the final answer if tests fail - Add tracing to show token usage per agent (mock values offline) - Swap providers (OpenAI ↔ Anthropic) via LiteLLM config only ## Deliverables - Working swarm with at least 3 roles - A recorded screen demo of the Streamlit explorer - A short doc: architecture, trade-offs, and next steps ## Grading Rubric - Functionality (40%): agents coordinate, produce coherent outputs - Clarity (30%): code readability, clear prompts, thoughtful defaults - UX (20%): UI communicates roles, history, and decisions visually - Rigor (10%): eval checks or tests"
  },
  {
    "id": "02-learning-paths\\beginner.md",
    "title": "Beginner Path (4 Weeks)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/beginner.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\beginner.md",
    "headings": [
      {
        "level": 1,
        "text": "Beginner Path (4 Weeks)"
      },
      {
        "level": 2,
        "text": "Weekly Flow"
      },
      {
        "level": 2,
        "text": "Practice Rituals"
      },
      {
        "level": 2,
        "text": "Graduation Checklist"
      }
    ],
    "excerpt": "# Beginner Path (4 Weeks) A ramp for builders spending ~6 hours/week. Pair it with the self-assessment to track progress. ## Weekly Flow | Week | Focus | Outcomes | Guided Assets | | --- | --- | --- | --- | | 1 | LLM fundamentals + safe prompting | Cheatsheet covering tokens, sampling, prompt patterns, safety guardrail",
    "content": "# Beginner Path (4 Weeks) A ramp for builders spending ~6 hours/week. Pair it with the self-assessment to track progress. ## Weekly Flow | Week | Focus | Outcomes | Guided Assets | | --- | --- | --- | --- | | 1 | LLM fundamentals + safe prompting | Cheatsheet covering tokens, sampling, prompt patterns, safety guardrails | , | | 2 | Retrieval + embeddings + pgvector basics | Hands-on notebook building hybrid retrieval over your own notes | , | | 3 | Intro agents + tool orchestration | Simple agent that calls two tools (search + summarise) with evaluation checklist | , | | 4 | Evaluations + observability | Promptfoo suite with faithfulness + toxicity checks, retro using | , | ## Practice Rituals - Assign one module per week from the [Micro-Learning Atlas](micro-learning.md) to reinforce hands-on practice. Log artefacts in . - Ship a tiny artifact each week (notebook, README, Loom) and store it in . - Use the as a visual anchor for study groups. - Run a 30-minute wrap-up retro every Friday using the prompts in . ## Graduation Checklist - You can explain how embeddings, prompts, and guardrails work together. - You have a working retrieval notebook and a basic agent script with evals. - You can run Langfuse locally and interpret trace metrics."
  },
  {
    "id": "02-learning-paths\\bootcamp.md",
    "title": "Bootcamp Path (AI CoE Inspired)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/bootcamp.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\bootcamp.md",
    "headings": [
      {
        "level": 1,
        "text": "Bootcamp Path (AI CoE Inspired)"
      },
      {
        "level": 2,
        "text": "Week 1 — Vision, Patterns, Customer Journeys"
      },
      {
        "level": 2,
        "text": "Week 2 — Architecture, Integration, Operations"
      },
      {
        "level": 2,
        "text": "Week 3 — Industry Focus, Launch, Storytelling"
      },
      {
        "level": 2,
        "text": "Final Gate Review"
      }
    ],
    "excerpt": "# Bootcamp Path (AI CoE Inspired) Three intense weeks (~35 hours/week) designed for leaders building or scaling internal AI programs. ## Week 1 — Vision, Patterns, Customer Journeys - Kick off with the [Micro-Learning Atlas](micro-learning.md) modules for Context Engineering and Executive Narrative to prime stakeholder",
    "content": "# Bootcamp Path (AI CoE Inspired) Three intense weeks (~35 hours/week) designed for leaders building or scaling internal AI programs. ## Week 1 — Vision, Patterns, Customer Journeys - Kick off with the [Micro-Learning Atlas](micro-learning.md) modules for Context Engineering and Executive Narrative to prime stakeholders. - **Discovery workshops:** Facilitate the questions in with stakeholders. - **Experience blueprint:** Pair with the hero image to outline end-to-end journeys. - **Outcome:** A signed-off narrative including personas, success metrics, and high-level roadmap stored in . ## Week 2 — Architecture, Integration, Operations - **Deep dives:** Map reference architectures using + . - **Implementation lab:** Stand up a production-ready retrieval service and an agent workflow with observability. - **Governance cockpit:** Complete DPIA, risk matrix, and policy guardrails; review with security using templates. - **Outcome:** Architecture dossier + cost model + operational checklists. ## Week 3 — Industry Focus, Launch, Storytelling - **Choose vertical:** Use and to pin down compliance + integration needs. - **Launch playbook:** Produce GTM briefs, enablement docs, and a walkthrough video starring the mentor persona. - **Community loop:** Publish a thought-leadership article in and schedule a governance retro. - **Outcome:** Executive-ready program brief, demo assets, and a 90-day execution plan. ## Final Gate Review Use the template. You are ready to scale if: 1. Strategy, architecture, and governance docs are linked from a single source of truth. 2. A working prototype demonstrates value + guardrails, with eval data attached. 3. Comms plan (slides, poster, article) is share-ready for leadership and customers."
  },
  {
    "id": "02-learning-paths\\logbook.md",
    "title": "Learning Logbook",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/logbook.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\logbook.md",
    "headings": [
      {
        "level": 1,
        "text": "Learning Logbook"
      },
      {
        "level": 2,
        "text": "Weekly Summary"
      },
      {
        "level": 2,
        "text": "Links"
      },
      {
        "level": 2,
        "text": "Reflection Prompts"
      }
    ],
    "excerpt": "# Learning Logbook Track daily progress, insights, and blockers. Copy this template per cohort or per individual. | Date | Focus | Time Spent | Key Insight | Blockers | Next Action | | --- | --- | --- | --- | --- | --- | | | | | | | | ## Weekly Summary - Highlights - Metrics movement (latency, cost, eval scores) - Stak",
    "content": "# Learning Logbook Track daily progress, insights, and blockers. Copy this template per cohort or per individual. | Date | Focus | Time Spent | Key Insight | Blockers | Next Action | | --- | --- | --- | --- | --- | --- | | | | | | | | ## Weekly Summary - Highlights - Metrics movement (latency, cost, eval scores) - Stakeholder feedback ## Links - Demos / notebooks - Slides / memos - Guardrail or governance updates ## Reflection Prompts - What pattern repeated this week? - What surprised you? - Where can you teach or document a new lesson?"
  },
  {
    "id": "02-learning-paths\\micro-learning.md",
    "title": "Micro-Learning Playlists",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-learning.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-learning.md",
    "headings": [
      {
        "level": 1,
        "text": "Micro-Learning Playlists"
      },
      {
        "level": 2,
        "text": "Week Sprint Templates"
      },
      {
        "level": 3,
        "text": "Sprint A — Retrieval Excellence"
      },
      {
        "level": 3,
        "text": "Sprint B — Agentic Delivery"
      },
      {
        "level": 3,
        "text": "Sprint C — Governance Ready"
      },
      {
        "level": 3,
        "text": "Sprint D — Storytelling & Adoption"
      },
      {
        "level": 3,
        "text": "Sprint E � Creator Studio Launch"
      },
      {
        "level": 2,
        "text": "Implementation Tips"
      }
    ],
    "excerpt": "# Micro-Learning Playlists Mix and match the [Micro-Learning Atlas](micro-modules/README.md) modules to build targeted sprints. Each playlist fits inside a week (~5 hours) and pairs with deliverables from the main curriculum. ## Week Sprint Templates ### Sprint A — Retrieval Excellence 1. [Context Engineering for LLMs]",
    "content": "# Micro-Learning Playlists Mix and match the [Micro-Learning Atlas](micro-modules/README.md) modules to build targeted sprints. Each playlist fits inside a week (~5 hours) and pairs with deliverables from the main curriculum. ## Week Sprint Templates ### Sprint A — Retrieval Excellence 1. [Context Engineering for LLMs](micro-modules/foundations-context-engineering.md) 2. [Hybrid Ranking Blueprint](micro-modules/retrieval-hybrid-ranking.md) 3. [RAG Guardrails Fast Track](micro-modules/retrieval-rag-guardrails.md) 4. [Domain RAG Clinic (Healthcare)](micro-modules/retrieval-domain-rag-healthcare.md) 5. **Ship:** Update your domain RAG README with guardrails + metrics. ### Sprint B — Agentic Delivery 1. [LangGraph Planner Loop](micro-modules/agents-langgraph-planner.md) 2. [Multi-Agent Handoff Lab](micro-modules/agents-multi-agent-handoff.md) 3. [Langfuse Telemetry Sprints](micro-modules/operations-langfuse-telemetry.md) 4. **Ship:** Record a demo of planner → reviewer flow and capture traces. ### Sprint C — Governance Ready 1. [Model Risk Review Sprint](micro-modules/governance-model-risk-review.md) 2. [Policy Automation Quick Start](micro-modules/governance-policy-automation.md) 3. [Cost & Latency Playbook](micro-modules/operations-cost-optimization.md) 4. **Ship:** Merge CI policy gate and circulate governance update via [Executive Narrative Builder](micro-modules/storytelling-exec-brief.md). ### Sprint D — Storytelling & Adoption 1. [Evaluation Signals Primer](micro-modules/foundations-evaluation-signals.md) 2. [Evaluation Automation Pipeline](micro-modules/evaluation-automation-pipeline.md) 3. [Adoption & ROI Metrics](micro-modules/storytelling-adoption-metrics.md) 4. [Executive Narrative Builder](micro-modules/storytelling-exec-brief.md) 5. **Ship:** Publish a metrics + narrative bundle in . ### Sprint E � Creator Studio Launch 1. [Creator Content Orchestration Sprint](micro-modules/creator-content-orchestration.md) 2. [Creator Evaluation Scorecards](micro-modules"
  },
  {
    "id": "02-learning-paths\\micro-modules\\README.md",
    "title": "Micro-Learning Atlas",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/README.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Micro-Learning Atlas"
      }
    ],
    "excerpt": "# Micro-Learning Atlas Rapid-fire modules designed for 45-minute deep dives. Each micro-lesson pairs a single outcome with hands-on artefacts, updated for the September 2025 AI architecture landscape. Use this index to stitch custom learning journeys or supplement the 100-Hour Plan. | Category | Module | Purpose | Dura",
    "content": "# Micro-Learning Atlas Rapid-fire modules designed for 45-minute deep dives. Each micro-lesson pairs a single outcome with hands-on artefacts, updated for the September 2025 AI architecture landscape. Use this index to stitch custom learning journeys or supplement the 100-Hour Plan. | Category | Module | Purpose | Duration | | --- | --- | --- | --- | | Foundations | [Context Engineering for LLMs](foundations-context-engineering.md) | Master prompt + retrieval scaffolding using 2025 best practices | 45 min | | Foundations | [Evaluation Signals Primer](foundations-evaluation-signals.md) | Build a minimal eval harness with production-grade metrics | 40 min | | Retrieval Systems | [Hybrid Ranking Blueprint](retrieval-hybrid-ranking.md) | Combine sparse/dense search with rerankers shipping in 2025 | 50 min | | Retrieval Systems | [RAG Guardrails Fast Track](retrieval-rag-guardrails.md) | Implement grounded answers with policy + factual checks | 45 min | | Domain & Vertical | [Domain RAG Clinic (Healthcare)](retrieval-domain-rag-healthcare.md) | Build a healthcare-focused RAG with compliance-ready datasets & evals | 55 min | | Agentic Patterns | [LangGraph Planner Loop](agents-langgraph-planner.md) | Orchestrate task decomposition using LangGraph 0.3 | 45 min | | Agentic Patterns | [Multi-Agent Handoff Lab](agents-multi-agent-handoff.md) | Execute cross-agent workflows with shared memory + audits | 55 min | | Operations & Observability | [Langfuse Telemetry Sprints](operations-langfuse-telemetry.md) | Wire traces, cost, and drift dashboards in one sprint | 40 min | | Operations & Observability | [Cost & Latency Playbook](operations-cost-optimization.md) | Apply batching, caching, and adaptive routing to trim spend | 50 min | | Operations & Observability | [Evaluation Automation Pipeline](evaluation-automation-pipeline.md) | Automate regression suites with promptfoo, Langfuse, and CI gates | 45 min | | Governance & Risk | [Model Risk Review Sprint](governance-model-risk-re"
  },
  {
    "id": "02-learning-paths\\micro-modules\\agents-langgraph-planner.md",
    "title": "LangGraph Planner Loop",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/agents-langgraph-planner.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\agents-langgraph-planner.md",
    "headings": [
      {
        "level": 1,
        "text": "LangGraph Planner Loop"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# LangGraph Planner Loop **Category:** Agentic Patterns **Duration:** 45 minutes **Outcome:** Build a LangGraph 0.3 planner that decomposes tasks, manages memory, and calls tools with guardrails. ## Why it matters - LangGraph 0.3 introduced native tool scheduling, retries, and graph visualisations. - Organisations rely",
    "content": "# LangGraph Planner Loop **Category:** Agentic Patterns **Duration:** 45 minutes **Outcome:** Build a LangGraph 0.3 planner that decomposes tasks, manages memory, and calls tools with guardrails. ## Why it matters - LangGraph 0.3 introduced native tool scheduling, retries, and graph visualisations. - Organisations rely on deterministic planners before enabling autonomous steps. ## Prerequisites - Python 3.11+ environment with LangChain/LangGraph installed. - Toolset: search function, retrieval tool, evaluator (could be simple mock). - Access to at least one high-signal dataset (e.g., company knowledge base). ## Step-by-step 1. **Define nodes:** Create planner, worker, evaluator, and reporter nodes as per . 2. **Configure memory:** Store intermediate states in or . Limit history to 5 steps. 3. **Implement planner:** Use with reasoning tokens enabled (OpenAI o4-mini or Claude 3.5 Sonnet) and define . 4. **Add guardrails:** Integrate the RAG guardrails module for citations + policy enforcement before responses exit the graph. 5. **Visualise:** Render the graph with LangGraph's built-in and save a PNG in . 6. **Validate:** Run three tasks (e.g., \"Draft migration plan\", \"Summarise governance policy\") and capture success/failure metrics in Langfuse. ## Deliverables - LangGraph graph definition file with planner loop and memory config. - PNG of the graph structure for storytelling decks. - Experiment notes in the Logbook + follow-up questions for the Multi-Agent Handoff module. ## References - LangGraph 0.3 release (July 2025) docs. - OpenAI o4 reasoning model best practices (August 2025). - for validating agent behaviours."
  },
  {
    "id": "02-learning-paths\\micro-modules\\agents-multi-agent-handoff.md",
    "title": "Multi-Agent Handoff Lab",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/agents-multi-agent-handoff.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\agents-multi-agent-handoff.md",
    "headings": [
      {
        "level": 1,
        "text": "Multi-Agent Handoff Lab"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Multi-Agent Handoff Lab **Category:** Agentic Patterns **Duration:** 55 minutes **Outcome:** Prototype a two-agent workflow (researcher + reviewer) with explicit handoffs, audit trails, and human-in-the-loop checkpoints. ## Why it matters - 2025 enterprise deployments favour collaborative agents with controlled auton",
    "content": "# Multi-Agent Handoff Lab **Category:** Agentic Patterns **Duration:** 55 minutes **Outcome:** Prototype a two-agent workflow (researcher + reviewer) with explicit handoffs, audit trails, and human-in-the-loop checkpoints. ## Why it matters - 2025 enterprise deployments favour collaborative agents with controlled autonomy. - Clear handoffs reduce hallucinations and improve compliance. ## Prerequisites - Completion of [LangGraph Planner Loop](agents-langgraph-planner.md) module. - Access to a shared datastore (Redis, Supabase) for passing artefacts. - Slack/Teams webhook or email integration for human approvals (optional but recommended). ## Step-by-step 1. **Define roles:** Researcher agent gathers evidence; Reviewer agent validates, adds citations, and triggers approvals. 2. **Model selection:** Use Claude 3.5 Sonnet for research (long context) and GPT-4.1 mini for reviewer (speed). Configure via OpenRouter if centralised billing needed. 3. **Artefact contract:** Create JSON schema (problem, evidence[], risks[], draft_response) stored in shared_state. 4. **Human gate:** Add a webhook step that posts reviewer output to Slack with Approve/Reject buttons (use Slack Workflow Builder or n8n). 5. **Logging:** Persist each handoff with Langfuse spans labelled handoff_researcher and handoff_reviewer. Attach metadata (latency, token cost, approved? yes/no). 6. **Post-mortem:** If rejection occurs, run [Retrospective with AI](../../15-workflows/retrospective-with-ai.md) to capture lessons and update prompts. ## Deliverables - LangGraph agent definitions + orchestration script. - Screenshot or recording of the Slack/Teams approval flow. - Updated governance log noting review cadence and escalation path. ## References - Slack Workflow Builder AI approvals (May 2025 release). - OpenAI Function Calling best practices (June 2025 update). - 16-collaboration/agentic-teams.md for collaboration patterns."
  },
  {
    "id": "02-learning-paths\\micro-modules\\creator-analytics-feedback-loop.md",
    "title": "Creator Analytics Feedback Loop",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/creator-analytics-feedback-loop.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\creator-analytics-feedback-loop.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Analytics Feedback Loop"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Creator Analytics Feedback Loop **Category:** Growth & Insights **Duration:** 45 minutes **Outcome:** Wire marketing analytics back into prompts, briefs, and experimentation plans to keep AI-assisted content improving. ## Why it matters - Performance data often lives in silos; connecting it to the creation pipeline u",
    "content": "# Creator Analytics Feedback Loop **Category:** Growth & Insights **Duration:** 45 minutes **Outcome:** Wire marketing analytics back into prompts, briefs, and experimentation plans to keep AI-assisted content improving. ## Why it matters - Performance data often lives in silos; connecting it to the creation pipeline unlocks rapid iteration. - Continuous learning avoids publishing fatigue and proves the ROI of creator automation investments. ## Prerequisites - Access to analytics exports (YouTube, TikTok, newsletter, web, CRM) with at least engagement and conversion metrics. - Basic familiarity with your data warehouse or BI tool (Mode, Looker, Metabase). - Outcome logs from recent campaigns (OKRs, narrative summaries). ## Step-by-step 1. **Collect key metrics:** Export last 4 weeks of channel performance and standardise columns (asset ID, publish date, watch time, CTR, conversions). 2. **Map to assets:** Link analytics rows to the AI-generated artefacts stored in your DAM or outputs. 3. **Derive insights:** Identify top/bottom performers and tag root causes (hook strength, distribution timing, CTA clarity). 4. **Feed prompts:** Update prompts/templates with insight snippets (e.g., �Lead with metric-driven hook referencing benefit X�). Store versions in . 5. **Schedule experiments:** Plan 2�3 tests for the next sprint (thumbnail variants, narrative angle). Document in . 6. **Automate refresh:** Add a job or reminder to pull analytics weekly and append to for trending dashboards. ## Deliverables - Normalised analytics dataset saved in . - Updated prompt variants capturing data-driven hooks. - Experiment plan summarising tests, success metrics, and owners. ## References - [ ](../../06-toolchains/stack-reference.md) for analytics connectors and warehouses. - [ ](../micro-learning.md) to slot this feedback loop into broader learning sequences. - [ ](../../16-collaboration/escalation-guide.md) for stakeholder communication during iteration."
  },
  {
    "id": "02-learning-paths\\micro-modules\\creator-content-orchestration.md",
    "title": "Creator Content Orchestration Sprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/creator-content-orchestration.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\creator-content-orchestration.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Content Orchestration Sprint"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Creator Content Orchestration Sprint **Category:** Creator Systems **Duration:** 50 minutes **Outcome:** Stand up an AI-assisted production board that takes briefs to multi-channel assets with brand-safe guardrails. ## Why it matters - Creator teams juggle ideation, scripting, editing, and publishing across multiple ",
    "content": "# Creator Content Orchestration Sprint **Category:** Creator Systems **Duration:** 50 minutes **Outcome:** Stand up an AI-assisted production board that takes briefs to multi-channel assets with brand-safe guardrails. ## Why it matters - Creator teams juggle ideation, scripting, editing, and publishing across multiple channels; automation keeps the cadence sustainable. - Structured hand-offs between humans and AI reduce brand risk while allowing experimentation with new formats. ## Prerequisites - Access to a campaign brief or editorial calendar (Notion/Asana export works). - Local copy of [ ](../../01-design-patterns/creator-studio-automation.md). - Prompt tooling (Promptfoo or LangChain notebooks) to test candidate prompts. ## Step-by-step 1. **Map the pipeline:** Sketch the stages (brief ? outline ? script ? edit ? publish) and assign human/AI responsibilities. 2. **Create prompt packs:** Draft structured prompts for outline, hook, CTA, and SEO blurb generation. Store in a folder. 3. **Wire data sources:** Connect brand voice snippets, product FAQs, and performance metrics into a lightweight retrieval layer (e.g., ). 4. **Set review gates:** Define human approval points and add checklist items (style, compliance, accessibility). Capture them in the orchestrator or a Kanban template. 5. **Automate publishing prep:** Generate distribution copy (YouTube description, LinkedIn post, newsletter teaser) reusing the same source snippets. 6. **Log metrics:** Decide which KPIs feed back (CTR, watch time, saves). Add placeholders for analytics ingestion in . ## Deliverables - Pipeline diagram or state machine exported to . - Prompt pack ( ) with rationale and guardrails. - Review checklist embedded in your project management tool or saved as . ## References - [ ](../../03-awesome/portfolio-examples.md) for storytelling inspiration. - [ ](../../06-toolchains/vercel-ai-sdk.md) for streaming editor experiences. - [ ](../../05-projects/eval-automation/README.md) to monitor tone"
  },
  {
    "id": "02-learning-paths\\micro-modules\\creator-evaluation-scorecards.md",
    "title": "Creator Evaluation Scorecards",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/creator-evaluation-scorecards.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\creator-evaluation-scorecards.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Evaluation Scorecards"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Creator Evaluation Scorecards **Category:** Evaluation & QA **Duration:** 40 minutes **Outcome:** Build a repeatable scoring flow that keeps AI-generated creative assets on-brand, accurate, and high-converting. ## Why it matters - Creative output fails fast when tone, metadata, or compliance slip. Quantitative scorec",
    "content": "# Creator Evaluation Scorecards **Category:** Evaluation & QA **Duration:** 40 minutes **Outcome:** Build a repeatable scoring flow that keeps AI-generated creative assets on-brand, accurate, and high-converting. ## Why it matters - Creative output fails fast when tone, metadata, or compliance slip. Quantitative scorecards keep stakeholders aligned. - Evaluations turn subjective feedback into measurable loops that improve prompts, datasets, and human guidance. ## Prerequisites - Access to the latest Promptfoo/Langfuse setup ( ). - Recently published assets or drafts ready for scoring (videos, blog posts, social variants). - Brand or compliance checklist (pull from or legal guidelines). ## Step-by-step 1. **Define success metrics:** Pick 3�5 signals (tone adherence, fact accuracy, CTA strength, SEO keyword coverage, safety). 2. **Instrument Promptfoo:** Add the signals to with expected outcomes, linking to specific tone rules or metadata requirements. 3. **Build human rubric:** Translate signals into a simple 1�5 scale spreadsheet or Notion template for reviewers. 4. **Automate ingestion:** Update to store outputs in and surface them in Langfuse dashboards. 5. **Close the loop:** Present results during weekly creative ops review; adjust prompts or guardrails based on missed thresholds. 6. **Document updates:** Log new findings in and share with stakeholders. ## Deliverables - Updated evaluation dataset targeting creator campaigns ( ). - Scorecard template for human reviewers saved in . - Retro notes summarising key improvements and action items. ## References - [ ](../../05-projects/domain-rag-healthcare/eval/healthcare.json) for structured expectation examples. - [ ](../../07-evaluation/metrics.md) to align score definitions. - [ ](../../15-workflows/retrospective-with-ai.md) for follow-up sessions."
  },
  {
    "id": "02-learning-paths\\micro-modules\\evaluation-automation-pipeline.md",
    "title": "Evaluation Automation Pipeline",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/evaluation-automation-pipeline.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\evaluation-automation-pipeline.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Automation Pipeline"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Evaluation Automation Pipeline **Category:** Operations & Observability **Duration:** 45 minutes **Outcome:** Automate regression evaluations with promptfoo, Langfuse, and CI gates to keep releases safe. ## Why it matters - AI teams now run nightly eval suites; automation prevents regressions from reaching prod. - Ti",
    "content": "# Evaluation Automation Pipeline **Category:** Operations & Observability **Duration:** 45 minutes **Outcome:** Automate regression evaluations with promptfoo, Langfuse, and CI gates to keep releases safe. ## Why it matters - AI teams now run nightly eval suites; automation prevents regressions from reaching prod. - Ties into policy automation requirements for regulated industries. ## Prerequisites - Node.js 18+, Python 3.11. - Langfuse project with API key. - [ ](../../05-projects/eval-automation/README.md) checked out. ## Step-by-step 1. **Define scenarios:** Populate with real-world prompts, expected behaviours, and critical metrics. 2. **Promptfoo config:** Edit to include accuracy, toxicity, and citation checks. Reference templates from [Evaluation Signals Primer](foundations-evaluation-signals.md). 3. **Langfuse integration:** Use to execute promptfoo and sync results to Langfuse synthetic evaluations. 4. **CI gate:** Enable the GitHub Action in to run on every PR. Block merges when metrics drop below thresholds. 5. **Reporting:** Generate the HTML summary via and attach to release notes / [Executive Narrative Builder](storytelling-exec-brief.md). 6. **Continuous improvement:** Schedule weekly evaluation reviews with the [Model Risk Review Sprint](governance-model-risk-review.md). ## Deliverables - Updated + pipeline. - CI job status badge linked in project README. - Langfuse dashboard showing trendlines for key metrics. ## References - Promptfoo 2.0 automation guide (August 2025). - Langfuse synthetic evaluation API docs (July 2025). - GitHub Actions best practices for AI eval gating."
  },
  {
    "id": "02-learning-paths\\micro-modules\\foundations-context-engineering.md",
    "title": "Context Engineering for LLMs",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/foundations-context-engineering.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\foundations-context-engineering.md",
    "headings": [
      {
        "level": 1,
        "text": "Context Engineering for LLMs"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Context Engineering for LLMs **Category:** Foundations **Duration:** 45 minutes **Outcome:** Produce a context blueprint that mixes system prompts, retrieval buffers, and grounding data tuned for enterprise-grade assistants in 2025. ## Why it matters - Llama-3.1, Claude 3.5, and GPT-4.1 ships alongside smaller edge m",
    "content": "# Context Engineering for LLMs **Category:** Foundations **Duration:** 45 minutes **Outcome:** Produce a context blueprint that mixes system prompts, retrieval buffers, and grounding data tuned for enterprise-grade assistants in 2025. ## Why it matters - Llama-3.1, Claude 3.5, and GPT-4.1 ships alongside smaller edge models—each needs explicit context allocation. - Poor prompt hygiene still causes hallucinations; context engineering is faster than model fine-tuning for many use cases. ## Prerequisites - Basic familiarity with prompt templates and retrieval-augmented generation. - Access to an embedding store (Supabase pgvector, Pinecone, or Qdrant) seeded with at least 50 documents. ## Step-by-step 1. **Capture intent:** Interview a stakeholder (or use the prompt in 15-workflows/ai-briefing.md) to extract success criteria, tone, and compliance requirements. 2. **Design the skeleton:** Use the framing in \u00001-design-patterns/content-generation.md to define system + developer + user message slots. 3. **Add retrieval buffers:** Pull top-8 documents via hybrid search (BM25 + dense). Keep the context_window_budget table from \u00005-projects/rag-on-supabase.md handy to stay under token limits. 4. **Layer guardrails:** Insert policy snippets from \u00008-governance/policy-packs.md (e.g., PII redaction or safety clauses). Add anti-hallucination checks referencing etrieval-rag-guardrails.md. 5. **Instrument:** Fire five representative queries through Langfuse using the template in \u00005-projects/evals-langfuse.md. Capture latency, cost, and hallucination metrics. 6. **Debrief:** Summarise findings in the [Learning Logbook](../logbook.md) and flag open risks in 16-collaboration/escalation-guide.md. ## Deliverables - Context blueprint (system/developer prompt plus retrieval plan) committed to your project repo. - Langfuse trace link showing before/after adjustments. - 3 lessons learned ready to share in a retrospective. ## References - Anthropic context engineering whitepaper (Aug 2025 upda"
  },
  {
    "id": "02-learning-paths\\micro-modules\\foundations-evaluation-signals.md",
    "title": "Evaluation Signals Primer",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/foundations-evaluation-signals.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\foundations-evaluation-signals.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Signals Primer"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Evaluation Signals Primer **Category:** Foundations **Duration:** 40 minutes **Outcome:** Stand up a lightweight evaluation harness that scores accuracy, safety, and UX signals for a single workflow. ## Why it matters - 2025-era teams track multi-dimensional signals (faithfulness, bias, latency) before shipping any a",
    "content": "# Evaluation Signals Primer **Category:** Foundations **Duration:** 40 minutes **Outcome:** Stand up a lightweight evaluation harness that scores accuracy, safety, and UX signals for a single workflow. ## Why it matters - 2025-era teams track multi-dimensional signals (faithfulness, bias, latency) before shipping any assistant. - Langfuse, Promptfoo, and Weights & Biases now interoperate—learn the minimal loop. ## Prerequisites - Working RAG or agent prototype (CLI or notebook). - Langfuse project + API key (free tier is fine). - Node.js 18+ installed for promptfoo. ## Step-by-step 1. **Define assertions:** Use \u00007-evaluation/metrics.md to pick 3 KPIs (e.g., faithfulness ≥ 0.7, response time < 2 s, tone = \"compliant\"). 2. **Assemble dataset:** Export 15 representative questions + references from \u00005-projects/rag-on-supabase.md or your own CSV. Store in evaluation/cases.yaml. 3. **Configure promptfoo:** Run px promptfoo init and wire Langfuse integration using the template in \u00005-projects/evals-langfuse.md. 4. **Add custom check:** Implement a short script that flags responses missing citations or violating safety rules (use etrieval-rag-guardrails.md as reference). 5. **Execute & log:** promptfoo eval with parallelism=3. Inspect Langfuse dashboard for latency + cost scatterplots. 6. **Snapshot results:** Export promptfoo report and attach to your project README. Log follow-up work in 15-workflows/postmortem.md if thresholds failed. ## Deliverables - promptfoo.yaml with assertions + custom check. - Langfuse dashboard link showing evaluation traces. - Summary paragraph for the [Executive Narrative Builder](storytelling-exec-brief.md) module. ## References - Promptfoo 2.0 evaluation recipes (July 2025). - Langfuse model quality scorecards blog (May 2025). - \u00007-evaluation/eval-harness.md for advanced setup."
  },
  {
    "id": "02-learning-paths\\micro-modules\\governance-model-risk-review.md",
    "title": "Model Risk Review Sprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/governance-model-risk-review.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\governance-model-risk-review.md",
    "headings": [
      {
        "level": 1,
        "text": "Model Risk Review Sprint"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Model Risk Review Sprint **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Facilitate a cross-functional risk review and produce an updated risk register entry with mitigations. ## Why it matters - Regulators (EU AI Act, UK FCA) expect documented risk reviews for high-impact systems. - Aligns prod",
    "content": "# Model Risk Review Sprint **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Facilitate a cross-functional risk review and produce an updated risk register entry with mitigations. ## Why it matters - Regulators (EU AI Act, UK FCA) expect documented risk reviews for high-impact systems. - Aligns product, legal, and engineering around a shared mitigation plan. ## Prerequisites - Current DPIA or risk log from \u00008-governance/. - Representatives from product, legal/compliance, and engineering (30-minute slot). - Latest telemetry snapshots (Langfuse dashboards, eval reports). ## Step-by-step 1. **Prep pack:** Gather metrics (latency, accuracy, failures) and policy documents. Use template in \u00008-governance/risk-review-template.md (create if missing). 2. **Run session:** Facilitate 20-minute workshop: identify new risks, rate severity/likelihood, assign owners. Use prompts in 16-collaboration/escalation-guide.md. 3. **Update register:** Document new risks (ID, description, severity, mitigation, owner, review date). Store in \u00008-governance/risk-register.csv. 4. **Mitigation backlog:** Create Jira or Linear tickets for critical mitigations. Link to [Policy Automation Quick Start](governance-policy-automation.md) for automation ideas. 5. **Communicate:** Summarise outcomes in the [Executive Narrative Builder](storytelling-exec-brief.md) and share with leadership. 6. **Schedule next review:** Set review cadence (monthly for high risk, quarterly otherwise) and log in the Learning Logbook. ## Deliverables - Updated risk register entry (CSV/Markdown). - Meeting notes with decisions + owners. - Follow-up backlog items for mitigations. ## References - EU AI Act compliance checklist (July 2025 version). - NIST AI RMF Playbook updates (May 2025). - \u00008-governance/model-risk.md for risk categories."
  },
  {
    "id": "02-learning-paths\\micro-modules\\governance-policy-automation.md",
    "title": "Policy Automation Quick Start",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/governance-policy-automation.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\governance-policy-automation.md",
    "headings": [
      {
        "level": 1,
        "text": "Policy Automation Quick Start"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Policy Automation Quick Start **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Automate DPIA/policy workflows using checklists, GitOps, and CI guards. ## Why it matters - Manual policy reviews don’t scale across hundreds of micro-features. - GitOps + policy-as-code ensures every release is tracea",
    "content": "# Policy Automation Quick Start **Category:** Governance & Risk **Duration:** 45 minutes **Outcome:** Automate DPIA/policy workflows using checklists, GitOps, and CI guards. ## Why it matters - Manual policy reviews don’t scale across hundreds of micro-features. - GitOps + policy-as-code ensures every release is traceable. ## Prerequisites - Access to repo workflows (GitHub Actions or GitLab CI). - Governance templates under \u00008-governance/. - Optional: OpenPolicyAgent/Rego knowledge. ## Step-by-step 1. **Codify checklist:** Convert \u00008-governance/checklists.md into YAML (governance/checklist.yaml) capturing required artefacts (DPIA doc, risk score, Langfuse link). 2. **Create CI gate:** Build GitHub Action that blocks PR merge if checklist items missing (use `ctions/github-script or OPA conftest). 3. **Generate DPIA summary:** Use prompt-packs/governance/dpia.nlg.json (create if missing) to auto-generate summaries after each change. 4. **Notify stakeholders:** Integrate with Teams/Slack to ping policy owners when PR flagged. 5. **Archive proofs:** Store generated PDFs in governance/artifacts/ with release tags. 6. **Review impact:** Run the [Model Risk Review Sprint](governance-model-risk-review.md) to confirm automation coverage. ## Deliverables - YAML checklist committed to repo. - CI workflow file enforcing policy automation. - Example DPIA summary attached to release notes. ## References - GitHub Actions OPA policy enforcement (June 2025 blog). - OpenPolicyAgent policy-as-code patterns for AI workflows. - \u00008-governance/privacy-gdpr.md and \u00008-governance/checklists.md."
  },
  {
    "id": "02-learning-paths\\micro-modules\\operations-cost-optimization.md",
    "title": "Cost & Latency Playbook",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/operations-cost-optimization.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\operations-cost-optimization.md",
    "headings": [
      {
        "level": 1,
        "text": "Cost & Latency Playbook"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Cost & Latency Playbook **Category:** Operations & Observability **Duration:** 50 minutes **Outcome:** Reduce per-request cost and latency by at least 20% using batching, caching, and adaptive routing. ## Why it matters - Model pricing volatility in 2025 requires real-time routing to cheaper or faster models. - CFOs ",
    "content": "# Cost & Latency Playbook **Category:** Operations & Observability **Duration:** 50 minutes **Outcome:** Reduce per-request cost and latency by at least 20% using batching, caching, and adaptive routing. ## Why it matters - Model pricing volatility in 2025 requires real-time routing to cheaper or faster models. - CFOs expect cost-per-outcome metrics before approving scale. ## Prerequisites - Baseline metrics from the [Langfuse Telemetry Sprints](operations-langfuse-telemetry.md). - Access to multiple models (OpenAI, Anthropic, Groq, or open-source via OpenRouter). - Cache layer (Redis, Upstash, or Vercel KV). ## Step-by-step 1. **Baseline snapshot:** Export latency & cost per endpoint from Langfuse. 2. **Implement caching:** Cache prompt → response pairs for low-risk endpoints with 1-hour TTL. Invalidate on underlying data updates. 3. **Add batching:** For retrieval-heavy flows, batch embedding requests (OpenAI text-embedding-3-small) and log throughput gains. 4. **Adaptive routing:** Configure Router (e.g., Anyscale Endpoints or custom) to route to Groq Llama-3.1-8B for low complexity tasks, fallback to Claude 3.5 for high complexity (use heuristics/prompt foo scoring). 5. **Measure again:** Compare latency/cost improvements using Langfuse dashboards. Aim for ≥20% improvement. 6. **Document policy:** Update \u00006-toolchains/stack-reference.md with routing rules, and note governance implications in [Policy Automation Quick Start](governance-policy-automation.md). ## Deliverables - Routing + caching code snippet. - Before/after metrics table. - Communication note for exec stakeholders (tie into [Executive Narrative Builder](storytelling-exec-brief.md)). ## References - Groq LPU latency benchmarks (May 2025). - OpenAI API cost management guidelines (July 2025). - Anyscale Router release notes (August 2025)."
  },
  {
    "id": "02-learning-paths\\micro-modules\\operations-langfuse-telemetry.md",
    "title": "Langfuse Telemetry Sprints",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/operations-langfuse-telemetry.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\operations-langfuse-telemetry.md",
    "headings": [
      {
        "level": 1,
        "text": "Langfuse Telemetry Sprints"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Langfuse Telemetry Sprints **Category:** Operations & Observability **Duration:** 40 minutes **Outcome:** Stand up a telemetry baseline in Langfuse covering latency, cost, drift, and dataset health. ## Why it matters - 2025 exec dashboards expect real-time AI performance stats. - Langfuse 2.3 ships drift detection, c",
    "content": "# Langfuse Telemetry Sprints **Category:** Operations & Observability **Duration:** 40 minutes **Outcome:** Stand up a telemetry baseline in Langfuse covering latency, cost, drift, and dataset health. ## Why it matters - 2025 exec dashboards expect real-time AI performance stats. - Langfuse 2.3 ships drift detection, cost anomaly alerts, and synthetic eval hooks. ## Prerequisites - Langfuse project + API key. - Access to your application logs or ability to instrument requests. - Optional: Snowflake/BigQuery for warehouse sync. ## Step-by-step 1. **Instrument client & server:** Use Langfuse middleware for Vercel AI SDK (see \u00006-toolchains/vercel-ai-sdk.md). Ensure trace IDs propagate end-to-end. 2. **Capture metadata:** Log model, temperature, tool invocations, and token counts. Add custom properties for \business_unit, customer_tier. 3. **Enable alerts:** Configure cost anomaly alert (20% spike) and latency SLA alert (p95 > 2s). Route notifications to Slack/Teams. 4. **Drift monitors:** Upload evaluation datasets via Langfuse Synthetic Evaluations. Track accuracy vs last baseline (tie into [Evaluation Signals Primer](foundations-evaluation-signals.md)). 5. **Warehouse sync:** If using Snowflake, enable the Langfuse connector and schedule daily sync for historical analysis. 6. **Snapshot dashboard:** Export charts for latency, cost, and accuracy. Attach to your Logbook and share in the AI Briefing workflow. ## Deliverables - Instrumented code snippet + Langfuse dashboard link. - Alert configuration screenshot. - Drift evaluation report with action items. ## References - Langfuse 2.3 release notes (August 2025). - Vercel AI SDK observability guide (July 2025). - \u00007-evaluation/metrics.md for aligning evaluation fields."
  },
  {
    "id": "02-learning-paths\\micro-modules\\retrieval-domain-rag-healthcare.md",
    "title": "Domain RAG Clinic (Healthcare)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/retrieval-domain-rag-healthcare.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\retrieval-domain-rag-healthcare.md",
    "headings": [
      {
        "level": 1,
        "text": "Domain RAG Clinic (Healthcare)"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Domain RAG Clinic (Healthcare) **Category:** Domain & Vertical **Duration:** 55 minutes **Outcome:** Customise a retrieval-augmented system for healthcare knowledge bases with compliance-ready guardrails. ## Why it matters - Healthcare deployments demand HIPAA-aware pipelines with provenance, consent, and auditabilit",
    "content": "# Domain RAG Clinic (Healthcare) **Category:** Domain & Vertical **Duration:** 55 minutes **Outcome:** Customise a retrieval-augmented system for healthcare knowledge bases with compliance-ready guardrails. ## Why it matters - Healthcare deployments demand HIPAA-aware pipelines with provenance, consent, and auditability. - Domain-tuned retrievers outperform generic embeddings when combined with medical ontologies. ## Prerequisites - Access to de-identified healthcare documents (clinical guidelines, FAQs) in . - Running vector store (Supabase pgvector or Qdrant). - [ ](../../05-projects/domain-rag-healthcare/README.md) cloned locally. ## Step-by-step 1. **Prep corpus:** Load the sample dataset from . Chunk using UMLS-aware segmentation (see ). 2. **Embed with domain model:** Use BioClinicalBERT or OpenAI text-embedding-3-large as configured in . Store metadata (source, revision date, PHI flag). 3. **Hybrid search:** Enable BM25 + dense search with reciprocal rank fusion. Evaluate top-10 precision using the scripts provided. 4. **Guardrails:** Configure policy checks from (PHI redaction, contraindication warnings). Integrate with the [RAG Guardrails Fast Track](retrieval-rag-guardrails.md) steps. 5. **Context blueprint:** Update prompts in with clinician persona instructions and safety disclaimers. 6. **Evaluate:** Run or the CLI to measure factual accuracy and coverage. Log metrics in Langfuse. ## Deliverables - Updated with domain-specific embedding + guardrail settings. - Eval report saved to . - Risk log entry covering residual PHI risks (use [Model Risk Review Sprint](governance-model-risk-review.md)). ## References - NIH ClinicalTrials.gov ontology export (August 2025). - BioClinicalBERT 2025 embeddings performance benchmarks. - for extended walkthrough."
  },
  {
    "id": "02-learning-paths\\micro-modules\\retrieval-hybrid-ranking.md",
    "title": "Hybrid Ranking Blueprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/retrieval-hybrid-ranking.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\retrieval-hybrid-ranking.md",
    "headings": [
      {
        "level": 1,
        "text": "Hybrid Ranking Blueprint"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Hybrid Ranking Blueprint **Category:** Retrieval Systems **Duration:** 50 minutes **Outcome:** Deploy a hybrid (sparse + dense + rerank) pipeline tuned for multi-lingual corpora, aligned with 2025 search benchmarks. ## Why it matters - BM25 + dense embeddings still outperform fancy architectures when tuned together. ",
    "content": "# Hybrid Ranking Blueprint **Category:** Retrieval Systems **Duration:** 50 minutes **Outcome:** Deploy a hybrid (sparse + dense + rerank) pipeline tuned for multi-lingual corpora, aligned with 2025 search benchmarks. ## Why it matters - BM25 + dense embeddings still outperform fancy architectures when tuned together. - 2025 rerankers (Cohere Rerank 5, Voyage Fusion) close accuracy gaps with minimal cost. ## Prerequisites - Supabase with pgvector or a managed vector DB (Pinecone, Qdrant). - Access to Cohere Rerank v5 or Voyage Fusion API key. - Dataset of at least 200 docs spanning two languages. ## Step-by-step 1. **Baseline search:** Follow \u00005-projects/vector-search-pgvector.md to index documents with OpenAI text-embedding-3-large or Nomic v1.5 embeddings. 2. **Add BM25:** Enable pg_search (Supabase) or use Typesense for lexical search. Log top-10 results for 5 seed queries. 3. **Blend scores:** Implement reciprocal rank fusion (see \u00006-toolchains/stack-reference.md). Weight dense=0.6, sparse=0.4 to start. 4. **Apply reranker:** Call Cohere Rerank 5 (or Voyage Fusion) on the top 8 results. Capture latency + cost per query. 5. **Evaluate:** Use the [Evaluation Signals Primer](foundations-evaluation-signals.md) to score NDCG, recall@5, and hallucination rate across 30 queries. 6. **Optimise:** Experiment with multilingual embeddings (e.g., Jina Embeddings v2) and reranker thresholds. Document improvements in the logbook. ## Deliverables - Notebook or script showing fusion + rerank code. - Metrics table comparing baseline vs hybrid vs hybrid+rerank. - Recommendation memo for production rollout (drop into \u00003-awesome/portfolio-examples.md template). ## References - Cohere Rerank v5 changelog (June 2025). - Supabase hybrid search recipe (August 2025 blog). - Voyage Fusion evaluation on multilingual Wikipedia (July 2025)."
  },
  {
    "id": "02-learning-paths\\micro-modules\\retrieval-rag-guardrails.md",
    "title": "RAG Guardrails Fast Track",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/retrieval-rag-guardrails.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\retrieval-rag-guardrails.md",
    "headings": [
      {
        "level": 1,
        "text": "RAG Guardrails Fast Track"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# RAG Guardrails Fast Track **Category:** Retrieval Systems **Duration:** 45 minutes **Outcome:** Wrap a RAG endpoint with safety, grounding, and compliance guardrails ready for pilot launch. ## Why it matters - Regulators expect grounded responses, citation tracking, and audit logs even during pilots. - Tooling like G",
    "content": "# RAG Guardrails Fast Track **Category:** Retrieval Systems **Duration:** 45 minutes **Outcome:** Wrap a RAG endpoint with safety, grounding, and compliance guardrails ready for pilot launch. ## Why it matters - Regulators expect grounded responses, citation tracking, and audit logs even during pilots. - Tooling like Guardrails AI, Protect AI, and LangChain Guardrails matured dramatically in 2025. ## Prerequisites - Existing RAG service (REST or GraphQL) from \u00005-projects/rag-on-supabase.md. - Guardrails AI or OpenAI Moderation v2 API key. - Access to a policy checklist (see \u00008-governance/checklists.md). ## Step-by-step 1. **Catalogue risks:** Use \u00008-governance/model-risk.md to list top failure modes (hallucination, PII leakage, policy violation). 2. **Implement content filters:** Add Guardrails AI YAML config or OpenAI Moderation v2 call before returning answers. Log decisions. 3. **Enforce citation threshold:** Require ≥2 supporting references. If missing, return fallback message with confidence = LOW metadata. 4. **Add anti-hallucination check:** Compare answers with retrieved chunks using Semantic Similarity > 0.76 (Sentence Transformers `ll-MiniLM-L6-v3 works). 5. **Audit logging:** Emit JSON logs to Langfuse or your SIEM capturing prompt, retrieved docs, policy verdicts, and user ID. 6. **Drill compliance:** Run the [Model Risk Review Sprint](governance-model-risk-review.md) module to validate guardrail coverage. ## Deliverables - Guardrails config + enforcement middleware committed to your project. - Audit log sample annotated with pass/fail outcomes. - Updated DPIA or risk register entry. ## References - Guardrails AI 2025 policy pack launch notes. - Protect AI Playbook for RAG pipelines (April 2025). - \u00007-evaluation/metrics.md for grounding metrics."
  },
  {
    "id": "02-learning-paths\\micro-modules\\storytelling-adoption-metrics.md",
    "title": "Adoption & ROI Metrics",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/storytelling-adoption-metrics.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\storytelling-adoption-metrics.md",
    "headings": [
      {
        "level": 1,
        "text": "Adoption & ROI Metrics"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Adoption & ROI Metrics **Category:** Storytelling & Adoption **Duration:** 40 minutes **Outcome:** Define a metrics stack that links AI capability usage to business value. ## Why it matters - AI programs in 2025 succeed when tied to measurable ROI (hours saved, revenue, risk reduction). - Product + finance teams need",
    "content": "# Adoption & ROI Metrics **Category:** Storytelling & Adoption **Duration:** 40 minutes **Outcome:** Define a metrics stack that links AI capability usage to business value. ## Why it matters - AI programs in 2025 succeed when tied to measurable ROI (hours saved, revenue, risk reduction). - Product + finance teams need shared dashboards. ## Prerequisites - Access to product analytics (Amplitude, Mixpanel, internal warehouse) or support logs. - Baseline telemetry from [Langfuse Telemetry Sprints](../micro-modules/operations-langfuse-telemetry.md). - Collaboration with finance/ops partner. ## Step-by-step 1. **Map funnel:** Identify user journey stages (activation, task completion, escalation). Align with \u00002-learning-paths/100-hour-ai-architect.md deliverables. 2. **Select metrics:** Choose 4 metrics: adoption (weekly active users), productivity (tasks per user), quality (eval pass rate), risk (escalations per 100 tasks). 3. **Instrument events:** Add tracking to dashboard/workflows (e.g., log ask_completed, eval_passed). Route to analytics + Langfuse tags. 4. **Calculate ROI:** Collaborate with finance to estimate savings/revenue uplift. Update \u00003-awesome/portfolio-examples.md with case study snippet. 5. **Build dashboard:** Use Metabase, PowerBI, or Looker to visualise metrics. Include trendlines + target thresholds. 6. **Share cadence:** Present metrics in the [Executive Narrative Builder](storytelling-exec-brief.md) and log action items. ## Deliverables - Metrics definition doc (KPIs, formulas, owners). - Dashboard screenshot or share link. - ROI summary paragraph for stakeholder updates. ## References - Gartner AI Product Metrics 2025 report. - Airbnb AI adoption case study (April 2025). - 15-workflows/ai-briefing.md for reporting cadence."
  },
  {
    "id": "02-learning-paths\\micro-modules\\storytelling-exec-brief.md",
    "title": "Executive Narrative Builder",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/micro-modules/storytelling-exec-brief.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\micro-modules\\storytelling-exec-brief.md",
    "headings": [
      {
        "level": 1,
        "text": "Executive Narrative Builder"
      },
      {
        "level": 2,
        "text": "Why it matters"
      },
      {
        "level": 2,
        "text": "Prerequisites"
      },
      {
        "level": 2,
        "text": "Step-by-step"
      },
      {
        "level": 2,
        "text": "Deliverables"
      },
      {
        "level": 2,
        "text": "References"
      }
    ],
    "excerpt": "# Executive Narrative Builder **Category:** Storytelling & Adoption **Duration:** 35 minutes **Outcome:** Produce a board-ready narrative that links AI delivery progress to business outcomes. ## Why it matters - Leaders need crisp proof of value, risks, and next bets each week. - Storytelling drives adoption and budget",
    "content": "# Executive Narrative Builder **Category:** Storytelling & Adoption **Duration:** 35 minutes **Outcome:** Produce a board-ready narrative that links AI delivery progress to business outcomes. ## Why it matters - Leaders need crisp proof of value, risks, and next bets each week. - Storytelling drives adoption and budget for AI programs. ## Prerequisites - Metrics snapshot from Langfuse or your telemetry stack. - Recent evaluation report (see [Evaluation Signals Primer](../micro-modules/foundations-evaluation-signals.md)). - Brand Voice guidance from BRAND-VOICE.md. ## Step-by-step 1. **Gather signals:** Extract top metrics (latency, cost, accuracy, adoption) and add qualitative wins + blockers. 2. **Structure brief:** Use the outline in 15-workflows/ai-briefing.md (headline, metrics pulse, highlights, blockers, next experiments). 3. **Add visuals:** Embed hero image (`ssets/ai-architect-campus.png) and relevant dashboards/screenshot. 4. **Highlight governance:** Summarise risk status + mitigations from [Model Risk Review Sprint](../micro-modules/governance-model-risk-review.md). 5. **Call to action:** Define decisions needed (budget, approvals) and deadlines. 6. **Distribute:** Package as PDF or Loom video; share via exec channel and archive in \u00009-articles/drafts/. ## Deliverables - 1-page executive brief (PDF/Markdown) referencing metrics + risks. - Key slide or graphic for town halls. - Log entry in the Learning Logbook capturing feedback. ## References - 2025 McKinsey AI adoption scorecard. - Stripe Radar storytelling examples (2024-2025). - \u00009-articles/templates/keynote.md for extended decks."
  },
  {
    "id": "02-learning-paths\\professional.md",
    "title": "Professional Path (6 Weeks)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/professional.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\professional.md",
    "headings": [
      {
        "level": 1,
        "text": "Professional Path (6 Weeks)"
      },
      {
        "level": 2,
        "text": "Structure Overview"
      },
      {
        "level": 2,
        "text": "Detailed Plan"
      },
      {
        "level": 2,
        "text": "Mentorship Moments"
      },
      {
        "level": 2,
        "text": "Portfolio & Promotion"
      }
    ],
    "excerpt": "# Professional Path (6 Weeks) Ideal for architects balancing delivery and leadership responsibilities (~12-15 hours/week). ## Structure Overview - **Weeks 1-2:** Solidify retrieval + agent systems while setting baselines for evals. - **Weeks 3-4:** Embed governance, risk, and cost practices into day-to-day operations. ",
    "content": "# Professional Path (6 Weeks) Ideal for architects balancing delivery and leadership responsibilities (~12-15 hours/week). ## Structure Overview - **Weeks 1-2:** Solidify retrieval + agent systems while setting baselines for evals. - **Weeks 3-4:** Embed governance, risk, and cost practices into day-to-day operations. - **Weeks 5-6:** Specialise by domain and create teaching material for your org. ## Detailed Plan | Week | Theme | Focus Areas | Deliverables | | --- | --- | --- | --- | | 1 | Retrieval Excellence | Hybrid search, chunking, multi-tenant design | Search service blueprint + benchmark report | | 2 | Agents & Automation | Tool choreography, LangGraph patterns, failure recovery | Agent flow deck + incident playbook | | 3 | Observability & Evals | Langfuse dashboards, promptfoo CI, data drift alerts | Eval summary + trace library | | 4 | Governance in Practice | DPIA updates, policy mapping, access reviews | Governance cockpit doc + risk board | | 5 | Industry Playbooks | Tailor patterns for a chosen vertical, map integrations | Domain playbook + partner alignment memo | | 6 | Teach & Scale | Build enablement assets, mentor peers, host workshop | Workshop agenda, recording, updated poster | ## Mentorship Moments - Add one [Micro-Learning Atlas](micro-learning.md) module per week to keep skills sharp (e.g., guardrails during Week 3, storytelling during Week 6). - Pair each week with a 30-minute mentor sync using the questions in . - Swap eval results and guardrail ideas with peers via . ## Portfolio & Promotion - Capture new visuals (screenshots, hero art) and append to + as you progress. - Maintain a change log in (create if needed) highlighting wins, metrics, and stories. - When ready, submit a PR to share your vertical playbook or workflow so others can build on it."
  },
  {
    "id": "02-learning-paths\\self-assessment.md",
    "title": "AI Architect Self-Assessment",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/02-learning-paths/self-assessment.md",
    "section": "Learning Paths",
    "path": "02-learning-paths\\self-assessment.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Architect Self-Assessment"
      },
      {
        "level": 2,
        "text": "Profile"
      },
      {
        "level": 2,
        "text": "Current Confidence (rate 1-5)"
      },
      {
        "level": 2,
        "text": "Strengths to Amplify"
      },
      {
        "level": 2,
        "text": "Gaps to Close"
      },
      {
        "level": 2,
        "text": "Learning Objectives (Top 3)"
      },
      {
        "level": 2,
        "text": "Success Signals"
      },
      {
        "level": 2,
        "text": "Support & Resources"
      },
      {
        "level": 2,
        "text": "Reflection Placeholder"
      }
    ],
    "excerpt": "# AI Architect Self-Assessment Capture a snapshot before starting any path, then revisit every 30 days. Copy this file, fill in the prompts, and store it alongside your learning journal. ## Profile - Name / Role / Team - Primary domain (product, data, infra, research, strategy) - Time available per week ## Current Conf",
    "content": "# AI Architect Self-Assessment Capture a snapshot before starting any path, then revisit every 30 days. Copy this file, fill in the prompts, and store it alongside your learning journal. ## Profile - Name / Role / Team - Primary domain (product, data, infra, research, strategy) - Time available per week ## Current Confidence (rate 1-5) | Area | Score | Notes | | --- | --- | --- | | Value framing & storytelling | | | | Retrieval & search systems | | | | Agents & orchestration | | | | Observability & evaluation | | | | Governance & policy collaboration | | | | Shipping to production | | | ## Strengths to Amplify - - ## Gaps to Close - - ## Learning Objectives (Top 3) 1. 2. 3. ## Success Signals - Example metric improvements or artifacts you want by the end of the program. - Stakeholder feedback you need. - Behaviours you want to see in your day-to-day work. ## Support & Resources - Peer / mentor / manager check-ins - Repos, talks, or courses to pair with this playbook - Risks or blockers that might slow progress ## Reflection Placeholder Revisit at the end of each week: What moved? What stayed stuck? What will you change?"
  },
  {
    "id": "03-awesome\\awesome-agents.md",
    "title": "Awesome Agents",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-agents.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-agents.md",
    "headings": [],
    "excerpt": "# Awesome Agents | Link | Why it matters | |---|---| | [microsoft/autogen](https://github.com/microsoft/autogen) | Multi-agent framework with tool support | | [joaomdmoura/crewai](https://github.com/joaomdmoura/crewai) | Agent teams and orchestration | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgrap",
    "content": "# Awesome Agents | Link | Why it matters | |---|---| | [microsoft/autogen](https://github.com/microsoft/autogen) | Multi-agent framework with tool support | | [joaomdmoura/crewai](https://github.com/joaomdmoura/crewai) | Agent teams and orchestration | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph) | Deterministic agent graphs for complex flows |"
  },
  {
    "id": "03-awesome\\awesome-aggregators.md",
    "title": "Awesome Aggregators (Meta Lists)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-aggregators.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-aggregators.md",
    "headings": [
      {
        "level": 1,
        "text": "Awesome Aggregators (Meta Lists)"
      }
    ],
    "excerpt": "# Awesome Aggregators (Meta Lists) High-signal GitHub repos that aggregate the best resources. - sindresorhus/awesome — The canonical index of curated “awesome” lists across topics - awesome-machine-learning/awesome-machine-learning — Curated machine learning resources and libraries - visenger/awesome-mlops — Curated M",
    "content": "# Awesome Aggregators (Meta Lists) High-signal GitHub repos that aggregate the best resources. - sindresorhus/awesome — The canonical index of curated “awesome” lists across topics - awesome-machine-learning/awesome-machine-learning — Curated machine learning resources and libraries - visenger/awesome-mlops — Curated MLOps tools, articles, and best practices - eugeneyan/applied-ml — Applied ML reading list (papers, blog posts, case studies) - papers-we-love/papers-we-love — Foundational computer science and systems papers - f/awesome-chatgpt-prompts — Community prompts and examples for ChatGPT-style models - e2b-dev/awesome-ai-agents — Curated list of AI agent frameworks and tooling Notes - Prefer recent, maintained lists; check stars and last update. - When adding links from these lists, include 1–2 lines on why they matter."
  },
  {
    "id": "03-awesome\\awesome-evals.md",
    "title": "Awesome Evals",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-evals.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-evals.md",
    "headings": [],
    "excerpt": "# Awesome Evals | Link | Why it matters | |---|---| | [langfuse/langfuse](https://github.com/langfuse/langfuse) | Traces, evaluations, and cost tracking | | [Arize-ai/phoenix](https://github.com/Arize-ai/phoenix) | Model and LLM analysis plus evaluation | | [promptfoo/promptfoo](https://github.com/promptfoo/promptfoo) ",
    "content": "# Awesome Evals | Link | Why it matters | |---|---| | [langfuse/langfuse](https://github.com/langfuse/langfuse) | Traces, evaluations, and cost tracking | | [Arize-ai/phoenix](https://github.com/Arize-ai/phoenix) | Model and LLM analysis plus evaluation | | [promptfoo/promptfoo](https://github.com/promptfoo/promptfoo) | Prompt evaluations in CI workflows | | [openai/evals](https://github.com/openai/evals) | Evaluation framework with reusable datasets | | [microsoft/promptflow](https://github.com/microsoft/promptflow) | Flow evaluation, orchestration, and tracking |"
  },
  {
    "id": "03-awesome\\awesome-llms.md",
    "title": "Awesome Llms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-llms.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-llms.md",
    "headings": [],
    "excerpt": "# Awesome LLMs | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Core models and tokenizers for NLP and LLM work | | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | Run models locally on CPU and edge devices | | [vllm-project/vllm](https://gi",
    "content": "# Awesome LLMs | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Core models and tokenizers for NLP and LLM work | | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | Run models locally on CPU and edge devices | | [vllm-project/vllm](https://github.com/vllm-project/vllm) | High-throughput, low-latency model serving | | [ollama/ollama](https://github.com/ollama/ollama) | Local model runtime and packaging | | [openai/openai-python](https://github.com/openai/openai-python) | Official Python SDK for OpenAI APIs | | [openai/openai-cookbook](https://github.com/openai/openai-cookbook) | Production patterns and practical examples |"
  },
  {
    "id": "03-awesome\\awesome-mlops.md",
    "title": "Awesome Mlops",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-mlops.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-mlops.md",
    "headings": [],
    "excerpt": "# Awesome MLOps | Link | Why it matters | |---|---| | [ray-project/ray](https://github.com/ray-project/ray) | Distributed compute for Python and LLM workloads | | [PrefectHQ/prefect](https://github.com/PrefectHQ/prefect) | Modern workflow orchestration | | [temporalio/sdk-typescript](https://github.com/temporalio/sdk-t",
    "content": "# Awesome MLOps | Link | Why it matters | |---|---| | [ray-project/ray](https://github.com/ray-project/ray) | Distributed compute for Python and LLM workloads | | [PrefectHQ/prefect](https://github.com/PrefectHQ/prefect) | Modern workflow orchestration | | [temporalio/sdk-typescript](https://github.com/temporalio/sdk-typescript) | Durable workflows with strong guarantees | | [flyteorg/flyte](https://github.com/flyteorg/flyte) | ML orchestrator built for scale | | [dagster-io/dagster](https://github.com/dagster-io/dagster) | Data and ML orchestration | | [mlflow/mlflow](https://github.com/mlflow/mlflow) | Experiment tracking and model registry | | [bentoml/BentoML](https://github.com/bentoml/BentoML) | Packaging and serving toolkit |"
  },
  {
    "id": "03-awesome\\awesome-rag.md",
    "title": "Awesome Rag",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-rag.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-rag.md",
    "headings": [],
    "excerpt": "# Awesome RAG | Link | Why it matters | |---|---| | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | Composable primitives for RAG and tool use | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph) | Graph and state orchestration for RAG and agents | | [run-llama/llama_index](http",
    "content": "# Awesome RAG | Link | Why it matters | |---|---| | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | Composable primitives for RAG and tool use | | [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph) | Graph and state orchestration for RAG and agents | | [run-llama/llama_index](https://github.com/run-llama/llama_index) | Data framework for indexing and retrieval | | [explodinggradients/ragas](https://github.com/explodinggradients/ragas) | Evaluate RAG quality and faithfulness | | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Vector similarity search inside Postgres | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Vector database with hybrid search |"
  },
  {
    "id": "03-awesome\\awesome-vector-dbs.md",
    "title": "Awesome Vector Dbs",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/awesome-vector-dbs.md",
    "section": "Awesome",
    "path": "03-awesome\\awesome-vector-dbs.md",
    "headings": [],
    "excerpt": "# Awesome Vector Databases | Link | Why it matters | |---|---| | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Postgres extension for vector similarity | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Hybrid",
    "content": "# Awesome Vector Databases | Link | Why it matters | |---|---| | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Postgres extension for vector similarity | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Hybrid search with modular extensions | | [milvus-io/milvus](https://github.com/milvus-io/milvus) | Distributed vector database | | [chroma-core/chroma](https://github.com/chroma-core/chroma) | Lightweight local vector store |"
  },
  {
    "id": "03-awesome\\portfolio-examples.md",
    "title": "Portfolio Examples to Model",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/03-awesome/portfolio-examples.md",
    "section": "Awesome",
    "path": "03-awesome\\portfolio-examples.md",
    "headings": [
      {
        "level": 1,
        "text": "Portfolio Examples to Model"
      },
      {
        "level": 2,
        "text": "Product Stories"
      },
      {
        "level": 2,
        "text": "Technical Deep Dives"
      },
      {
        "level": 2,
        "text": "Suggested Prompts"
      },
      {
        "level": 2,
        "text": "Assets to Capture"
      },
      {
        "level": 2,
        "text": "Share & Iterate"
      }
    ],
    "excerpt": "# Portfolio Examples to Model Use these prompts + resources when shaping your own case studies and demos. ## Product Stories - Stripe Radar launch memo (highlight alignment of metrics + narrative) - GitHub Copilot case study (focus on developer workflow impact) - Intercom Fin AI assistant release notes (clear guardrail",
    "content": "# Portfolio Examples to Model Use these prompts + resources when shaping your own case studies and demos. ## Product Stories - Stripe Radar launch memo (highlight alignment of metrics + narrative) - GitHub Copilot case study (focus on developer workflow impact) - Intercom Fin AI assistant release notes (clear guardrail messaging) ## Technical Deep Dives - OpenAI retrieval plugin walkthrough - Netflix personalization architecture notes - Datadog incident retrospectives (observability framing) ## Suggested Prompts 1. \"Draft a case study using the structure: problem, approach, architecture, metrics, next steps.\" 2. \"Summarise the top 3 stakeholder questions likely to appear for this launch.\" 3. \"Translate the architecture into a slide-friendly diagram narrative.\" ## Assets to Capture - Hero visual ( ) - Mentor persona ( ) - Curriculum poster ( ) - Live product screenshots (see ) ## Share & Iterate - Store drafts in - Use to reflect on feedback - Convert your best story into a talk or webinar outline"
  },
  {
    "id": "04-templates\\bom-template.md",
    "title": "Bill of Materials Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/bom-template.md",
    "section": "Templates",
    "path": "04-templates\\bom-template.md",
    "headings": [
      {
        "level": 1,
        "text": "Bill of Materials Template"
      }
    ],
    "excerpt": "# Bill of Materials Template - Services & SKUs - Usage assumptions - Monthly estimate - Notes & tradeoffs",
    "content": "# Bill of Materials Template - Services & SKUs - Usage assumptions - Monthly estimate - Notes & tradeoffs"
  },
  {
    "id": "04-templates\\discovery-questions.md",
    "title": "Discovery Questions Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/discovery-questions.md",
    "section": "Templates",
    "path": "04-templates\\discovery-questions.md",
    "headings": [
      {
        "level": 1,
        "text": "Discovery Questions Template"
      }
    ],
    "excerpt": "# Discovery Questions Template - Business goals, constraints, KPIs - Data sources, privacy, compliance - Users, workflows, SLAs - Risks and mitigations",
    "content": "# Discovery Questions Template - Business goals, constraints, KPIs - Data sources, privacy, compliance - Users, workflows, SLAs - Risks and mitigations"
  },
  {
    "id": "04-templates\\solution-doc.md",
    "title": "Solution Document Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/solution-doc.md",
    "section": "Templates",
    "path": "04-templates\\solution-doc.md",
    "headings": [
      {
        "level": 1,
        "text": "Solution Document Template"
      }
    ],
    "excerpt": "# Solution Document Template - Problem & value - Architecture overview - Data flows & components - Security & compliance - Rollout & costs",
    "content": "# Solution Document Template - Problem & value - Architecture overview - Data flows & components - Security & compliance - Rollout & costs"
  },
  {
    "id": "04-templates\\technical-architecture.md",
    "title": "Technical Architecture Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/technical-architecture.md",
    "section": "Templates",
    "path": "04-templates\\technical-architecture.md",
    "headings": [
      {
        "level": 1,
        "text": "Technical Architecture Template"
      }
    ],
    "excerpt": "# Technical Architecture Template - Context & assumptions - Component diagram - Data model - Scaling & SLOs - Observability",
    "content": "# Technical Architecture Template - Context & assumptions - Component diagram - Data model - Scaling & SLOs - Observability"
  },
  {
    "id": "04-templates\\workshop-agenda.md",
    "title": "Workshop Agenda Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/04-templates/workshop-agenda.md",
    "section": "Templates",
    "path": "04-templates\\workshop-agenda.md",
    "headings": [
      {
        "level": 1,
        "text": "Workshop Agenda Template"
      }
    ],
    "excerpt": "# Workshop Agenda Template - Intro & goals - Discovery questions - Architecture co-design - Risks & next steps",
    "content": "# Workshop Agenda Template - Intro & goals - Discovery questions - Architecture co-design - Risks & next steps"
  },
  {
    "id": "05-projects\\100-projects.md",
    "title": "100 Projects for AI Architects (Starter Set)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/100-projects.md",
    "section": "Projects",
    "path": "05-projects\\100-projects.md",
    "headings": [
      {
        "level": 1,
        "text": "100 Projects for AI Architects (Starter Set)"
      }
    ],
    "excerpt": "# 100 Projects for AI Architects (Starter Set) Practical builds to master architecture by shipping. Sorted by theme; each project lists difficulty, estimated time, and links to guides or repos. Contribute: PR new projects with a short description, difficulty, est. hours, and links. Keep impact high and scope clear. | #",
    "content": "# 100 Projects for AI Architects (Starter Set) Practical builds to master architecture by shipping. Sorted by theme; each project lists difficulty, estimated time, and links to guides or repos. Contribute: PR new projects with a short description, difficulty, est. hours, and links. Keep impact high and scope clear. | # | Project | Theme | Difficulty | Est. Hours | Links | |---|---|---|---|---|---| | 1 | RAG on Supabase with Citations | RAG | Beginner | 8–12 | [Guide](rag-on-supabase.md) • [pgvector](https://github.com/pgvector/pgvector) | | 2 | Hybrid Search (BM25 + Vectors) | Retrieval | Intermediate | 6–10 | [Concepts](../03-awesome/awesome-vector-dbs.md) | | 3 | Chunking Strategies Benchmark | Retrieval | Intermediate | 6–10 | [RAGAS](https://github.com/explodinggradients/ragas) | | 4 | Evals Harness (Faithfulness, Coverage) | Evaluation | Intermediate | 6–10 | [Langfuse](https://github.com/langfuse/langfuse) | | 5 | Cost Guardrails & Caching Layer | Ops | Intermediate | 6–10 | [openai-cookbook](https://github.com/openai/openai-cookbook) | | 6 | Agent with Tools (Web + DB) | Agents | Intermediate | 10–14 | [LangGraph](https://github.com/langchain-ai/langgraph) | | 7 | Multi-Agent Workflow (Reviewer Loops) | Agents | Advanced | 12–18 | [AutoGen](https://github.com/microsoft/autogen) | | 8 | Observability Dash (Traces, Costs) | Observability | Intermediate | 6–10 | [Langfuse](https://github.com/langfuse/langfuse) | | 9 | Prompt Registry + Versioning | MLOps | Intermediate | 6–10 | [promptfoo](https://github.com/promptfoo/promptfoo) | | 10 | Model Serving with vLLM | Serving | Intermediate | 8–12 | [vLLM](https://github.com/vllm-project/vllm) | | 11 | Local Inference Prototype | Edge | Beginner | 4–8 | [llama.cpp](https://github.com/ggerganov/llama.cpp) | | 12 | Orchestration with Temporal | Orchestration | Intermediate | 8–12 | [Temporal TS](https://github.com/temporalio/sdk-typescript) | | 13 | Vector DB Benchmarks (Qdrant/Weaviate/Milvus) | Retrieval | Advanced |"
  },
  {
    "id": "05-projects\\agentic-saas-planner.md",
    "title": "Project: Agentic SaaS Planner",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/agentic-saas-planner.md",
    "section": "Projects",
    "path": "05-projects\\agentic-saas-planner.md",
    "headings": [
      {
        "level": 1,
        "text": "Project: Agentic SaaS Planner"
      },
      {
        "level": 2,
        "text": "Steps"
      },
      {
        "level": 2,
        "text": "Acceptance Criteria"
      },
      {
        "level": 2,
        "text": "Stretch"
      }
    ],
    "excerpt": "# Project: Agentic SaaS Planner Goal: Use a Planner → Worker → Reviewer swarm to produce a product plan, architecture, and build plan for a simple SaaS idea. ## Steps 1) Run the example 2) Add a agent to block release if risks are high 3) Extend output: Bill of Materials, API list, day-by-day build plan 4) Optional: St",
    "content": "# Project: Agentic SaaS Planner Goal: Use a Planner → Worker → Reviewer swarm to produce a product plan, architecture, and build plan for a simple SaaS idea. ## Steps 1) Run the example 2) Add a agent to block release if risks are high 3) Extend output: Bill of Materials, API list, day-by-day build plan 4) Optional: Streamlit UI button to export a PDF brief ## Acceptance Criteria - Outputs include target user, jobs-to-be-done, key features, risks, MVP outline - Technical plan includes components, data flow, APIs, eval plan - Reviewer summary is clear and actionable ## Stretch - Integrate LiteLLM to compare models (OpenAI vs Anthropic) - Add a test harness that verifies the plan includes eval metrics"
  },
  {
    "id": "05-projects\\creator-evals\\README.md",
    "title": "Creator Evaluation Harness",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/creator-evals/README.md",
    "section": "Projects",
    "path": "05-projects\\creator-evals\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Evaluation Harness"
      },
      {
        "level": 2,
        "text": "What's Inside"
      },
      {
        "level": 2,
        "text": "Quick Start"
      },
      {
        "level": 1,
        "text": "Reuse the shared automation tooling"
      },
      {
        "level": 2,
        "text": "Extend"
      },
      {
        "level": 2,
        "text": "Suggested Workflow"
      },
      {
        "level": 2,
        "text": "Related Resources"
      }
    ],
    "excerpt": "# Creator Evaluation Harness Opinionated scaffold for scoring AI-assisted creator workflows. It extends with creator-specific scenarios, datasets, and reporting conventions. ## What's Inside - � Promptfoo-ready creator prompts covering hooks, performance summaries, and next-step experiments. - � Sample analytics export",
    "content": "# Creator Evaluation Harness Opinionated scaffold for scoring AI-assisted creator workflows. It extends with creator-specific scenarios, datasets, and reporting conventions. ## What's Inside - � Promptfoo-ready creator prompts covering hooks, performance summaries, and next-step experiments. - � Sample analytics export used during evaluations. - � Drop evaluation outputs and retro notes here (gitignored by default). ## Quick Start Reports land in within the eval-automation project. Copy the relevant JSON/HTML into if you want creator-specific archives. ## Extend - Append new creator scenarios (e.g., newsletter variants, podcast scripts) to . - Mirror analytics exports from your BI stack into for reproducible tests. - Sync metrics with [ ](../../02-learning-paths/micro-modules/creator-analytics-feedback-loop.md) as you build longer feedback loops. ## Suggested Workflow 1. Run micro-modules: orchestration, scorecards, analytics loop. 2. Update Promptfoo configs to include creator guardrails (tone, compliance, CTA strength). 3. Execute evaluations pre- and post-campaign; log results in . 4. Share wins in the [Creator Studio Launch Guide](../../09-articles/creator-studio-launch-guide.md). ## Related Resources - [ ](../../01-design-patterns/creator-studio-automation.md) - [ ](../eval-automation/README.md) - [ ](../../02-learning-paths/micro-modules/creator-evaluation-scorecards.md)"
  },
  {
    "id": "05-projects\\domain-rag-healthcare\\README.md",
    "title": "Domain RAG Healthcare Blueprint",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/domain-rag-healthcare/README.md",
    "section": "Projects",
    "path": "05-projects\\domain-rag-healthcare\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Domain RAG Healthcare Blueprint"
      },
      {
        "level": 2,
        "text": "What's Inside"
      },
      {
        "level": 2,
        "text": "Quick Start"
      },
      {
        "level": 2,
        "text": "Configure"
      },
      {
        "level": 2,
        "text": "Evaluate & Share"
      },
      {
        "level": 2,
        "text": "Extend"
      }
    ],
    "excerpt": "# Domain RAG Healthcare Blueprint Opinionated scaffolding for building a healthcare-ready retrieval augmented generation (RAG) service tailored to AI Architect Health Alliance (AIHA) clinical teams. ## What's Inside - � AIHA hypertension, diabetes, telehealth, kidney, and medication safety playbooks. - � end-to-end ing",
    "content": "# Domain RAG Healthcare Blueprint Opinionated scaffolding for building a healthcare-ready retrieval augmented generation (RAG) service tailored to AI Architect Health Alliance (AIHA) clinical teams. ## What's Inside - � AIHA hypertension, diabetes, telehealth, kidney, and medication safety playbooks. - � end-to-end ingestion, embedding, and retrieval workflows with environment-tunable models. - � UMLS-aware chunker with sentence fallback for lightweight deployments. - � PHI redaction, contraindication alerts, and escalation triggers. - � CLI wrapper to score responses against and push metrics to Langfuse. - � drop evaluation outputs, risk reviews, and decision logs for audit. ## Quick Start ## Configure Create from and supply: Override or if you point ingest at alternate datasets. ## Evaluate & Share - Run after ingesting new knowledge. - Store summaries in (e.g., ). - Feed insights back into the [Domain RAG Clinic (Healthcare)](../../02-learning-paths/micro-modules/retrieval-domain-rag-healthcare.md) micro-module. ## Extend - Drop additional CSVs in (e.g., policy memos, care coordination scripts) and set to the combined export. - Enrich with terminology services (SNOMED, LOINC) for enterprise-grade retrieval. - Pair with to capture post-release retrospectives and risk mitigations."
  },
  {
    "id": "05-projects\\eval-automation\\README.md",
    "title": "Evaluation Automation",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/eval-automation/README.md",
    "section": "Projects",
    "path": "05-projects\\eval-automation\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Automation"
      },
      {
        "level": 2,
        "text": "What's Inside"
      },
      {
        "level": 2,
        "text": "Quick Start"
      },
      {
        "level": 2,
        "text": "Live Eval Checklist"
      },
      {
        "level": 2,
        "text": "Wire It Into CI"
      },
      {
        "level": 2,
        "text": "Reports & Storytelling"
      },
      {
        "level": 2,
        "text": "Creator Ops Integration"
      }
    ],
    "excerpt": "# Evaluation Automation Scaffolding to run automated evaluation suites via Promptfoo and Langfuse with AIHA-specific scenarios. ## What's Inside - - accuracy, toxicity, and custom citation guardrails. - - curated prompts covering Langfuse telemetry, evaluation automation, and AIHA care protocols. - - orchestrates Promp",
    "content": "# Evaluation Automation Scaffolding to run automated evaluation suites via Promptfoo and Langfuse with AIHA-specific scenarios. ## What's Inside - - accuracy, toxicity, and custom citation guardrails. - - curated prompts covering Langfuse telemetry, evaluation automation, and AIHA care protocols. - - orchestrates Promptfoo runs and writes timestamped JSON/HTML reports. - - quickcheck for provider API keys before running live evaluations. - - GitHub Action template to block merges when guardrails regress. - - drop-off point for generated artefacts and exec-ready summaries. - - template for provider keys and optional Promptfoo overrides. ## Quick Start If your Promptfoo binary lives outside of , set in to the absolute path. ## Live Eval Checklist 1. Populate with production-ready and values. 2. Run ; address any missing variables before continuing. 3. (Optional) Override if you manage multiple Promptfoo installs. 4. Execute to generate live-model reports. When provider keys are absent, the script falls back to ungraded summaries tagged as . ## Wire It Into CI 1. Copy into . 2. Store / (and any LLM provider keys) in repository secrets. 3. Tune thresholds or providers inside to reflect your policies. ## Reports & Storytelling Generated HTML/JSON reports land in . Attach highlights to the [Executive Narrative Builder](../../02-learning-paths/micro-modules/storytelling-exec-brief.md) or share in leadership forums. Offline runs remain in the same directory but clearly indicate the fallback status. Complete the [Evaluation Automation Pipeline](../../02-learning-paths/micro-modules/evaluation-automation-pipeline.md) micro-module to unlock advanced workflows. ## Creator Ops Integration - Run creator-specific test sets from [ ](../creator-evals/README.md) with . - Schedule micro-modules for your ops team: [Creator Content Orchestration Sprint](../../02-learning-paths/micro-modules/creator-content-orchestration.md), [Creator Evaluation Scorecards](../../02-learning-paths/micro-"
  },
  {
    "id": "05-projects\\evals-langfuse.md",
    "title": "Evals Langfuse",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/evals-langfuse.md",
    "section": "Projects",
    "path": "05-projects\\evals-langfuse.md",
    "headings": [],
    "excerpt": "# Project: Evals with Langfuse - Instrument API endpoints with traces, latency, and cost spans - Create an evaluation dataset (Q/A with citations) - Implement scoring for faithfulness and helpfulness - Track versions and regressions with CI that fails on quality drops",
    "content": "# Project: Evals with Langfuse - Instrument API endpoints with traces, latency, and cost spans - Create an evaluation dataset (Q/A with citations) - Implement scoring for faithfulness and helpfulness - Track versions and regressions with CI that fails on quality drops"
  },
  {
    "id": "05-projects\\rag-on-supabase.md",
    "title": "Rag On Supabase",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/rag-on-supabase.md",
    "section": "Projects",
    "path": "05-projects\\rag-on-supabase.md",
    "headings": [],
    "excerpt": "# Project: RAG on Supabase + OpenAI (Step-by-Step) 1. **Setup** - Create a Supabase project (EU recommended) and enable - Create tables: , - Secure OpenAI and YouTube API keys 2. **Ingest** - Fetch video transcripts, clean them, and segment into 200-400 token chunks at semantic boundaries - Compute embeddings (small mo",
    "content": "# Project: RAG on Supabase + OpenAI (Step-by-Step) 1. **Setup** - Create a Supabase project (EU recommended) and enable - Create tables: , - Secure OpenAI and YouTube API keys 2. **Ingest** - Fetch video transcripts, clean them, and segment into 200-400 token chunks at semantic boundaries - Compute embeddings (small model) and store vectors in 3. **Retrieval API** - Combine vector similarity with keyword search and re-ranking - Return chunks with timestamps and resource metadata 4. **Tutor Endpoint** - Compose a system prompt with rules (require citations, abstain on low confidence) - Retrieve top-k chunks, call OpenAI, and return answers with citations 5. **Evals and Observability** - Add Langfuse traces and build a small evaluation dataset (questions plus expected sources) - Track faithfulness and citation coverage 6. **Hardening** - Apply rate limits per user or tier and cache frequent queries - Add cost guardrails and alerts"
  },
  {
    "id": "05-projects\\vector-search-pgvector.md",
    "title": "Vector Search Pgvector",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/05-projects/vector-search-pgvector.md",
    "section": "Projects",
    "path": "05-projects\\vector-search-pgvector.md",
    "headings": [],
    "excerpt": "# Project: Vector Search with pgvector - Install the extension and prepare schema - Choose embedding model; test cosine versus inner product - Tune indexing (IVFFlat) and storage parameters - Benchmark recall and latency trade-offs, including hybrid search",
    "content": "# Project: Vector Search with pgvector - Install the extension and prepare schema - Choose embedding model; test cosine versus inner product - Tune indexing (IVFFlat) and storage parameters - Benchmark recall and latency trade-offs, including hybrid search"
  },
  {
    "id": "06-toolchains\\agentic-swarms-stack.md",
    "title": "Reference Stack: Agentic Code Swarms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/06-toolchains/agentic-swarms-stack.md",
    "section": "Toolchains",
    "path": "06-toolchains\\agentic-swarms-stack.md",
    "headings": [
      {
        "level": 1,
        "text": "Reference Stack: Agentic Code Swarms"
      },
      {
        "level": 2,
        "text": "Runtime"
      },
      {
        "level": 2,
        "text": "Providers"
      },
      {
        "level": 2,
        "text": "Data & Memory"
      },
      {
        "level": 2,
        "text": "Observability"
      },
      {
        "level": 2,
        "text": "Evals & Guardrails"
      },
      {
        "level": 2,
        "text": "Deployment"
      },
      {
        "level": 2,
        "text": "Security & Costs"
      }
    ],
    "excerpt": "# Reference Stack: Agentic Code Swarms ## Runtime - Python 3.11, , , - Orchestrations: sequential, P–W–R, round‑robin, map‑reduce - UI: Streamlit Explorer for learning and demos ## Providers - LiteLLM routing (OpenAI, Anthropic, etc.) - Offline mock provider for classrooms ## Data & Memory - Vector DB: pgvector or Qdra",
    "content": "# Reference Stack: Agentic Code Swarms ## Runtime - Python 3.11, , , - Orchestrations: sequential, P–W–R, round‑robin, map‑reduce - UI: Streamlit Explorer for learning and demos ## Providers - LiteLLM routing (OpenAI, Anthropic, etc.) - Offline mock provider for classrooms ## Data & Memory - Vector DB: pgvector or Qdrant (optional for advanced tools) - Cache: Redis for short‑term memory (optional) ## Observability - Tracing: OpenTelemetry export from agents (future) - Logs: structured JSON logs per agent step ## Evals & Guardrails - Metrics: task success, latency, cost - Harness: deterministic prompts + expected checks ## Deployment - Dockerfile provided; run Streamlit on 8501 - Suggested: Fly.io/Render for demos; Kubernetes for internal portals ## Security & Costs - Secrets via or platform secret store - Budget limits per run; circuit breakers on failure counts"
  },
  {
    "id": "06-toolchains\\stack-reference.md",
    "title": "Stack Reference",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/06-toolchains/stack-reference.md",
    "section": "Toolchains",
    "path": "06-toolchains\\stack-reference.md",
    "headings": [
      {
        "level": 1,
        "text": "Stack Reference"
      }
    ],
    "excerpt": "# Stack Reference - Frontend: Next.js, Tailwind - API: Node/Express - DB: Supabase (Postgres + pgvector) - AI: OpenAI (with OSS fallback) - Obs: Sentry, Langfuse",
    "content": "# Stack Reference - Frontend: Next.js, Tailwind - API: Node/Express - DB: Supabase (Postgres + pgvector) - AI: OpenAI (with OSS fallback) - Obs: Sentry, Langfuse"
  },
  {
    "id": "06-toolchains\\vercel-ai-sdk.md",
    "title": "Vercel AI SDK Integration Playbook",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/06-toolchains/vercel-ai-sdk.md",
    "section": "Toolchains",
    "path": "06-toolchains\\vercel-ai-sdk.md",
    "headings": [
      {
        "level": 1,
        "text": "Vercel AI SDK Integration Playbook"
      },
      {
        "level": 2,
        "text": "Why Use the SDK"
      },
      {
        "level": 2,
        "text": "Quick Start (Dashboard)"
      },
      {
        "level": 2,
        "text": "Pair With Repo Patterns"
      },
      {
        "level": 2,
        "text": "OpenRouter & Local Models"
      },
      {
        "level": 2,
        "text": "Observability Hooks"
      },
      {
        "level": 2,
        "text": "Deployment Checklist"
      },
      {
        "level": 2,
        "text": "Next Steps"
      }
    ],
    "excerpt": "# Vercel AI SDK Integration Playbook Leverage the Vercel AI SDK to power composable chat, RAG, and agent experiences across the AI Architect Academy ecosystem. This guide shows how to connect the SDK to the dashboard, reuse repo patterns, and stay compliant with governance expectations. ## Why Use the SDK - **Streaming",
    "content": "# Vercel AI SDK Integration Playbook Leverage the Vercel AI SDK to power composable chat, RAG, and agent experiences across the AI Architect Academy ecosystem. This guide shows how to connect the SDK to the dashboard, reuse repo patterns, and stay compliant with governance expectations. ## Why Use the SDK - **Streaming UX out of the box:** Token streaming, tool call events, and optimistic UI updates with minimal boilerplate. - **Provider flexibility:** Swap OpenAI, Anthropic, Cohere, Groq, or OpenRouter adapters without refactoring business logic. - **Edge-ready:** First-class support for Vercel Edge Functions and Next.js App Router (used in ). - **Trace hooks:** Easily attach Langfuse or custom telemetry, keeping eval and observability loops tight. ## Quick Start (Dashboard) 1. Install dependencies (already present in ). 2. Set provider keys in : 3. Use the SDK inside edge routes, e.g. : 4. Surface responses in the React client using the hook from for full streaming UX. ## Pair With Repo Patterns - **RAG Projects:** Use the SDK’s support to call retrieval endpoints built from . - **Agentic Swarms:** Map tool calls to orchestrations described in and log hand-offs via . - **Governance:** Pipe model + prompt metadata into and for traceability. ## OpenRouter & Local Models - Add an adapter (e.g. ) and set in . - For local models (Open WebUI, Big-AGI), point the SDK’s fetcher to your local endpoint: - Document the setup in [ ](../dashboard/AGENT.md) so teammates can replicate your environment. ## Observability Hooks - Wrap with Langfuse logging (see ) or add custom spans to track latency and cost. - Use the dashboards in Langfuse to validate evals from . ## Deployment Checklist - Secrets stored via Vercel environment variables or your secrets manager. - Feature flags ( , ) toggled per environment in . - Regression suite staged via (add tests as your flows mature). ## Next Steps - Extend the SDK usage to power the dashboard Playground and Builder modules. - Pair with to "
  },
  {
    "id": "07-evaluation\\eval-harness.md",
    "title": "Eval Harness",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/07-evaluation/eval-harness.md",
    "section": "Evaluation",
    "path": "07-evaluation\\eval-harness.md",
    "headings": [
      {
        "level": 1,
        "text": "Eval Harness"
      }
    ],
    "excerpt": "# Eval Harness - Dataset format - Scoring functions - CI integration",
    "content": "# Eval Harness - Dataset format - Scoring functions - CI integration"
  },
  {
    "id": "07-evaluation\\metrics.md",
    "title": "Evaluation Metrics",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/07-evaluation/metrics.md",
    "section": "Evaluation",
    "path": "07-evaluation\\metrics.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation Metrics"
      }
    ],
    "excerpt": "# Evaluation Metrics - Faithfulness, groundedness, factuality - Relevance, recall@k, MRR - UX metrics: satisfaction, time-to-answer",
    "content": "# Evaluation Metrics - Faithfulness, groundedness, factuality - Relevance, recall@k, MRR - UX metrics: satisfaction, time-to-answer"
  },
  {
    "id": "08-governance\\model-risk.md",
    "title": "Model Risk Management",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/model-risk.md",
    "section": "Governance",
    "path": "08-governance\\model-risk.md",
    "headings": [
      {
        "level": 1,
        "text": "Model Risk Management"
      }
    ],
    "excerpt": "# Model Risk Management - Risk taxonomy - Controls & monitoring - Review cadence",
    "content": "# Model Risk Management - Risk taxonomy - Controls & monitoring - Review cadence"
  },
  {
    "id": "08-governance\\privacy-gdpr.md",
    "title": "Privacy & GDPR",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/08-governance/privacy-gdpr.md",
    "section": "Governance",
    "path": "08-governance\\privacy-gdpr.md",
    "headings": [
      {
        "level": 1,
        "text": "Privacy & GDPR"
      }
    ],
    "excerpt": "# Privacy & GDPR - Data residency - Consent & retention - Data subject rights",
    "content": "# Privacy & GDPR - Data residency - Consent & retention - Data subject rights"
  },
  {
    "id": "09-articles\\README.md",
    "title": "Articles",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/09-articles/README.md",
    "section": "Articles",
    "path": "09-articles\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Articles"
      }
    ],
    "excerpt": "# Articles - RAG in Production: Chunking, Hybrid Search, Evals - Grounded Tutoring: Enforcing Citations - Design Patterns for Enterprise AI - Agentic Code Swarms: Orchestration patterns, UX for agents, and pedagogy - Creator Studio Launch Guide: Automating briefs-to-publish workflows with eval loops",
    "content": "# Articles - RAG in Production: Chunking, Hybrid Search, Evals - Grounded Tutoring: Enforcing Citations - Design Patterns for Enterprise AI - Agentic Code Swarms: Orchestration patterns, UX for agents, and pedagogy - Creator Studio Launch Guide: Automating briefs-to-publish workflows with eval loops"
  },
  {
    "id": "09-articles\\creator-studio-launch-guide.md",
    "title": "Creator Studio Launch Guide",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/09-articles/creator-studio-launch-guide.md",
    "section": "Articles",
    "path": "09-articles\\creator-studio-launch-guide.md",
    "headings": [
      {
        "level": 1,
        "text": "Creator Studio Launch Guide"
      },
      {
        "level": 2,
        "text": "1. Anchor on Outcomes"
      },
      {
        "level": 2,
        "text": "2. Structure the Production Pipeline"
      },
      {
        "level": 2,
        "text": "3. Assign Roles & Rituals"
      },
      {
        "level": 2,
        "text": "4. Build the Tech Stack"
      },
      {
        "level": 2,
        "text": "5. Implement the Feedback Loop"
      },
      {
        "level": 2,
        "text": "6. Operational Excellence"
      },
      {
        "level": 2,
        "text": "7. Launch Checklist"
      },
      {
        "level": 2,
        "text": "8. Next Steps"
      }
    ],
    "excerpt": "# Creator Studio Launch Guide Launch a high-velocity creator studio that blends human taste with AI copilots. This guide distils best practices from the new Creator Studio Automation pattern, micro-learning modules, and evaluation workflows so you can move from concept to multi-channel distribution in days, not months.",
    "content": "# Creator Studio Launch Guide Launch a high-velocity creator studio that blends human taste with AI copilots. This guide distils best practices from the new Creator Studio Automation pattern, micro-learning modules, and evaluation workflows so you can move from concept to multi-channel distribution in days, not months. ## 1. Anchor on Outcomes - **North-star metrics:** retention, audience growth, revenue per campaign, turnaround time. - **Guardrails:** brand voice, compliance, accessibility, and regional requirements. - **Success definition:** a single campaign should demonstrate faster production, higher engagement, or better reuse of assets. ## 2. Structure the Production Pipeline 1. **Brief intake:** capture persona, offer, CTA, channel mix, and success metrics. Use forms with schema validation. 2. **Knowledge graph:** store brand voice snippets, value props, FAQs, and evergreen offers in a vector store. 3. **Generation tiers:** split prompts for outlines, long-form scripts, short-form hooks, thumbnails, and distribution copy. 4. **Guardrail service:** enforce tone/style rules, run compliance linting, and attach citations for claims. 5. **Review lanes:** surface diffs, highlight AI-generated sections, and collect reviewer notes in one dashboard. 6. **Publish & measure:** push to CMS, social schedulers, newsletters, and update analytics warehouse for tracking. ## 3. Assign Roles & Rituals | Persona | Responsibility | AI Assistants | | --- | --- | --- | | Creative Director | Approves campaign brief, calibrates tone, manages backlog | Outline generator, persona reminder bot | | Editor / Producer | Refines scripts, ensures compliance, orchestrates publishing | Fact-check agent, compliance lint tool | | Analyst | Monitors performance, designs experiments, communicates insights | Analytics notebook agent, cohort analysis prompts | | Marketing Ops | Maintains workflow automation, env secrets, CI checks | Promptfoo regression suite, Langfuse dashboards | ## 4. Build the "
  },
  {
    "id": "09-articles\\drafts\\README.md",
    "title": "Draft Articles",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/09-articles/drafts/README.md",
    "section": "Articles",
    "path": "09-articles\\drafts\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Draft Articles"
      }
    ],
    "excerpt": "# Draft Articles Drop in-progress narratives here. Use the prompts from and the templates in to accelerate publishing.",
    "content": "# Draft Articles Drop in-progress narratives here. Use the prompts from and the templates in to accelerate publishing."
  },
  {
    "id": "10-resources\\channels.md",
    "title": "Channels",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/channels.md",
    "section": "Resources",
    "path": "10-resources\\channels.md",
    "headings": [],
    "excerpt": "# YouTube Channels (Curated) - Andrej Karpathy - https://www.youtube.com/@AndrejKarpathy - Sebastian Raschka - https://www.youtube.com/@SebastianRaschka - Hugging Face - https://www.youtube.com/@HuggingFace - LangChain - https://www.youtube.com/@LangChain - OpenAI - https://www.youtube.com/@OpenAI - MLOps Community - h",
    "content": "# YouTube Channels (Curated) - Andrej Karpathy - https://www.youtube.com/@AndrejKarpathy - Sebastian Raschka - https://www.youtube.com/@SebastianRaschka - Hugging Face - https://www.youtube.com/@HuggingFace - LangChain - https://www.youtube.com/@LangChain - OpenAI - https://www.youtube.com/@OpenAI - MLOps Community - https://www.youtube.com/@MLOps - Two Minute Papers (curate for depth) - https://www.youtube.com/@TwoMinutePapers"
  },
  {
    "id": "10-resources\\papers.md",
    "title": "Papers & Reading",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/papers.md",
    "section": "Resources",
    "path": "10-resources\\papers.md",
    "headings": [
      {
        "level": 1,
        "text": "Papers & Reading"
      }
    ],
    "excerpt": "# Papers & Reading - Attention Is All You Need - Rethinking RAG architectures (surveys) - Hallucination evaluation methods",
    "content": "# Papers & Reading - Attention Is All You Need - Rethinking RAG architectures (surveys) - Hallucination evaluation methods"
  },
  {
    "id": "10-resources\\platforms.md",
    "title": "Platforms & Tools",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/platforms.md",
    "section": "Resources",
    "path": "10-resources\\platforms.md",
    "headings": [
      {
        "level": 1,
        "text": "Platforms & Tools"
      }
    ],
    "excerpt": "# Platforms & Tools - Supabase — https://supabase.com/ - Vercel — https://vercel.com/ - Railway — https://railway.app/ - Fly.io — https://fly.io/ - Cloudflare — https://www.cloudflare.com/ - Stripe — https://stripe.com/ - Sentry — https://sentry.io/ - Langfuse — https://langfuse.com/",
    "content": "# Platforms & Tools - Supabase — https://supabase.com/ - Vercel — https://vercel.com/ - Railway — https://railway.app/ - Fly.io — https://fly.io/ - Cloudflare — https://www.cloudflare.com/ - Stripe — https://stripe.com/ - Sentry — https://sentry.io/ - Langfuse — https://langfuse.com/"
  },
  {
    "id": "10-resources\\playlists.md",
    "title": "Playlists (First Pull Targets)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/playlists.md",
    "section": "Resources",
    "path": "10-resources\\playlists.md",
    "headings": [
      {
        "level": 1,
        "text": "Playlists (First Pull Targets)"
      }
    ],
    "excerpt": "# Playlists (First Pull Targets) - Karpathy: Neural Networks, LLMs — https://www.youtube.com/@AndrejKarpathy/playlists - Hugging Face: Transformers — https://www.youtube.com/@HuggingFace/playlists - LangChain: RAG & Agents — https://www.youtube.com/@LangChain/playlists - OpenAI Dev — https://www.youtube.com/@OpenAI/pla",
    "content": "# Playlists (First Pull Targets) - Karpathy: Neural Networks, LLMs — https://www.youtube.com/@AndrejKarpathy/playlists - Hugging Face: Transformers — https://www.youtube.com/@HuggingFace/playlists - LangChain: RAG & Agents — https://www.youtube.com/@LangChain/playlists - OpenAI Dev — https://www.youtube.com/@OpenAI/playlists - MLOps Community Sessions — https://www.youtube.com/@MLOps/playlists"
  },
  {
    "id": "10-resources\\repos.md",
    "title": "Repos",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/repos.md",
    "section": "Resources",
    "path": "10-resources\\repos.md",
    "headings": [
      {
        "level": 2,
        "text": "Meta Aggregators"
      }
    ],
    "excerpt": "# Repositories to Know (With Links) | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Models and tokenizers: foundation for NLP and LLM projects | | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | RAG and tool orchestration; fast from p",
    "content": "# Repositories to Know (With Links) | Link | Why it matters | |---|---| | [huggingface/transformers](https://github.com/huggingface/transformers) | Models and tokenizers: foundation for NLP and LLM projects | | [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | RAG and tool orchestration; fast from prototype to production | | [run-llama/llama_index](https://github.com/run-llama/llama_index) | Data framework for LLM applications | | [vllm-project/vllm](https://github.com/vllm-project/vllm) | High-throughput, low-latency serving | | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | Local inference on CPU and edge devices | | [ollama/ollama](https://github.com/ollama/ollama) | Local model runner and packaging | | [pgvector/pgvector](https://github.com/pgvector/pgvector) | Vector similarity inside Postgres | | [qdrant/qdrant](https://github.com/qdrant/qdrant) | Production-ready open source vector database | | [weaviate/weaviate](https://github.com/weaviate/weaviate) | Vector database with hybrid search | | [milvus-io/milvus](https://github.com/milvus-io/milvus) | Distributed vector database | | [langfuse/langfuse](https://github.com/langfuse/langfuse) | Observability for LLM applications | | [Arize-ai/phoenix](https://github.com/Arize-ai/phoenix) | Observability and analysis toolkit | | [promptfoo/promptfoo](https://github.com/promptfoo/promptfoo) | Prompt evaluations in CI | | [microsoft/autogen](https://github.com/microsoft/autogen) | Multi-agent framework | | [joaomdmoura/crewai](https://github.com/joaomdmoura/crewai) | Agent teams and workflows | ## Meta Aggregators | Link | Why it matters | |---|---| | [sindresorhus/awesome](https://github.com/sindresorhus/awesome) | Canonical index of awesome lists | | [awesome-machine-learning/awesome-machine-learning](https://github.com/awesome-machine-learning/awesome-machine-learning) | Broad machine learning resources | | [visenger/awesome-mlops](https://github.com/visenger/awesome-mlops) | Cu"
  },
  {
    "id": "10-resources\\videos.md",
    "title": "Videos (Selected)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/10-resources/videos.md",
    "section": "Resources",
    "path": "10-resources\\videos.md",
    "headings": [
      {
        "level": 1,
        "text": "Videos (Selected)"
      }
    ],
    "excerpt": "# Videos (Selected) - Karpathy: Intro to LLMs — https://www.youtube.com/watch?v=zjkBMFhNj_g - Raschka: LLM Evaluation — https://www.youtube.com/watch?v=Rr8a8Y3G9Vw - LangChain: RAG with pgvector — https://www.youtube.com/watch?v=b8bXqSxTq0E - Hugging Face: Transformers Intro — https://www.youtube.com/watch?v=G5RY_SUJih",
    "content": "# Videos (Selected) - Karpathy: Intro to LLMs — https://www.youtube.com/watch?v=zjkBMFhNj_g - Raschka: LLM Evaluation — https://www.youtube.com/watch?v=Rr8a8Y3G9Vw - LangChain: RAG with pgvector — https://www.youtube.com/watch?v=b8bXqSxTq0E - Hugging Face: Transformers Intro — https://www.youtube.com/watch?v=G5RY_SUJih4 - OpenAI: Dev Day Highlights — https://www.youtube.com/watch?v=U9mJuUkhUzk"
  },
  {
    "id": "11-hyperscalers\\README.md",
    "title": "Hyperscalers (OCI • AWS • GCP • Azure)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Hyperscalers (OCI • AWS • GCP • Azure)"
      }
    ],
    "excerpt": "# Hyperscalers (OCI • AWS • GCP • Azure) Decision-focused guides to build GenAI/RAG on each cloud with links to official docs, vector options, and quick starts. - Oracle Cloud Infrastructure (OCI): - Amazon Web Services (AWS): - Google Cloud (GCP): - Microsoft Azure: See also: [Choose a Platform](choose-platform.md)",
    "content": "# Hyperscalers (OCI • AWS • GCP • Azure) Decision-focused guides to build GenAI/RAG on each cloud with links to official docs, vector options, and quick starts. - Oracle Cloud Infrastructure (OCI): - Amazon Web Services (AWS): - Google Cloud (GCP): - Microsoft Azure: See also: [Choose a Platform](choose-platform.md)"
  },
  {
    "id": "11-hyperscalers\\aws\\README.md",
    "title": "AWS for AI Architects",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/aws/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\aws\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "AWS for AI Architects"
      }
    ],
    "excerpt": "# AWS for AI Architects Key Services - Amazon Bedrock: managed GenAI (model providers) - OpenSearch Serverless: vector search; filters - Aurora/ RDS Postgres: pgvector support - Lambda, API Gateway, ECS/EKS for serving - Observability: CloudWatch + 3rd‑party Vector Options - OpenSearch kNN / Faiss under the hood - Auro",
    "content": "# AWS for AI Architects Key Services - Amazon Bedrock: managed GenAI (model providers) - OpenSearch Serverless: vector search; filters - Aurora/ RDS Postgres: pgvector support - Lambda, API Gateway, ECS/EKS for serving - Observability: CloudWatch + 3rd‑party Vector Options - OpenSearch kNN / Faiss under the hood - Aurora Postgres + pgvector Quick Start (RAG) 1) Ingest → embeddings → OpenSearch index or pgvector 2) Retrieval API: vector + BM25 hybrid; re‑rank if needed 3) Tutor API: Bedrock (or OpenAI) with citations 4) Logs & metrics; costs via CloudWatch + tags Docs & Links - Amazon Bedrock — https://aws.amazon.com/bedrock/ - OpenSearch Vector Search — https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html - Aurora pgvector — https://aws.amazon.com/blogs/database/amazon-aurora-postgresql-adhere-pgvector/"
  },
  {
    "id": "11-hyperscalers\\azure\\README.md",
    "title": "Azure for AI Architects",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/azure/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\azure\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Azure for AI Architects"
      }
    ],
    "excerpt": "# Azure for AI Architects Key Services - Azure OpenAI Service: GPT models with enterprise controls - Azure AI Search (Cognitive Search) with vectors - Cosmos DB vector, Azure Postgres + pgvector - Azure Functions / AKS for serving - Observability: App Insights + 3rd‑party Vector Options - Azure AI Search (vector) - Cos",
    "content": "# Azure for AI Architects Key Services - Azure OpenAI Service: GPT models with enterprise controls - Azure AI Search (Cognitive Search) with vectors - Cosmos DB vector, Azure Postgres + pgvector - Azure Functions / AKS for serving - Observability: App Insights + 3rd‑party Vector Options - Azure AI Search (vector) - Cosmos DB vector - Azure Postgres + pgvector Quick Start (RAG) 1) Choose vector: AI Search or pgvector 2) Retrieval API: vector + filters; optional rerank 3) Tutor API: Azure OpenAI with citations + safety 4) Monitor with App Insights; track costs Docs & Links - Azure OpenAI — https://learn.microsoft.com/azure/ai-services/openai/ - Azure AI Search Vector — https://learn.microsoft.com/azure/search/vector-search-overview - Cosmos DB Vector — https://learn.microsoft.com/azure/cosmos-db/vector-database"
  },
  {
    "id": "11-hyperscalers\\choose-platform.md",
    "title": "Choose a Platform (Decision Guide)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/choose-platform.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\choose-platform.md",
    "headings": [
      {
        "level": 1,
        "text": "Choose a Platform (Decision Guide)"
      }
    ],
    "excerpt": "# Choose a Platform (Decision Guide) Criteria - Data residency & compliance (EU, HIPAA, ISO) - Managed vector search options and performance - GenAI service availability (models, safety) - Integration with data sources and identity - Cost transparency and predictable budgets - Lock‑in vs. portability (pgvector, open SD",
    "content": "# Choose a Platform (Decision Guide) Criteria - Data residency & compliance (EU, HIPAA, ISO) - Managed vector search options and performance - GenAI service availability (models, safety) - Integration with data sources and identity - Cost transparency and predictable budgets - Lock‑in vs. portability (pgvector, open SDKs) Quick Take - If you already use a cloud: pick native services (pgvector/managed vector + GenAI) and keep portability via open embeddings and RAG patterns. - If you need maximum portability and EU control: Postgres + pgvector + OpenAI/Anthropic with OSS fallbacks. Baseline Stack (All Clouds) - Postgres + pgvector or managed vector DB - Retrieval API (hybrid search), Tutor API (RAG with citations) - Observability (Langfuse/Phoenix), Evals (promptfoo/ragas) - AuthZ, rate limits, and cost guardrails"
  },
  {
    "id": "11-hyperscalers\\gcp\\README.md",
    "title": "Google Cloud for AI Architects",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/gcp/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\gcp\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Google Cloud for AI Architects"
      }
    ],
    "excerpt": "# Google Cloud for AI Architects Key Services - Vertex AI (GenAI Studio, endpoints) - AlloyDB AI: integrated vector embeddings; Cloud SQL pgvector - Cloud Run / GKE for serving - Observability: Cloud Logging/Trace + 3rd‑party Vector Options - AlloyDB AI vector; Cloud SQL pgvector - OpenSearch-compatible via partners Qu",
    "content": "# Google Cloud for AI Architects Key Services - Vertex AI (GenAI Studio, endpoints) - AlloyDB AI: integrated vector embeddings; Cloud SQL pgvector - Cloud Run / GKE for serving - Observability: Cloud Logging/Trace + 3rd‑party Vector Options - AlloyDB AI vector; Cloud SQL pgvector - OpenSearch-compatible via partners Quick Start (RAG) 1) Store embeddings in AlloyDB AI or Cloud SQL pgvector 2) Retrieval API: vector + keyword hybrid 3) Tutor API: Vertex AI models (or OpenAI) with citations 4) Traces/evals; pub/sub for ingestion Docs & Links - Vertex AI — https://cloud.google.com/vertex-ai - AlloyDB AI — https://cloud.google.com/alloydb/docs/ai - Cloud SQL pgvector — https://cloud.google.com/sql/docs/postgres/extensions/pgvector"
  },
  {
    "id": "11-hyperscalers\\oci\\README.md",
    "title": "OCI for AI Architects (Oracle Cloud Infrastructure)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/11-hyperscalers/oci/README.md",
    "section": "Hyperscalers",
    "path": "11-hyperscalers\\oci\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "OCI for AI Architects (Oracle Cloud Infrastructure)"
      }
    ],
    "excerpt": "# OCI for AI Architects (Oracle Cloud Infrastructure) Key Services - GenAI Service: managed access to foundation models - Oracle Database 23ai: AI Vector Search + JSON Relational Duality - Autonomous Database: managed Oracle DB with vector - Functions/OKE: serverless and Kubernetes - Observability: Logging, APM; 3rd‑pa",
    "content": "# OCI for AI Architects (Oracle Cloud Infrastructure) Key Services - GenAI Service: managed access to foundation models - Oracle Database 23ai: AI Vector Search + JSON Relational Duality - Autonomous Database: managed Oracle DB with vector - Functions/OKE: serverless and Kubernetes - Observability: Logging, APM; 3rd‑party Langfuse/Sentry Vector Options - Oracle DB 23ai AI Vector Search (native) - Postgres (OCI) + pgvector Quick Start (RAG) 1) Store docs in Oracle DB 23ai (vector index) 2) Retrieval API: cosine similarity + lexical fallback 3) Tutor API: call GenAI Service/OpenAI with citations 4) Observability: traces, evals, cost budgets Docs & Links - Oracle GenAI Service — https://docs.oracle.com/en-us/iaas/Content/generative-ai/overview.htm - Database 23ai Vector Search — https://www.oracle.com/database/ai/ - JSON Relational Duality — https://www.oracle.com/database/json/"
  },
  {
    "id": "12-concepts\\caching-and-observability.md",
    "title": "Caching & Observability",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/caching-and-observability.md",
    "section": "Concepts",
    "path": "12-concepts\\caching-and-observability.md",
    "headings": [
      {
        "level": 1,
        "text": "Caching & Observability"
      }
    ],
    "excerpt": "# Caching & Observability - Semantic caches; TTL strategies; invalidation - Traces and spans for prompts, retrieval, answers - Cost tracking and dashboards",
    "content": "# Caching & Observability - Semantic caches; TTL strategies; invalidation - Traces and spans for prompts, retrieval, answers - Cost tracking and dashboards"
  },
  {
    "id": "12-concepts\\cost-and-latency-slos.md",
    "title": "Cost & Latency SLOs",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/cost-and-latency-slos.md",
    "section": "Concepts",
    "path": "12-concepts\\cost-and-latency-slos.md",
    "headings": [
      {
        "level": 1,
        "text": "Cost & Latency SLOs"
      }
    ],
    "excerpt": "# Cost & Latency SLOs - Budgets per feature/tier; alerts - Latency budgets per step; caching and batch - Model choice tradeoffs; fallback strategies",
    "content": "# Cost & Latency SLOs - Budgets per feature/tier; alerts - Latency budgets per step; caching and batch - Model choice tradeoffs; fallback strategies"
  },
  {
    "id": "12-concepts\\embeddings-and-chunking.md",
    "title": "Embeddings & Chunking",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/embeddings-and-chunking.md",
    "section": "Concepts",
    "path": "12-concepts\\embeddings-and-chunking.md",
    "headings": [
      {
        "level": 1,
        "text": "Embeddings & Chunking"
      }
    ],
    "excerpt": "# Embeddings & Chunking - Embedding model selection: cost vs quality - Chunking: semantic boundaries, window overlap, structure aware - Metadata: titles, headings, source, timestamps - Vector hygiene: dedupe, normalization, index tuning",
    "content": "# Embeddings & Chunking - Embedding model selection: cost vs quality - Chunking: semantic boundaries, window overlap, structure aware - Metadata: titles, headings, source, timestamps - Vector hygiene: dedupe, normalization, index tuning"
  },
  {
    "id": "12-concepts\\eval-and-guardrails.md",
    "title": "Evaluation & Guardrails",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/eval-and-guardrails.md",
    "section": "Concepts",
    "path": "12-concepts\\eval-and-guardrails.md",
    "headings": [
      {
        "level": 1,
        "text": "Evaluation & Guardrails"
      }
    ],
    "excerpt": "# Evaluation & Guardrails - Datasets: Q/A with citations; negative tests - Metrics: faithfulness, coverage, relevance, MRR - Guardrails: safety filters, policy checks, PII redaction",
    "content": "# Evaluation & Guardrails - Datasets: Q/A with citations; negative tests - Metrics: faithfulness, coverage, relevance, MRR - Guardrails: safety filters, policy checks, PII redaction"
  },
  {
    "id": "12-concepts\\hybrid-search-and-reranking.md",
    "title": "Hybrid Search & Reranking",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/hybrid-search-and-reranking.md",
    "section": "Concepts",
    "path": "12-concepts\\hybrid-search-and-reranking.md",
    "headings": [
      {
        "level": 1,
        "text": "Hybrid Search & Reranking"
      }
    ],
    "excerpt": "# Hybrid Search & Reranking - Combine BM25 and vectors for recall and precision - Reranking: cross-encoders or LLM rerankers - Filters and faceting; scoring and thresholds",
    "content": "# Hybrid Search & Reranking - Combine BM25 and vectors for recall and precision - Reranking: cross-encoders or LLM rerankers - Filters and faceting; scoring and thresholds"
  },
  {
    "id": "12-concepts\\prompt-injection-and-security.md",
    "title": "Prompt Injection & Security",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/prompt-injection-and-security.md",
    "section": "Concepts",
    "path": "12-concepts\\prompt-injection-and-security.md",
    "headings": [
      {
        "level": 1,
        "text": "Prompt Injection & Security"
      }
    ],
    "excerpt": "# Prompt Injection & Security - Threats: injection, data exfiltration, jailbreaking - Controls: isolation, allowlists, output validation, sandboxing - Monitoring: anomaly detection, logging, forensics",
    "content": "# Prompt Injection & Security - Threats: injection, data exfiltration, jailbreaking - Controls: isolation, allowlists, output validation, sandboxing - Monitoring: anomaly detection, logging, forensics"
  },
  {
    "id": "12-concepts\\rag-architecture.md",
    "title": "RAG Architecture (Deep Dive)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/12-concepts/rag-architecture.md",
    "section": "Concepts",
    "path": "12-concepts\\rag-architecture.md",
    "headings": [
      {
        "level": 1,
        "text": "RAG Architecture (Deep Dive)"
      }
    ],
    "excerpt": "# RAG Architecture (Deep Dive) - Indexing pipeline: cleaning, chunking, embeddings, metadata - Retrieval: vector similarity + lexical; filters; rerankers - Answering: prompts with rules (citations), abstain paths - Quality: faithfulness, coverage, latency, cost",
    "content": "# RAG Architecture (Deep Dive) - Indexing pipeline: cleaning, chunking, embeddings, metadata - Retrieval: vector similarity + lexical; filters; rerankers - Answering: prompts with rules (citations), abstain paths - Quality: faithfulness, coverage, latency, cost"
  },
  {
    "id": "13-platforms\\README.md",
    "title": "Platforms to Know",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/README.md",
    "section": "Platforms",
    "path": "13-platforms\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "Platforms to Know"
      }
    ],
    "excerpt": "# Platforms to Know - Databricks: Lakehouse, MLflow, vector search integrations - Snowflake: Cortex, Snowflake Native Apps, pgvector via partners - Elastic: Elasticsearch vector, hybrid search, relevance - Neo4j: Knowledge graphs and RAG enrichment - Pinecone/Weaviate/Qdrant/Milvus: managed and OSS vector DBs - Redis: ",
    "content": "# Platforms to Know - Databricks: Lakehouse, MLflow, vector search integrations - Snowflake: Cortex, Snowflake Native Apps, pgvector via partners - Elastic: Elasticsearch vector, hybrid search, relevance - Neo4j: Knowledge graphs and RAG enrichment - Pinecone/Weaviate/Qdrant/Milvus: managed and OSS vector DBs - Redis: Redis Stack with vectors, caching for LLM apps"
  },
  {
    "id": "13-platforms\\databricks.md",
    "title": "Databricks",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/databricks.md",
    "section": "Platforms",
    "path": "13-platforms\\databricks.md",
    "headings": [
      {
        "level": 1,
        "text": "Databricks"
      }
    ],
    "excerpt": "# Databricks - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Databricks - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\elasticsearch.md",
    "title": "Elasticsearch",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/elasticsearch.md",
    "section": "Platforms",
    "path": "13-platforms\\elasticsearch.md",
    "headings": [
      {
        "level": 1,
        "text": "Elasticsearch"
      }
    ],
    "excerpt": "# Elasticsearch - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Elasticsearch - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\milvus.md",
    "title": "Milvus",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/milvus.md",
    "section": "Platforms",
    "path": "13-platforms\\milvus.md",
    "headings": [
      {
        "level": 1,
        "text": "Milvus"
      }
    ],
    "excerpt": "# Milvus - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Milvus - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\neo4j.md",
    "title": "Neo4j",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/neo4j.md",
    "section": "Platforms",
    "path": "13-platforms\\neo4j.md",
    "headings": [
      {
        "level": 1,
        "text": "Neo4j"
      }
    ],
    "excerpt": "# Neo4j - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Neo4j - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\pinecone.md",
    "title": "Pinecone",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/pinecone.md",
    "section": "Platforms",
    "path": "13-platforms\\pinecone.md",
    "headings": [
      {
        "level": 1,
        "text": "Pinecone"
      }
    ],
    "excerpt": "# Pinecone - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Pinecone - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\qdrant.md",
    "title": "Qdrant",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/qdrant.md",
    "section": "Platforms",
    "path": "13-platforms\\qdrant.md",
    "headings": [
      {
        "level": 1,
        "text": "Qdrant"
      }
    ],
    "excerpt": "# Qdrant - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Qdrant - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\redis.md",
    "title": "Redis",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/redis.md",
    "section": "Platforms",
    "path": "13-platforms\\redis.md",
    "headings": [
      {
        "level": 1,
        "text": "Redis"
      }
    ],
    "excerpt": "# Redis - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Redis - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\snowflake.md",
    "title": "Snowflake",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/snowflake.md",
    "section": "Platforms",
    "path": "13-platforms\\snowflake.md",
    "headings": [
      {
        "level": 1,
        "text": "Snowflake"
      }
    ],
    "excerpt": "# Snowflake - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Snowflake - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "13-platforms\\weaviate.md",
    "title": "Weaviate",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/13-platforms/weaviate.md",
    "section": "Platforms",
    "path": "13-platforms\\weaviate.md",
    "headings": [
      {
        "level": 1,
        "text": "Weaviate"
      }
    ],
    "excerpt": "# Weaviate - What: - Why it matters: - How to use in RAG: - Links:",
    "content": "# Weaviate - What: - Why it matters: - How to use in RAG: - Links:"
  },
  {
    "id": "14-ai-tools\\README.md",
    "title": "AI Coding Tools & Agents",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/README.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\README.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Coding Tools & Agents"
      }
    ],
    "excerpt": "# AI Coding Tools & Agents Practical guidance for using AI coding assistants and agent tools effectively as an AI Architect. Covers setup, strengths/limits, and playbooks for pairing with your repo. Tools covered: - Claude Code (Anthropic) — deep reasoning in code; inline edits - Gemini Code Assist (Google) — cloud-nat",
    "content": "# AI Coding Tools & Agents Practical guidance for using AI coding assistants and agent tools effectively as an AI Architect. Covers setup, strengths/limits, and playbooks for pairing with your repo. Tools covered: - Claude Code (Anthropic) — deep reasoning in code; inline edits - Gemini Code Assist (Google) — cloud-native integrations - Codex CLI (open-source agentic interface) — repo-aware CLI workflows - Cerebras (Model Studio/Inference) — cost/perf options and CLI use - Aider (OSS) — Git-aware assistant via diffs/commits - Continue (OSS) — local/remote models inside VS Code/JetBrains - Cursor (editor) — AI-native IDE (commercial) - Devin (agent) — long-running coding agent (commercial; conceptual workflow) See also: and ."
  },
  {
    "id": "14-ai-tools\\aider.md",
    "title": "Aider (OSS)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/aider.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\aider.md",
    "headings": [
      {
        "level": 1,
        "text": "Aider (OSS)"
      }
    ],
    "excerpt": "# Aider (OSS) - Repo: https://github.com/Aider-AI/aider - What: Git-aware CLI pair programmer; proposes diffs and commits. - Use: → in repo; ask for changes; review patches. - Tips: Use small, focused asks; rely on branch flows; run tests between steps.",
    "content": "# Aider (OSS) - Repo: https://github.com/Aider-AI/aider - What: Git-aware CLI pair programmer; proposes diffs and commits. - Use: → in repo; ask for changes; review patches. - Tips: Use small, focused asks; rely on branch flows; run tests between steps."
  },
  {
    "id": "14-ai-tools\\cerebras.md",
    "title": "Cerebras (Models & CLI)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/cerebras.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\cerebras.md",
    "headings": [
      {
        "level": 1,
        "text": "Cerebras (Models & CLI)"
      }
    ],
    "excerpt": "# Cerebras (Models & CLI) - What: Efficient LLM inference and training offerings; explore CLI/SDK for cost/perf. - Info: https://www.cerebras.net/ and https://github.com/Cerebras - Use: For workloads needing lower-cost inference at scale; integrate via standard SDKs.",
    "content": "# Cerebras (Models & CLI) - What: Efficient LLM inference and training offerings; explore CLI/SDK for cost/perf. - Info: https://www.cerebras.net/ and https://github.com/Cerebras - Use: For workloads needing lower-cost inference at scale; integrate via standard SDKs."
  },
  {
    "id": "14-ai-tools\\claude-code.md",
    "title": "Claude Code (Anthropic)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/claude-code.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\claude-code.md",
    "headings": [
      {
        "level": 1,
        "text": "Claude Code (Anthropic)"
      }
    ],
    "excerpt": "# Claude Code (Anthropic) - What: IDE and web assistant for code with strong reasoning and refactoring. - Setup: https://www.anthropic.com/claude - Use: Pair for big-picture refactors, doc generation, and design. Provide repo context (files, errors) and ask for patch diffs. - Tips: Prefer \"show me a minimal diff\"; iter",
    "content": "# Claude Code (Anthropic) - What: IDE and web assistant for code with strong reasoning and refactoring. - Setup: https://www.anthropic.com/claude - Use: Pair for big-picture refactors, doc generation, and design. Provide repo context (files, errors) and ask for patch diffs. - Tips: Prefer \"show me a minimal diff\"; iterate with test stubs; paste stack traces and failing tests."
  },
  {
    "id": "14-ai-tools\\codex-cli.md",
    "title": "Codex CLI (Open-source agentic coding interface)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/codex-cli.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\codex-cli.md",
    "headings": [
      {
        "level": 1,
        "text": "Codex CLI (Open-source agentic coding interface)"
      }
    ],
    "excerpt": "# Codex CLI (Open-source agentic coding interface) - Repo: https://github.com/openai/codex - What: Terminal-centric agent that can plan, run commands, and patch files with your approval. - Use: Explain goals → let it propose a plan → approve tool calls → review diffs before applying. - Tips: Keep tasks atomic; commit o",
    "content": "# Codex CLI (Open-source agentic coding interface) - Repo: https://github.com/openai/codex - What: Terminal-centric agent that can plan, run commands, and patch files with your approval. - Use: Explain goals → let it propose a plan → approve tool calls → review diffs before applying. - Tips: Keep tasks atomic; commit often; use a separate branch."
  },
  {
    "id": "14-ai-tools\\continue.md",
    "title": "Continue (OSS)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/continue.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\continue.md",
    "headings": [
      {
        "level": 1,
        "text": "Continue (OSS)"
      }
    ],
    "excerpt": "# Continue (OSS) - Repo: https://github.com/continuedev/continue - What: Open-source AI assistant inside VS Code/JetBrains; supports local/remote models. - Use: Install extension → configure providers → chat over code, run quick edits. - Tips: Add a context window with key files; save prompts as recipes.",
    "content": "# Continue (OSS) - Repo: https://github.com/continuedev/continue - What: Open-source AI assistant inside VS Code/JetBrains; supports local/remote models. - Use: Install extension → configure providers → chat over code, run quick edits. - Tips: Add a context window with key files; save prompts as recipes."
  },
  {
    "id": "14-ai-tools\\cursor.md",
    "title": "Cursor (Editor)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/cursor.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\cursor.md",
    "headings": [
      {
        "level": 1,
        "text": "Cursor (Editor)"
      }
    ],
    "excerpt": "# Cursor (Editor) - Site: https://www.cursor.com/ - What: AI-first IDE with strong inline and repo-wide refactors (commercial). - Use: Great for day-to-day implementation; keep commits small; use integrated diff view.",
    "content": "# Cursor (Editor) - Site: https://www.cursor.com/ - What: AI-first IDE with strong inline and repo-wide refactors (commercial). - Use: Great for day-to-day implementation; keep commits small; use integrated diff view."
  },
  {
    "id": "14-ai-tools\\devin.md",
    "title": "Devin (Agent)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/devin.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\devin.md",
    "headings": [
      {
        "level": 1,
        "text": "Devin (Agent)"
      }
    ],
    "excerpt": "# Devin (Agent) - Site: https://www.cognition.ai/ - What: Autonomous coding agent (commercial, limited access). Use this as a conceptual workflow: plan tasks → monitor progress → validate outputs. - Guidance: Even with autonomous agents, keep tight scopes, observable outputs, and manual review gates.",
    "content": "# Devin (Agent) - Site: https://www.cognition.ai/ - What: Autonomous coding agent (commercial, limited access). Use this as a conceptual workflow: plan tasks → monitor progress → validate outputs. - Guidance: Even with autonomous agents, keep tight scopes, observable outputs, and manual review gates."
  },
  {
    "id": "14-ai-tools\\gemini-code-assist.md",
    "title": "Gemini Code Assist (Google)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/14-ai-tools/gemini-code-assist.md",
    "section": "Ai Tools",
    "path": "14-ai-tools\\gemini-code-assist.md",
    "headings": [
      {
        "level": 1,
        "text": "Gemini Code Assist (Google)"
      }
    ],
    "excerpt": "# Gemini Code Assist (Google) - What: Code assistance integrated with Google Cloud and Vertex AI. - Setup: https://cloud.google.com/products/code-assist - Use: Cloud-native integrations (Vertex, GKE); ask for infra snippets and policy checks. - Tips: Keep prompts grounded with exact service names and links to docs.",
    "content": "# Gemini Code Assist (Google) - What: Code assistance integrated with Google Cloud and Vertex AI. - Setup: https://cloud.google.com/products/code-assist - Use: Cloud-native integrations (Vertex, GKE); ask for infra snippets and policy checks. - Tips: Keep prompts grounded with exact service names and links to docs."
  },
  {
    "id": "15-workflows\\agentic-swarms.md",
    "title": "Workflows: Agentic Code Swarms",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/agentic-swarms.md",
    "section": "Workflows",
    "path": "15-workflows\\agentic-swarms.md",
    "headings": [
      {
        "level": 1,
        "text": "Workflows: Agentic Code Swarms"
      },
      {
        "level": 2,
        "text": "Planner → Worker → Reviewer (P–W–R)"
      },
      {
        "level": 2,
        "text": "Round‑Robin with Ownership"
      },
      {
        "level": 2,
        "text": "Map‑Reduce"
      },
      {
        "level": 2,
        "text": "Operational Flow"
      }
    ],
    "excerpt": "# Workflows: Agentic Code Swarms ## Planner → Worker → Reviewer (P–W–R) - Use for small, well‑scoped tasks - Add for gated release when safety critical ## Round‑Robin with Ownership - Each agent updates a shared plan; ownership fields resolve conflicts - Good for brainstorming or multi‑discipline drafts ## Map‑Reduce -",
    "content": "# Workflows: Agentic Code Swarms ## Planner → Worker → Reviewer (P–W–R) - Use for small, well‑scoped tasks - Add for gated release when safety critical ## Round‑Robin with Ownership - Each agent updates a shared plan; ownership fields resolve conflicts - Good for brainstorming or multi‑discipline drafts ## Map‑Reduce - Fan‑out to N workers and aggregate; add deduplication and scoring ## Operational Flow 1) Goal defined with acceptance criteria 2) Orchestrator runs; traces captured 3) Evals check outputs; failures routed to triage 4) Approved outputs promoted to deliverables"
  },
  {
    "id": "15-workflows\\ai-briefing.md",
    "title": "AI Briefing Template",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/ai-briefing.md",
    "section": "Workflows",
    "path": "15-workflows\\ai-briefing.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Briefing Template"
      },
      {
        "level": 2,
        "text": "Executive Summary"
      },
      {
        "level": 2,
        "text": "Metrics Pulse"
      },
      {
        "level": 2,
        "text": "Highlights"
      },
      {
        "level": 2,
        "text": "Blockers & Risks"
      },
      {
        "level": 2,
        "text": "Next Experiments"
      },
      {
        "level": 2,
        "text": "Appendices"
      }
    ],
    "excerpt": "# AI Briefing Template Use for weekly leadership touchpoints or launch reviews. Keep it to 10 minutes live with a pre-read. ## Executive Summary - Headline win this week - Biggest risk / decision needed - Upcoming launch date or milestone ## Metrics Pulse | Metric | Target | Latest | Commentary | | --- | --- | --- | --",
    "content": "# AI Briefing Template Use for weekly leadership touchpoints or launch reviews. Keep it to 10 minutes live with a pre-read. ## Executive Summary - Headline win this week - Biggest risk / decision needed - Upcoming launch date or milestone ## Metrics Pulse | Metric | Target | Latest | Commentary | | --- | --- | --- | --- | | Latency | | | | | Cost per call | | | | | Eval score (faithfulness/toxicity/etc.) | | | | | Adoption / usage | | | | ## Highlights - Customer or stakeholder quote - Visual (drop screenshot or reference / Langfuse chart) ## Blockers & Risks - Policy / compliance - Data or infra - Staffing / skills ## Next Experiments - Experiment | Owner | ETA | Success Criteria | ## Appendices - Links to demos, dashboards, notebooks - Guardrail updates or incidents log"
  },
  {
    "id": "15-workflows\\ai-pair-programming.md",
    "title": "AI Pair Programming (Daily Flow)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/ai-pair-programming.md",
    "section": "Workflows",
    "path": "15-workflows\\ai-pair-programming.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Pair Programming (Daily Flow)"
      }
    ],
    "excerpt": "# AI Pair Programming (Daily Flow) 1) Plan: Write a short plan in the issue; define acceptance criteria. 2) Context: Share key files/errors; ask for a minimal diff. 3) Apply: Review patches; run tests; iterate. 4) Document: Update README/docs; generate changelog. 5) PR: Ask AI to draft PR summary; link to issues; reque",
    "content": "# AI Pair Programming (Daily Flow) 1) Plan: Write a short plan in the issue; define acceptance criteria. 2) Context: Share key files/errors; ask for a minimal diff. 3) Apply: Review patches; run tests; iterate. 4) Document: Update README/docs; generate changelog. 5) PR: Ask AI to draft PR summary; link to issues; request reviews. Tools: Aider/Continue/Claude Code. Keep changes small; commit early/often."
  },
  {
    "id": "15-workflows\\issue-triage-with-linear.md",
    "title": "Issue Triage with Linear/GitHub",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/issue-triage-with-linear.md",
    "section": "Workflows",
    "path": "15-workflows\\issue-triage-with-linear.md",
    "headings": [
      {
        "level": 1,
        "text": "Issue Triage with Linear/GitHub"
      }
    ],
    "excerpt": "# Issue Triage with Linear/GitHub - Triage template: priority, scope, acceptance tests - Use AI to cluster issues and propose epics - Auto-draft tasks from 100 Projects with tags and timeboxes",
    "content": "# Issue Triage with Linear/GitHub - Triage template: priority, scope, acceptance tests - Use AI to cluster issues and propose epics - Auto-draft tasks from 100 Projects with tags and timeboxes"
  },
  {
    "id": "15-workflows\\peer-review.md",
    "title": "Peer Review Ritual",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/peer-review.md",
    "section": "Workflows",
    "path": "15-workflows\\peer-review.md",
    "headings": [
      {
        "level": 1,
        "text": "Peer Review Ritual"
      },
      {
        "level": 2,
        "text": "Preparation"
      },
      {
        "level": 2,
        "text": "Live Agenda"
      },
      {
        "level": 2,
        "text": "Prompt Pack"
      },
      {
        "level": 2,
        "text": "Follow-up"
      }
    ],
    "excerpt": "# Peer Review Ritual Use this 20-minute loop with another architect or engineer to stress-test work before shipping. ## Preparation - Share artifact in advance (notebook, PR, deck). - Provide context: goal, audience, current risks. ## Live Agenda 1. **Orientation (2 min):** Presenter frames scope + desired feedback. 2.",
    "content": "# Peer Review Ritual Use this 20-minute loop with another architect or engineer to stress-test work before shipping. ## Preparation - Share artifact in advance (notebook, PR, deck). - Provide context: goal, audience, current risks. ## Live Agenda 1. **Orientation (2 min):** Presenter frames scope + desired feedback. 2. **Walkthrough (6 min):** Presenter demos while reviewer captures questions. 3. **Deep dive (8 min):** Reviewer probes on assumptions, eval coverage, governance, comms. 4. **Decide (2 min):** Agree on ship/no-ship, outstanding tasks, owners. 5. **Log (2 min):** Update or issue tracker. ## Prompt Pack - \"List the top three failure modes for this design.\" - \"Suggest guardrails or evals missing from this plan.\" - \"Draft a one-line executive summary of this artifact.\" ## Follow-up - Capture decisions in PR comments or shared doc. - Book a follow-up if high-risk items remain."
  },
  {
    "id": "15-workflows\\postmortem.md",
    "title": "Postmortem Template for AI Initiatives",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/postmortem.md",
    "section": "Workflows",
    "path": "15-workflows\\postmortem.md",
    "headings": [
      {
        "level": 1,
        "text": "Postmortem Template for AI Initiatives"
      },
      {
        "level": 2,
        "text": "1. Snapshot"
      },
      {
        "level": 2,
        "text": "2. Timeline"
      },
      {
        "level": 2,
        "text": "3. What Went Well"
      },
      {
        "level": 2,
        "text": "4. What Went Wrong / Gaps"
      },
      {
        "level": 2,
        "text": "5. Metrics"
      },
      {
        "level": 2,
        "text": "6. Root Causes"
      },
      {
        "level": 2,
        "text": "7. Action Items"
      },
      {
        "level": 2,
        "text": "8. Follow-up"
      }
    ],
    "excerpt": "# Postmortem Template for AI Initiatives Use after a major milestone, production incident, or learning sprint. ## 1. Snapshot - Event name + date - Participants - Systems / repos touched ## 2. Timeline | Time | Event | Notes | | --- | --- | --- | | | | | ## 3. What Went Well - - ## 4. What Went Wrong / Gaps - Model per",
    "content": "# Postmortem Template for AI Initiatives Use after a major milestone, production incident, or learning sprint. ## 1. Snapshot - Event name + date - Participants - Systems / repos touched ## 2. Timeline | Time | Event | Notes | | --- | --- | --- | | | | | ## 3. What Went Well - - ## 4. What Went Wrong / Gaps - Model performance - Data quality issues - Process or communication misses ## 5. Metrics | Metric | Target | Actual | Delta | Notes | | --- | --- | --- | --- | --- | | Latency | | | | | | Cost | | | | | | Eval Score | | | | | ## 6. Root Causes Break down with 5 Whys or fishbone-style categories (People, Process, Tech, Data, Policy). ## 7. Action Items | Item | Owner | Due Date | Status | | --- | --- | --- | --- | | | | | | ## 8. Follow-up - Schedule review date - Link to updated docs, dashboards, or guardrails - Capture short Loom/Deck summary if useful for stakeholders"
  },
  {
    "id": "15-workflows\\pr-review-with-agents.md",
    "title": "PR Review with AI Agents",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/pr-review-with-agents.md",
    "section": "Workflows",
    "path": "15-workflows\\pr-review-with-agents.md",
    "headings": [
      {
        "level": 1,
        "text": "PR Review with AI Agents"
      }
    ],
    "excerpt": "# PR Review with AI Agents - Generate PR descriptions with context and risk notes. - Ask AI for test cases and edge cases; run suggested tests. - Use prompt pack: and .",
    "content": "# PR Review with AI Agents - Generate PR descriptions with context and risk notes. - Ask AI for test cases and edge cases; run suggested tests. - Use prompt pack: and ."
  },
  {
    "id": "15-workflows\\repo-maintenance-with-ai.md",
    "title": "Repo Maintenance with AI",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/repo-maintenance-with-ai.md",
    "section": "Workflows",
    "path": "15-workflows\\repo-maintenance-with-ai.md",
    "headings": [
      {
        "level": 1,
        "text": "Repo Maintenance with AI"
      }
    ],
    "excerpt": "# Repo Maintenance with AI - Link checks (CI): keep external links fresh - Auto-generate TOCs and badges as needed - Prompt pack: - Batch sanitize links and add context blurbs per link",
    "content": "# Repo Maintenance with AI - Link checks (CI): keep external links fresh - Auto-generate TOCs and badges as needed - Prompt pack: - Batch sanitize links and add context blurbs per link"
  },
  {
    "id": "15-workflows\\retrospective-with-ai.md",
    "title": "Retrospective with AI Assistant",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/15-workflows/retrospective-with-ai.md",
    "section": "Workflows",
    "path": "15-workflows\\retrospective-with-ai.md",
    "headings": [
      {
        "level": 1,
        "text": "Retrospective with AI Assistant"
      },
      {
        "level": 2,
        "text": "Prep"
      },
      {
        "level": 2,
        "text": "Script"
      },
      {
        "level": 2,
        "text": "Capture Template"
      },
      {
        "level": 2,
        "text": "Share-out"
      }
    ],
    "excerpt": "# Retrospective with AI Assistant 15-minute loop to capture learnings with your AI copilot after every sprint or study week. ## Prep - Share the latest metrics, eval results, and guardrail incidents. - Keep the screenshot from Langfuse or your dashboard ready. ## Script 1. **Win scan (3 min):** Prompt the assistant to ",
    "content": "# Retrospective with AI Assistant 15-minute loop to capture learnings with your AI copilot after every sprint or study week. ## Prep - Share the latest metrics, eval results, and guardrail incidents. - Keep the screenshot from Langfuse or your dashboard ready. ## Script 1. **Win scan (3 min):** Prompt the assistant to list outcomes, surprises, and high-signal artefacts. Validate and add context. 2. **Metric review (4 min):** Inspect latency, cost, eval scores. Ask \"What changed?\" and \"What pattern do we see?\" 3. **Risk radar (3 min):** Have the assistant enumerate new risks, policy changes, or data quality issues. 4. **Next experiments (3 min):** Co-draft the next two experiments, including success criteria. 5. **Commitments (2 min):** Log owners, dates, and supporting assets. ## Capture Template ## Share-out - Push notes to or your team workspace. - Attach visual assets (screens, charts, posters) to reinforce narrative."
  },
  {
    "id": "16-collaboration\\agentic-teams.md",
    "title": "Agentic Teams: Roles, RACI, and Operating Model",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/agentic-teams.md",
    "section": "Collaboration",
    "path": "16-collaboration\\agentic-teams.md",
    "headings": [
      {
        "level": 1,
        "text": "Agentic Teams: Roles, RACI, and Operating Model"
      },
      {
        "level": 2,
        "text": "Roles"
      },
      {
        "level": 2,
        "text": "RACI (example)"
      },
      {
        "level": 2,
        "text": "Cadence"
      },
      {
        "level": 2,
        "text": "Guardrails"
      },
      {
        "level": 2,
        "text": "Artifacts"
      }
    ],
    "excerpt": "# Agentic Teams: Roles, RACI, and Operating Model ## Roles - Orchestrator (Architect): defines goals, orchestrations, and acceptance criteria - Agent Lead (Tech): designs agents, tools, error handling, and eval hooks - Data/Evals: defines metrics, datasets, and guardrails; triages failures - Platform: CI/CD, secrets, c",
    "content": "# Agentic Teams: Roles, RACI, and Operating Model ## Roles - Orchestrator (Architect): defines goals, orchestrations, and acceptance criteria - Agent Lead (Tech): designs agents, tools, error handling, and eval hooks - Data/Evals: defines metrics, datasets, and guardrails; triages failures - Platform: CI/CD, secrets, cost controls, observability - Product: scope, risks, UX, compliance sign‑offs ## RACI (example) - Plan: Product (A), Architect (R), Tech (C), Evals (C) - Build: Tech (A/R), Platform (C), Architect (C) - Evaluate: Evals (A/R), Tech (C), Product (C) - Ship: Platform (A/R), Product (C), Architect (C) ## Cadence - Daily: 15‑min swarm review (top failures, costs, blocked items) - Weekly: model/agent perf review; update evals and guardrails - Monthly: pattern retro; consolidate lessons into the playbook ## Guardrails - Budget envelopes per run and per environment - Red‑team prompts and jailbreak checks in CI - PII scans, citations required for claims, source attribution ## Artifacts - Goals → plans → traces → final deliverables - Architecture documents and BoMs - Eval dashboards with trend lines and incidents"
  },
  {
    "id": "16-collaboration\\checklists.md",
    "title": "Checklists (Before/After)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/checklists.md",
    "section": "Collaboration",
    "path": "16-collaboration\\checklists.md",
    "headings": [
      {
        "level": 1,
        "text": "Checklists (Before/After)"
      }
    ],
    "excerpt": "# Checklists (Before/After) Before Asking - [ ] Define success in 1–2 sentences - [ ] Gather context (files, logs, env) - [ ] Decide output format (diff, plan, code, table) - [ ] Note constraints (performance, security, style) After Receiving - [ ] Sanity check: does it meet acceptance criteria? - [ ] Apply minimal dif",
    "content": "# Checklists (Before/After) Before Asking - [ ] Define success in 1–2 sentences - [ ] Gather context (files, logs, env) - [ ] Decide output format (diff, plan, code, table) - [ ] Note constraints (performance, security, style) After Receiving - [ ] Sanity check: does it meet acceptance criteria? - [ ] Apply minimal diff; run tests; lint - [ ] Add/adjust docs; create PR with summary - [ ] Capture follow‑ups as issues"
  },
  {
    "id": "16-collaboration\\escalation-guide.md",
    "title": "Escalation Guide for AI Architect Teams",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/escalation-guide.md",
    "section": "Collaboration",
    "path": "16-collaboration\\escalation-guide.md",
    "headings": [
      {
        "level": 1,
        "text": "Escalation Guide for AI Architect Teams"
      },
      {
        "level": 2,
        "text": "Levels"
      },
      {
        "level": 2,
        "text": "Escalation Checklist"
      },
      {
        "level": 2,
        "text": "Communication Templates"
      },
      {
        "level": 2,
        "text": "Prevention Loops"
      }
    ],
    "excerpt": "# Escalation Guide for AI Architect Teams Use when incidents, policy risks, or critical blockers arise. Keep it visible next to your runbook. ## Levels | Level | Trigger | Who to Engage | Response Time | | --- | --- | --- | --- | | P0 | Production outage, customer impact, critical security issue | Engineering lead, sec",
    "content": "# Escalation Guide for AI Architect Teams Use when incidents, policy risks, or critical blockers arise. Keep it visible next to your runbook. ## Levels | Level | Trigger | Who to Engage | Response Time | | --- | --- | --- | --- | | P0 | Production outage, customer impact, critical security issue | Engineering lead, security, exec sponsor | Immediate | | P1 | Major degradation (latency, cost spikes), policy breach | Tech lead, product lead, legal/compliance | < 2 hours | | P2 | Evaluation failure, new risk identified, missing dependency | Team core, governance partner | < 1 business day | | P3 | FYI / observation | Team async channel | Asynchronous | ## Escalation Checklist 1. Capture context (time, system, version, owner). 2. Attach observability evidence (Langfuse trace, logs, eval results). 3. Identify customer or stakeholder impact. 4. Propose immediate mitigation + longer-term fix. 5. Book follow-up with postmortem owner ( ). ## Communication Templates ## Prevention Loops - Track incidents in a shared doc; review trends monthly. - Review guardrail dashboards weekly with governance partners. - Rotate incident commander role to build muscle across the team."
  },
  {
    "id": "16-collaboration\\issue-templates.md",
    "title": "Issue Templates for AI Pairing",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/issue-templates.md",
    "section": "Collaboration",
    "path": "16-collaboration\\issue-templates.md",
    "headings": [
      {
        "level": 1,
        "text": "Issue Templates for AI Pairing"
      }
    ],
    "excerpt": "# Issue Templates for AI Pairing Feature - Objective - Acceptance criteria - Constraints / non‑goals - Context (links, files) - Definition of done Bug - Observed behavior & logs - Expected behavior - Repro steps - Env (OS, versions) - Hypothesis (optional)",
    "content": "# Issue Templates for AI Pairing Feature - Objective - Acceptance criteria - Constraints / non‑goals - Context (links, files) - Definition of done Bug - Observed behavior & logs - Expected behavior - Repro steps - Env (OS, versions) - Hypothesis (optional)"
  },
  {
    "id": "16-collaboration\\prompting-guide.md",
    "title": "Prompting Guide (Structure & Recipes)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/prompting-guide.md",
    "section": "Collaboration",
    "path": "16-collaboration\\prompting-guide.md",
    "headings": [
      {
        "level": 1,
        "text": "Prompting Guide (Structure & Recipes)"
      }
    ],
    "excerpt": "# Prompting Guide (Structure & Recipes) Prompt Canvas - Role: what assistant is (e.g., senior TS engineer) - Task: the concrete outcome - Context: files, errors, versions, constraints - Output: format requirements (diff, table, steps) - Checks: tests, constraints, guardrails Example (Patch‑First) Recipes - Code Review:",
    "content": "# Prompting Guide (Structure & Recipes) Prompt Canvas - Role: what assistant is (e.g., senior TS engineer) - Task: the concrete outcome - Context: files, errors, versions, constraints - Output: format requirements (diff, table, steps) - Checks: tests, constraints, guardrails Example (Patch‑First) Recipes - Code Review: see - Test Generator: see - Refactor: see - Docs Writer: see - Pattern Drafter: see - RAG Eval: see"
  },
  {
    "id": "16-collaboration\\working-with-ai.md",
    "title": "Working With AI Assistants (Playbook)",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/16-collaboration/working-with-ai.md",
    "section": "Collaboration",
    "path": "16-collaboration\\working-with-ai.md",
    "headings": [
      {
        "level": 1,
        "text": "Working With AI Assistants (Playbook)"
      }
    ],
    "excerpt": "# Working With AI Assistants (Playbook) Goal: Get high‑quality, verifiable outcomes quickly while keeping control over scope, cost, and quality. Principles - Be specific: State goal, constraints, acceptance criteria, and non‑goals. - Provide context: File paths, snippets, errors, env, versions. - Ask for a plan: Have t",
    "content": "# Working With AI Assistants (Playbook) Goal: Get high‑quality, verifiable outcomes quickly while keeping control over scope, cost, and quality. Principles - Be specific: State goal, constraints, acceptance criteria, and non‑goals. - Provide context: File paths, snippets, errors, env, versions. - Ask for a plan: Have the AI outline steps; approve/refine. - Prefer minimal diffs: Request patch/diff instead of full files. - Verify: Run tests, check outputs, and ask for self‑checks. - Iterate: Small scopes → apply → re‑ask with updated context. When to use which tool - Editor‑native (Claude Code, Continue, Cursor): fast inline edits and refactors. - Git‑aware CLI (Aider, Codex CLI): patch‑first changes with review and commits. - Long‑running agents (Devin): exploratory tasks; keep guardrails and review gates. Common patterns - Spec → Tests → Code: Ask for acceptance tests before implementation. - Investigate → Hypothesize → Patch: Paste stack traces; ask for debugging steps. - Design Doc → Skeleton → Fill‑in: Draft and iterate on design before coding. Anti‑patterns - Vague asks; no acceptance criteria. - Oversized changes in one shot. - Blindly pasting generated code without review."
  },
  {
    "id": "README.md",
    "title": "AI Architect Academy � Command Center for Visionary Builders",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/README.md",
    "section": "README.md",
    "path": "README.md",
    "headings": [
      {
        "level": 1,
        "text": "AI Architect Academy � Command Center for Visionary Builders"
      },
      {
        "level": 2,
        "text": "Why This Playbook Wins"
      },
      {
        "level": 2,
        "text": "Search-Optimized Highlights"
      },
      {
        "level": 2,
        "text": "What You Will Learn"
      },
      {
        "level": 2,
        "text": "Run the Experience"
      },
      {
        "level": 2,
        "text": "Launch Tracks"
      },
      {
        "level": 2,
        "text": "Curriculum Builder (Teach & Scale)"
      },
      {
        "level": 2,
        "text": "Micro-Learning Atlas"
      },
      {
        "level": 2,
        "text": "Tooling & Integrations"
      },
      {
        "level": 2,
        "text": "Operate with Confidence"
      },
      {
        "level": 2,
        "text": "Dashboard & Agents"
      },
      {
        "level": 2,
        "text": "Tell the Story"
      },
      {
        "level": 2,
        "text": "Contribute & Extend"
      }
    ],
    "excerpt": "<p align=\"center\"><img src=\"assets/logo.svg\" width=\"420\" alt=\"AI Architect Academy\"></p> <p align=\"center\"> <a href=\"https://github.com/frankxai/ai-architect-academy/stargazers\"><img alt=\"Stars\" src=\"https://img.shields.io/github/stars/frankxai/ai-architect-academy?style=flat-square\"></a> <a href=\"https://github.com/fr",
    "content": "<p align=\"center\"><img src=\"assets/logo.svg\" width=\"420\" alt=\"AI Architect Academy\"></p> <p align=\"center\"> <a href=\"https://github.com/frankxai/ai-architect-academy/stargazers\"><img alt=\"Stars\" src=\"https://img.shields.io/github/stars/frankxai/ai-architect-academy?style=flat-square\"></a> <a href=\"https://github.com/frankxai/ai-architect-academy/pulls\"><img alt=\"PRs\" src=\"https://img.shields.io/badge/PRs-welcome-cyan?style=flat-square\"></a> <a href=\"https://ai-architect-academy.github.io/ai-architect-academy/\"><img alt=\"Pages\" src=\"https://img.shields.io/badge/Pages-live-green?style=flat-square\"></a> </p> # AI Architect Academy � Command Center for Visionary Builders Design, ship, and operate AI systems with confidence. This open playbook gives you the artefacts, visuals, and workflows to lead conversations, execute fast, and tell a world-class story. <div align=\"center\"> <a href=\"START-HERE.md\"><img alt=\"Start Here\" src=\"https://img.shields.io/badge/Start-Now-cyan?style=for-the-badge\"></a> <a href=\"docs/experience.html\"><img alt=\"Explore the Experience\" src=\"https://img.shields.io/badge/Explore-Experience-purple?style=for-the-badge\"></a> <a href=\"https://github.com/frankxai/ai-architect-academy/archive/refs/heads/main.zip\"><img alt=\"Download\" src=\"https://img.shields.io/badge/Clone-Repo-black?style=for-the-badge\"></a> </div> ![AI Architect Academy](assets/ai-architect-campus.png) <p align=\"center\"> <img src=\"assets/ai-architect-professor.png\" alt=\"AI Architect mentor persona using AI co-creation tools\" width=\"320\"> <img src=\"assets/ai-architect-education-poster.png\" alt=\"Think like an AI Architect curriculum poster\" width=\"320\"> </p> ## Why This Playbook Wins - **Everything is cross-linked.** Jump between repo, GitHub Pages, and dashboard without losing context. - **Production-calibre patterns.** Value framing, discovery questions, architecture diagrams, evaluation harnesses, and governance packs sit side-by-side. - **Launch-ready visuals.** Hero art, poster layout"
  },
  {
    "id": "START-HERE.md",
    "title": "Start Here — Build Your AI Architect Command Center",
    "href": "https://github.com/frankxai/ai-architect-academy/blob/main/START-HERE.md",
    "section": "START HERE.md",
    "path": "START-HERE.md",
    "headings": [
      {
        "level": 1,
        "text": "Start Here — Build Your AI Architect Command Center"
      },
      {
        "level": 2,
        "text": "1. Choose Your Mission Profile"
      },
      {
        "level": 3,
        "text": "Launchpad — First 100 Hours"
      },
      {
        "level": 3,
        "text": "Creator & Influencer Track"
      },
      {
        "level": 3,
        "text": "Enterprise & AI CoE Leadership"
      },
      {
        "level": 3,
        "text": "Advisors, Clients, Friends & Family"
      },
      {
        "level": 2,
        "text": "2. Map Your Architecture Playbook"
      },
      {
        "level": 2,
        "text": "3. Run Value Loops Fast"
      },
      {
        "level": 2,
        "text": "4. Instrument, Govern, and Operate"
      },
      {
        "level": 2,
        "text": "5. Launch the Dashboard & Agents"
      },
      {
        "level": 2,
        "text": "6. Amplify and Share the Story"
      },
      {
        "level": 2,
        "text": "Clone & Personalise the Repo"
      },
      {
        "level": 2,
        "text": "Keep Exploring"
      }
    ],
    "excerpt": "# Start Here — Build Your AI Architect Command Center You’re here because you want to ship real AI value, guide stakeholders with confidence, and make your work simple to share. This guide orients you across the repo, the live site, and the dashboard so you can get momentum within minutes. ## 1. Choose Your Mission Pro",
    "content": "# Start Here — Build Your AI Architect Command Center You’re here because you want to ship real AI value, guide stakeholders with confidence, and make your work simple to share. This guide orients you across the repo, the live site, and the dashboard so you can get momentum within minutes. ## 1. Choose Your Mission Profile ### Launchpad — First 100 Hours - Follow the [100-Hour AI Architect Plan](02-learning-paths/100-hour-ai-architect.md) to sprint through retrieval, agents, governance, and storytelling. - Pair the plan with three anchor projects: [RAG on Supabase](05-projects/rag-on-supabase.md), [Langfuse eval harness](05-projects/evals-langfuse.md), and [Vector search benchmarks](05-projects/vector-search-pgvector.md). ### Creator & Influencer Track - Use the [Beginner](02-learning-paths/beginner.md) and [Professional](02-learning-paths/professional.md) paths to pace long-form content, workshops, and community sessions. - Pull examples and citations from [Awesome collections](03-awesome/) and [Articles](09-articles/) to fuel newsletters, talks, and threads. ### Enterprise & AI CoE Leadership - Run the [Bootcamp](02-learning-paths/bootcamp.md) to align product, platform, risk, and ops teams. - Combine architecture patterns ( ) with the [Governance toolkit](08-governance/) and [Collaboration playbooks](16-collaboration/) for executive visibility. ### Advisors, Clients, Friends & Family - Share the [Experience guide](docs/experience.html) when introducing the hub to new collaborators. - Use to spin up proposals, meeting notes, and recap reports fast. ## 2. Map Your Architecture Playbook - Start with three core patterns: [Content Generation](01-design-patterns/content-generation.md), [Decision Support](01-design-patterns/decision-support.md), and [Model Lifecycle Management](01-design-patterns/model-lifecycle-management.md). - Explore the [Concept decks](12-concepts/) for mental models (retrieval, evaluation, observability, safety) you can present to stakeholders. - "
  }
]